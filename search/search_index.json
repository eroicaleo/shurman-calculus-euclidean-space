{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! This is my notes to the exercises of Professor Jerry Shurman's \"Calculus and Analysis in Euclidean Space\". You can find this book from the publisher here .","title":"Home"},{"location":"#welcome","text":"This is my notes to the exercises of Professor Jerry Shurman's \"Calculus and Analysis in Euclidean Space\". You can find this book from the publisher here .","title":"Welcome!"},{"location":"ch01ex/","text":"Chapter 1 Results from One-Variable Calculus 1.1 The Real Number System 1.1.1. Referring only to the field axioms, show that 0x = 0 for all x \u2208 R . Proof : Use (m2), (d1), (a2), (m2), we have 0x + x = 0x + 1x = (0 + 1)x = 1x = x Let y be the additive inverse of x , then (0x + x) + y = 0x + (x + y) = 0x + 0 = 0x \\\\ x + y = 0 So 0x = 0 \\square 1.1.2. Prove that in every ordered field, 1 is positive. Prove that the complex number field \\mathbb{C} cannot be made an ordered field. Proof : If 1 = 0 , then given x \\in \\mathbb{F} , x = 1x = 0x = 0 . Then \\mathbb{F} = \\{0\\} . Now we assume 1 \\neq 0 . If 1 \\not \\in \\mathbb{F}^+ , then -1 \\in \\mathbb{F}^+ . Then note (-1)(-1) + (-1)1 = (-1)((-1) + 1) = (-1)0 = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) + (-1) = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) = 1 So from (o3), 1 \\in \\mathbb{F}^+ , we have a contradiction. Then 1 \\in \\mathbb{F}^+ . For the second part. if i \\in \\mathbb{C}^+ , then i \\cdot i = -1 \\in \\mathbb{C}^- , we have a contradiction. if i \\in \\mathbb{C}^- , then (-i) \\cdot (-i) = i^2 = -1 \\in \\mathbb{C}^- , we have a contradiction again. \\square 1.1.3 Use a completeness property of the real number system to show that 2 has a positive square root. Proof : Let A = \\{x : x \\geq 0 \\text{ and } x^2 < 2\\} A is not empty, e.g. 1 \\in A . A is upper bounded, e.g. 2 is an upper bound. Now assume r is the least upper bound. If r^2 < 2 , then let d = 2 - r^2 . We can find n such that 2r/n + 1/n^2 < d , then (r + 1/n)^2 = r^2 + 2r/n + 1/n^2 < r^2 + d = 2 That means r < r+1/n \\in A , so r is not an upper bound, we have a contradiction. If r^2 > 2 , then let d = r^2 - 2 . We can find n such that 2r/n - 1/n^2 < d , then (r - 1/n)^2 = r^2 - 2r/n + 1/n^2 > r^2 - d = 2 That means r - 1/n is an upper bound of A . So r is not the least upper bound. Then we have to have r^2 = 2 . \\square 1.1.4. (a) Prove by induction that \\sum_{i=1}^{n} i^2 = \\frac{n(n+1)(2n+1)}{6} \\text{ for all } n \\in \\mathbb{Z}^+ Proof : When n = 1 , 1^2 = \\frac{1 \\cdot 2 \\cdot 3}{6} Assume when n = k this is correct \\begin{split} \\sum_{i=1}^{k+1} i^2 &= \\frac{k(k+1)(2k+1)}{6} + (k+1)^2 \\\\ &= \\frac{[k(2k+1) + 6(k+1)](k+1)}{6} \\\\ &= \\frac{(k+1)(k+2)(2(k+1)+1)}{6} \\\\ \\end{split} So when n = k + 1 , it's still correct. \\square (b) (Bernoulli\u2019s inequality) For every real number r \u2265 \u22121 , prove that (1+r)^n \\geq 1 + rn \\text{ for all } n \\in \\mathbb{N}. Proof : It's correct when n = 1 . (1+r)^{k+1} \\geq (1+rk)(1+r) = 1 + r(k+1) + r^2k \\geq 1 + r(k+1) So it's correct for all n \\in \\mathbb{N} . \\square (c) For what positive integers n is 2^n > n^3 ? Proof : Note when n = 10 2^{10} = 1024 > 1000 = 10^3 Assume 2^k > k^3, k \\geq 10 (k+1)^3 = (k^3 + 3k^2 + 3k + 1) < k^3 + 3k^2 + 3k^2 + 3k^2 \\\\ = k^3 + 9k^2 < k^3 + k^3 = 2 k^3 < 2 \\cdot 2^k = 2^{k+1} \\square 1.1.5. (a) Use the induction theorem to show that for every natural number m , the sum m+n and the product mn are again natural for every natural number n . Thus \\mathbb{N} is closed under addition and multiplication, and consequently so is \\mathbb{Z} . Proof : skip (b) Which of the field axioms continue to hold for the natural numbers? Solution : The following does not hold: (a3) Existence of additive inverses (m3) Existence of multiplicative inverses (c) Which of the field axioms continue to hold for the integers? The following does not hold: (m3) Existence of multiplicative inverses \\square 1.1.6. For every positive integer n , let \\mathbb{Z}/n\\mathbb{Z} denote the set \\{0,1,...,n\u22121\\} with the usual operations of addition and multiplication carried out taking remainders on division by n . That is, add and multiply in the usual fashion but subject to the additional condition that n = 0 . For example, in \\mathbb{Z}/5\\mathbb{Z} we have 2+4 = 1 and 2\u00b74 = 3 . For what values of n does \\mathbb{Z}/n\\mathbb{Z} form a field? Solution : If n is a composite number, and assume n = a \\cdot b, a > 1, b > 1 . If a has a multiplication inverse a' , then a \\cdot a' \\equiv 1 \\pmod n So a \\cdot a' - kn = 1 , but a | n and a | a , so a | 1 , we have a contradiction. So a does not have a multiplication inverse, then \\mathbb{Z}/n\\mathbb{Z} is not a field. If n is prime number, and 1 < a < n , then (a, n) = 1 . We can find p, q such that ap + nq = 1 . So ap \\equiv 1 \\pmod n Then p is the multiplication inverse of a . \\mathbb{Z}/n\\mathbb{Z} is a field. \\square 1.2 Foundational and Basic Theorems 1.2.1. Use the intermediate value theorem to show that 2 has a positive square root. 1.3 1.3.1. (a) Let n \u2208 \\mathbb{N} . What is the (2n + 1) st-degree Taylor polynomial T_{2n+1}(x) for the function f(x) = \\sin x at 0 ? (The reason for the strange indexing here is that every second term of the Taylor polynomial is 0 .) Prove that \\sin x is equal to the limit of T_{2n+1}(x) as n \u2192 \u221e , similarly to the argument in the text for e^x . Also find T_{2n}(x) for f(x) = \\cos x at 0 , and explain why the argument for \\sin shows that \\cos x is the limit of its even-degree Taylor polynomials as well. Solution T_{2n+1}(x) = \\frac{x}{1!} - \\frac{x^3}{3!} + + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots The Taylor remainder is |R_{2n+1}(x)| = \\left| \\frac{\\sin^{(2n+2)}(c) x^{2n+2}}{(2n+2)!} \\right| \\leq \\frac{x^{2n+2}}{(2n+2)!} \\rightarrow 0 For \\cos x T_{2n}(x) = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots |R_{2n}(x)| = \\left| \\frac{\\cos^{(2n+1)}(c) x^{2n+1}}{(2n+1)!} \\right| \\leq \\frac{x^{2n+1}}{(2n+1)!} \\rightarrow 0 \\square (b) Many years ago, the author\u2019s high-school physics textbook asserted, ba\ufb04ingly, that the approximation \\sin x \u2248 x is good for x up to 8^\u25e6 . Deconstruct. Solution : 8^\\circ in terms of radius is \\frac{\\pi }{2} \\times 8 / 90 \\approx 0.14 . And we have |R_1(x)| \\leq \\frac{x^2}{2} = \\frac{0.14^2}{2} \\approx 0.01 So the error with 8^\\circ is less than 1\\% . \\square 1.3.2. What is the n th-degree Taylor polynomial T_n(x) for the following functions at 0 ? (a) f(x) = \\arctan x . (This exercise is not just a matter of routine mechanics. One way to proceed involves the geometric series, and another makes use of the factorization 1 + x^2 = (1-ix)(1+ix).) Solution : Here are steps in Understanding Analysis First, we know \\arctan ' x = \\frac{1}{1 + x^2} Note the following geometric series converges in (-1, 1) . \\frac{1}{1 - x} = 1 + x + x^2 + \\cdots We replace x with -x^2 and get \\frac{1}{1 + x^2} = 1 - x^2 + x^4 - x^6 + \\cdots Now take anti-differentiation on both side: \\arctan x = x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7} + \\cdots This is the n th-degree Taylor polynomial T_n(x) for \\arctan x . \\square (b) f(x) = (1 + x)^\u03b1 where \u03b1 \u2208 \\mathbb{R} . (Although the answer can be written in a uniform way for all \u03b1 , it behaves di\ufb00erently when \u03b1 \u2208 \\mathbb{N} . Introduce the generalized binomial coe\ufb03cient symbol \\binom{\u03b1}{k} = \\frac{\u03b1(\u03b1-1)(\u03b1-2)\\cdots (\u03b1-k+1)}{k!}, k \\in \\mathbb{N} to help produce a tidy answer.) Solution : f'(x) = \u03b1(1+x)^{\u03b1-1} = \\binom{\u03b1}{1} \\\\ f''(x) = \u03b1(\u03b1-1)(1+x)^{\u03b1-2} = 2! \\binom{\u03b1}{2} \\\\ \\cdots \\\\ f^{n}(x) = \u03b1(\u03b1-1)\\cdots(\u03b1-n+1) (1+x)^{\u03b1-n} = n! \\binom{\u03b1}{n} \\\\ So T_n(x) = 1 + \\binom{\u03b1}{1} x + \\binom{\u03b1}{2} x^2 + \\cdots + \\binom{\u03b1}{n} x^n \\square 1.3.3. (a) Further tighten the numerical estimate of \\ln(1.1) from this section by reasoning as follows. As n increases, the Taylor polynomials T_n(0.1) add terms of decreasing magnitude and alternating sign. Therefore T_4(0.1) underestimates \\ln(1.1) . Now that we know this, it is useful to find the smallest possible value of the remainder (by setting c = 0.1 rather than c = 0 in the formula). Then \\ln(1.1) lies between T_4(0.1) plus this smallest possible remainder value and T_4(0.1) plus the largest possible remainder value, obtained in the section. Supply the numbers, and verify by machine that the tighter estimate of \\ln(1.1) is correct. Solution : T_n(x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\cdots = \\sum_{k=1}^{n} (-1)^{k-1} \\frac{x^k}{k}, And we also have R_n(x) = \\frac{(-1)^n x^{n+1}}{(1+c)^{n+1}(n+1)} And we can plug c = 0, 0.1, x = 0.1 into the above equation. So we have \\frac{0.1^5}{(1.1^5 \\cdot 5)} \\leq R_4(x) \\leq \\frac{0.1^5}{(1.0^5 \\cdot 5)} We calculate \\ln (1.1) = 0.09531018 , T_4(1.1) = 0.09530833 T_4(1.1) + \\frac{0.1^5}{(1.1^5 \\cdot 5)} = 0.09530957 Which is indeed than the lower bound obtained in the book, which is 0.09530633 . \\square (b) In Figure 1.1, identify the graphs of T_1 through T_5 and the graph of \\ln near x = 0 and near x = 2 . Solution : It's easy to see T_1 is a straight line. Now focus on x \\geq 0 . From part (a), we know T_1(x) > T_3(x) > T_5(x) > \\ln (x) and T_2(x) < T_4(x) < \\ln (x) . Then focus on x < 0 , notice T_{n}(x) - T_{n+1}(x) = (-1)^{n+1} \\frac{x^{n+1}}{n+1} No matter of n , it is always positive, so T_1(x) > T_2(x) > \\cdots > T_5(x) > \\ln (x) . \\square 1.3.4. Working by hand, use the third-degree Taylor polynomial for \\sin(x) at 0 to approximate a decimal representation of \\sin(0.1) . Also compute the decimal representation of an upper bound for the error of the approximation. Bound \\sin(0.1) between two decimal representations. Solution : T_3(x) = \\frac{x}{1} - \\frac{x^3}{3!} And also |R_3(x)| \\leq \\frac{x^4}{4!} T_3(0.1) = 0.09983333 \\\\ |R_3(0.1)| \\leq \\frac{0.1^4}{4!} = 0.00000417 \\\\ 0.09982916 \\leq \\sin (0.1) \\leq 0.0998375 With calculator, \\sin (0.1) \\approx 0.09983342 . 1.3.5. Use a second-degree Taylor polynomial to approximate \\sqrt[]{4.2} . Use Taylor\u2019s theorem to find a guaranteed accuracy of the approximation and thus to find upper and lower bounds for \\sqrt[]{4.2} . Solution : Consider f(x) = \\sqrt[]{4+x} \\begin{split} f'(x) &= \\frac{1}{2} (4+x)^{-\\frac{1}{2}} \\\\ f''(x) &= -\\frac{1}{2^2} (4+x) ^{-\\frac{3}{2}} \\\\ f^{(3)}(x) &= \\frac{1 \\cdot 3}{2^3} (4+x) ^{-\\frac{5}{2}} \\\\ \\end{split} So we have T_2(x) = f(0) + f'(0) x + \\frac{f''(0)}{2!}x^2 \\\\ = 2 + \\frac{x}{4} - \\frac{x^2}{64} The remainder is: |R_2(x)| = \\left| \\frac{f^{(3)}(c)}{3!}x^3 \\right| \\leq \\frac{f^{(3)}(0)}{3!}x^3 = \\frac{3x^3}{3!2^8} = \\frac{x^3}{512} Then we have T_2(0.2) = 2.049375 \\\\ |R_2(0.2)| = \\frac{0.2^3}{512} = 0.00001562 \\\\ \\sqrt[]{4.2} = 2.04939015 \\\\ Finally, we have 2.04935938 = T_2(0.2) - |R_2(0.2)| < \\sqrt[]{4.2} < T_2(0.2) + |R_2(0.2)| = 2.04939062 \\square 1.3.6. (a) Another proof of Taylor\u2019s Theorem uses the fundamental theorem of integral calculus once and then integrates by parts repeatedly. Begin with the hypotheses of Theorem 1.3.3, and let x \u2208 I . By the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(t) dt Let u = f'(t) and v = t - x , so that the integral is \\int_{a}^{x} u v' , and integrating by parts gives \\int_{a}^{x} u v' = uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v = f'(x)(x - x) - f'(a) (a - x) - \\int_{a}^{x} f''(t)(t-x) \\\\ = f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt So f(x) = f(a) + f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt Now let u = f''(t) and v = \\frac{1}{2} (t - x)^2 , \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f''(x) \\frac{1}{2} (x-x)^2 - f''(a) \\frac{1}{2}(a - x)^2 - \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt \\end{split} So f(x) = f(a) + f'(a) (x - a) + f''(a) \\frac{(x-a)^2}{2} + \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt Then we can use induction and assume f(x) = T_n(x) + (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt Let u = f^{(n+1)}(t), v = \\frac{(t-x)^{(n+1)}}{(n+1)!} \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f^{(n+1)}(x) \\frac{(x-x)^{n+1}}{{(n+1)}!} - f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} - \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\\\ &= (-1) \\cdot \\left( f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} + \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\right) \\end{split} No mattern n is even or odd, (-1)^{n} (a-x)^n = (x-a)^n So f(x) = T_{n+1} + (-1)^{n+1} \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} dt Whereas the expression for f(x)\u2212T_n(x) in Theorem 1.3.3 is called the Lagrange form of the remainder, this exercise has derived the integral form of the remainder. Use the extreme value theorem and the intermediate value theorem to derive the Lagrange form of the remainder from the integral form. Proof : We want to estimate A = (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} Assume m \\leq f^{(n+1)}(t) \\leq M , and let B = (-1)^n \\int_{a}^{x} \\frac{(t-x)^n}{n!} dt = (-1)^n \\frac{(t-x)^{(n+1)}}{{(n+1)}!}\\Bigg|_{a}^{x} = (-1)^{n+1} \\frac{(a-x)^{(n+1)}}{{(n+1)}!} = \\frac{(x-a)^{(n+1)}}{{(n+1)}!} Note (-1)^n \\frac{(t-x)^n}{n!} \\geq 0, t \\in [a, x] , so m \\frac{(x-a)^{(n+1)}}{{(n+1)}!} \\leq (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} \\leq M \\frac{(x-a)^{(n+1)}}{{(n+1)}!} i.e. mB \\leq A \\leq MB Since f^{(n+1)} is continuous, with extreme value theorem, we can find f^{(n+1)}(t_m) = m, f^{(n+1)}(t_M) = M . Then with intermediate value theorem, we can find c , such that f^{(n+1)}(c) = A/B , i.e. (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1} \\square (b) Use the integral form of the remainder to show that \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] Proof : f^{(n+1)}(t) = \\frac{(-1)^n n!}{(1+t)^{n+1}} So \\begin{split} (-1)^n \\int_{0}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt &= (-1)^{2n} \\int_{0}^{x} \\frac{(-1)^n n!}{(1+t)^{n+1}} \\frac{(t-x)^n}{n!} dt \\\\ &= \\int_{0}^{x} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\end{split} Consider, x \\in (-1, -1/2] , and t \\geq x . t \\geq x > (2+t)x \\\\ \\Rightarrow \\\\ t > (2+t)x \\\\ \\Rightarrow \\\\ t - x > x + tx \\\\ \\Rightarrow \\\\ \\frac{t-x}{1+t} > x \\\\ So \\left| \\frac{t-x}{1+t} \\right| < |x| Then \\begin{split} \\left| \\int_{x}^{0} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\right| & \\leq \\int_{x}^{0} \\left| \\frac{(t-x)^n}{(1+t)^{n+1}} \\right| dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} \\left| \\frac{(t-x)}{(1+t)} \\right|^n dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} |x|^n dt\\\\ &= \\frac{|x|^{n+1}}{1+x} \\to 0 \\end{split} The book has proved the case for (-1/2, 1] . Then we can conclude \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] \\square","title":"Chapter 01 Exercises"},{"location":"ch01ex/#chapter-1-results-from-one-variable-calculus","text":"","title":"Chapter 1 Results from One-Variable Calculus"},{"location":"ch01ex/#11-the-real-number-system","text":"","title":"1.1 The Real Number System"},{"location":"ch01ex/#111","text":"Referring only to the field axioms, show that 0x = 0 for all x \u2208 R . Proof : Use (m2), (d1), (a2), (m2), we have 0x + x = 0x + 1x = (0 + 1)x = 1x = x Let y be the additive inverse of x , then (0x + x) + y = 0x + (x + y) = 0x + 0 = 0x \\\\ x + y = 0 So 0x = 0 \\square","title":"1.1.1."},{"location":"ch01ex/#112","text":"Prove that in every ordered field, 1 is positive. Prove that the complex number field \\mathbb{C} cannot be made an ordered field. Proof : If 1 = 0 , then given x \\in \\mathbb{F} , x = 1x = 0x = 0 . Then \\mathbb{F} = \\{0\\} . Now we assume 1 \\neq 0 . If 1 \\not \\in \\mathbb{F}^+ , then -1 \\in \\mathbb{F}^+ . Then note (-1)(-1) + (-1)1 = (-1)((-1) + 1) = (-1)0 = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) + (-1) = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) = 1 So from (o3), 1 \\in \\mathbb{F}^+ , we have a contradiction. Then 1 \\in \\mathbb{F}^+ . For the second part. if i \\in \\mathbb{C}^+ , then i \\cdot i = -1 \\in \\mathbb{C}^- , we have a contradiction. if i \\in \\mathbb{C}^- , then (-i) \\cdot (-i) = i^2 = -1 \\in \\mathbb{C}^- , we have a contradiction again. \\square","title":"1.1.2."},{"location":"ch01ex/#113","text":"Use a completeness property of the real number system to show that 2 has a positive square root. Proof : Let A = \\{x : x \\geq 0 \\text{ and } x^2 < 2\\} A is not empty, e.g. 1 \\in A . A is upper bounded, e.g. 2 is an upper bound. Now assume r is the least upper bound. If r^2 < 2 , then let d = 2 - r^2 . We can find n such that 2r/n + 1/n^2 < d , then (r + 1/n)^2 = r^2 + 2r/n + 1/n^2 < r^2 + d = 2 That means r < r+1/n \\in A , so r is not an upper bound, we have a contradiction. If r^2 > 2 , then let d = r^2 - 2 . We can find n such that 2r/n - 1/n^2 < d , then (r - 1/n)^2 = r^2 - 2r/n + 1/n^2 > r^2 - d = 2 That means r - 1/n is an upper bound of A . So r is not the least upper bound. Then we have to have r^2 = 2 . \\square","title":"1.1.3"},{"location":"ch01ex/#114","text":"(a) Prove by induction that \\sum_{i=1}^{n} i^2 = \\frac{n(n+1)(2n+1)}{6} \\text{ for all } n \\in \\mathbb{Z}^+ Proof : When n = 1 , 1^2 = \\frac{1 \\cdot 2 \\cdot 3}{6} Assume when n = k this is correct \\begin{split} \\sum_{i=1}^{k+1} i^2 &= \\frac{k(k+1)(2k+1)}{6} + (k+1)^2 \\\\ &= \\frac{[k(2k+1) + 6(k+1)](k+1)}{6} \\\\ &= \\frac{(k+1)(k+2)(2(k+1)+1)}{6} \\\\ \\end{split} So when n = k + 1 , it's still correct. \\square (b) (Bernoulli\u2019s inequality) For every real number r \u2265 \u22121 , prove that (1+r)^n \\geq 1 + rn \\text{ for all } n \\in \\mathbb{N}. Proof : It's correct when n = 1 . (1+r)^{k+1} \\geq (1+rk)(1+r) = 1 + r(k+1) + r^2k \\geq 1 + r(k+1) So it's correct for all n \\in \\mathbb{N} . \\square (c) For what positive integers n is 2^n > n^3 ? Proof : Note when n = 10 2^{10} = 1024 > 1000 = 10^3 Assume 2^k > k^3, k \\geq 10 (k+1)^3 = (k^3 + 3k^2 + 3k + 1) < k^3 + 3k^2 + 3k^2 + 3k^2 \\\\ = k^3 + 9k^2 < k^3 + k^3 = 2 k^3 < 2 \\cdot 2^k = 2^{k+1} \\square","title":"1.1.4."},{"location":"ch01ex/#115","text":"(a) Use the induction theorem to show that for every natural number m , the sum m+n and the product mn are again natural for every natural number n . Thus \\mathbb{N} is closed under addition and multiplication, and consequently so is \\mathbb{Z} . Proof : skip (b) Which of the field axioms continue to hold for the natural numbers? Solution : The following does not hold: (a3) Existence of additive inverses (m3) Existence of multiplicative inverses (c) Which of the field axioms continue to hold for the integers? The following does not hold: (m3) Existence of multiplicative inverses \\square","title":"1.1.5."},{"location":"ch01ex/#116","text":"For every positive integer n , let \\mathbb{Z}/n\\mathbb{Z} denote the set \\{0,1,...,n\u22121\\} with the usual operations of addition and multiplication carried out taking remainders on division by n . That is, add and multiply in the usual fashion but subject to the additional condition that n = 0 . For example, in \\mathbb{Z}/5\\mathbb{Z} we have 2+4 = 1 and 2\u00b74 = 3 . For what values of n does \\mathbb{Z}/n\\mathbb{Z} form a field? Solution : If n is a composite number, and assume n = a \\cdot b, a > 1, b > 1 . If a has a multiplication inverse a' , then a \\cdot a' \\equiv 1 \\pmod n So a \\cdot a' - kn = 1 , but a | n and a | a , so a | 1 , we have a contradiction. So a does not have a multiplication inverse, then \\mathbb{Z}/n\\mathbb{Z} is not a field. If n is prime number, and 1 < a < n , then (a, n) = 1 . We can find p, q such that ap + nq = 1 . So ap \\equiv 1 \\pmod n Then p is the multiplication inverse of a . \\mathbb{Z}/n\\mathbb{Z} is a field. \\square","title":"1.1.6."},{"location":"ch01ex/#12-foundational-and-basic-theorems","text":"","title":"1.2 Foundational and Basic Theorems"},{"location":"ch01ex/#121","text":"Use the intermediate value theorem to show that 2 has a positive square root.","title":"1.2.1."},{"location":"ch01ex/#13","text":"","title":"1.3"},{"location":"ch01ex/#131","text":"(a) Let n \u2208 \\mathbb{N} . What is the (2n + 1) st-degree Taylor polynomial T_{2n+1}(x) for the function f(x) = \\sin x at 0 ? (The reason for the strange indexing here is that every second term of the Taylor polynomial is 0 .) Prove that \\sin x is equal to the limit of T_{2n+1}(x) as n \u2192 \u221e , similarly to the argument in the text for e^x . Also find T_{2n}(x) for f(x) = \\cos x at 0 , and explain why the argument for \\sin shows that \\cos x is the limit of its even-degree Taylor polynomials as well. Solution T_{2n+1}(x) = \\frac{x}{1!} - \\frac{x^3}{3!} + + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots The Taylor remainder is |R_{2n+1}(x)| = \\left| \\frac{\\sin^{(2n+2)}(c) x^{2n+2}}{(2n+2)!} \\right| \\leq \\frac{x^{2n+2}}{(2n+2)!} \\rightarrow 0 For \\cos x T_{2n}(x) = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots |R_{2n}(x)| = \\left| \\frac{\\cos^{(2n+1)}(c) x^{2n+1}}{(2n+1)!} \\right| \\leq \\frac{x^{2n+1}}{(2n+1)!} \\rightarrow 0 \\square (b) Many years ago, the author\u2019s high-school physics textbook asserted, ba\ufb04ingly, that the approximation \\sin x \u2248 x is good for x up to 8^\u25e6 . Deconstruct. Solution : 8^\\circ in terms of radius is \\frac{\\pi }{2} \\times 8 / 90 \\approx 0.14 . And we have |R_1(x)| \\leq \\frac{x^2}{2} = \\frac{0.14^2}{2} \\approx 0.01 So the error with 8^\\circ is less than 1\\% . \\square","title":"1.3.1."},{"location":"ch01ex/#132","text":"What is the n th-degree Taylor polynomial T_n(x) for the following functions at 0 ? (a) f(x) = \\arctan x . (This exercise is not just a matter of routine mechanics. One way to proceed involves the geometric series, and another makes use of the factorization 1 + x^2 = (1-ix)(1+ix).) Solution : Here are steps in Understanding Analysis First, we know \\arctan ' x = \\frac{1}{1 + x^2} Note the following geometric series converges in (-1, 1) . \\frac{1}{1 - x} = 1 + x + x^2 + \\cdots We replace x with -x^2 and get \\frac{1}{1 + x^2} = 1 - x^2 + x^4 - x^6 + \\cdots Now take anti-differentiation on both side: \\arctan x = x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7} + \\cdots This is the n th-degree Taylor polynomial T_n(x) for \\arctan x . \\square (b) f(x) = (1 + x)^\u03b1 where \u03b1 \u2208 \\mathbb{R} . (Although the answer can be written in a uniform way for all \u03b1 , it behaves di\ufb00erently when \u03b1 \u2208 \\mathbb{N} . Introduce the generalized binomial coe\ufb03cient symbol \\binom{\u03b1}{k} = \\frac{\u03b1(\u03b1-1)(\u03b1-2)\\cdots (\u03b1-k+1)}{k!}, k \\in \\mathbb{N} to help produce a tidy answer.) Solution : f'(x) = \u03b1(1+x)^{\u03b1-1} = \\binom{\u03b1}{1} \\\\ f''(x) = \u03b1(\u03b1-1)(1+x)^{\u03b1-2} = 2! \\binom{\u03b1}{2} \\\\ \\cdots \\\\ f^{n}(x) = \u03b1(\u03b1-1)\\cdots(\u03b1-n+1) (1+x)^{\u03b1-n} = n! \\binom{\u03b1}{n} \\\\ So T_n(x) = 1 + \\binom{\u03b1}{1} x + \\binom{\u03b1}{2} x^2 + \\cdots + \\binom{\u03b1}{n} x^n \\square","title":"1.3.2."},{"location":"ch01ex/#133","text":"(a) Further tighten the numerical estimate of \\ln(1.1) from this section by reasoning as follows. As n increases, the Taylor polynomials T_n(0.1) add terms of decreasing magnitude and alternating sign. Therefore T_4(0.1) underestimates \\ln(1.1) . Now that we know this, it is useful to find the smallest possible value of the remainder (by setting c = 0.1 rather than c = 0 in the formula). Then \\ln(1.1) lies between T_4(0.1) plus this smallest possible remainder value and T_4(0.1) plus the largest possible remainder value, obtained in the section. Supply the numbers, and verify by machine that the tighter estimate of \\ln(1.1) is correct. Solution : T_n(x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\cdots = \\sum_{k=1}^{n} (-1)^{k-1} \\frac{x^k}{k}, And we also have R_n(x) = \\frac{(-1)^n x^{n+1}}{(1+c)^{n+1}(n+1)} And we can plug c = 0, 0.1, x = 0.1 into the above equation. So we have \\frac{0.1^5}{(1.1^5 \\cdot 5)} \\leq R_4(x) \\leq \\frac{0.1^5}{(1.0^5 \\cdot 5)} We calculate \\ln (1.1) = 0.09531018 , T_4(1.1) = 0.09530833 T_4(1.1) + \\frac{0.1^5}{(1.1^5 \\cdot 5)} = 0.09530957 Which is indeed than the lower bound obtained in the book, which is 0.09530633 . \\square (b) In Figure 1.1, identify the graphs of T_1 through T_5 and the graph of \\ln near x = 0 and near x = 2 . Solution : It's easy to see T_1 is a straight line. Now focus on x \\geq 0 . From part (a), we know T_1(x) > T_3(x) > T_5(x) > \\ln (x) and T_2(x) < T_4(x) < \\ln (x) . Then focus on x < 0 , notice T_{n}(x) - T_{n+1}(x) = (-1)^{n+1} \\frac{x^{n+1}}{n+1} No matter of n , it is always positive, so T_1(x) > T_2(x) > \\cdots > T_5(x) > \\ln (x) . \\square","title":"1.3.3."},{"location":"ch01ex/#134","text":"Working by hand, use the third-degree Taylor polynomial for \\sin(x) at 0 to approximate a decimal representation of \\sin(0.1) . Also compute the decimal representation of an upper bound for the error of the approximation. Bound \\sin(0.1) between two decimal representations. Solution : T_3(x) = \\frac{x}{1} - \\frac{x^3}{3!} And also |R_3(x)| \\leq \\frac{x^4}{4!} T_3(0.1) = 0.09983333 \\\\ |R_3(0.1)| \\leq \\frac{0.1^4}{4!} = 0.00000417 \\\\ 0.09982916 \\leq \\sin (0.1) \\leq 0.0998375 With calculator, \\sin (0.1) \\approx 0.09983342 .","title":"1.3.4."},{"location":"ch01ex/#135","text":"Use a second-degree Taylor polynomial to approximate \\sqrt[]{4.2} . Use Taylor\u2019s theorem to find a guaranteed accuracy of the approximation and thus to find upper and lower bounds for \\sqrt[]{4.2} . Solution : Consider f(x) = \\sqrt[]{4+x} \\begin{split} f'(x) &= \\frac{1}{2} (4+x)^{-\\frac{1}{2}} \\\\ f''(x) &= -\\frac{1}{2^2} (4+x) ^{-\\frac{3}{2}} \\\\ f^{(3)}(x) &= \\frac{1 \\cdot 3}{2^3} (4+x) ^{-\\frac{5}{2}} \\\\ \\end{split} So we have T_2(x) = f(0) + f'(0) x + \\frac{f''(0)}{2!}x^2 \\\\ = 2 + \\frac{x}{4} - \\frac{x^2}{64} The remainder is: |R_2(x)| = \\left| \\frac{f^{(3)}(c)}{3!}x^3 \\right| \\leq \\frac{f^{(3)}(0)}{3!}x^3 = \\frac{3x^3}{3!2^8} = \\frac{x^3}{512} Then we have T_2(0.2) = 2.049375 \\\\ |R_2(0.2)| = \\frac{0.2^3}{512} = 0.00001562 \\\\ \\sqrt[]{4.2} = 2.04939015 \\\\ Finally, we have 2.04935938 = T_2(0.2) - |R_2(0.2)| < \\sqrt[]{4.2} < T_2(0.2) + |R_2(0.2)| = 2.04939062 \\square","title":"1.3.5."},{"location":"ch01ex/#136","text":"(a) Another proof of Taylor\u2019s Theorem uses the fundamental theorem of integral calculus once and then integrates by parts repeatedly. Begin with the hypotheses of Theorem 1.3.3, and let x \u2208 I . By the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(t) dt Let u = f'(t) and v = t - x , so that the integral is \\int_{a}^{x} u v' , and integrating by parts gives \\int_{a}^{x} u v' = uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v = f'(x)(x - x) - f'(a) (a - x) - \\int_{a}^{x} f''(t)(t-x) \\\\ = f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt So f(x) = f(a) + f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt Now let u = f''(t) and v = \\frac{1}{2} (t - x)^2 , \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f''(x) \\frac{1}{2} (x-x)^2 - f''(a) \\frac{1}{2}(a - x)^2 - \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt \\end{split} So f(x) = f(a) + f'(a) (x - a) + f''(a) \\frac{(x-a)^2}{2} + \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt Then we can use induction and assume f(x) = T_n(x) + (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt Let u = f^{(n+1)}(t), v = \\frac{(t-x)^{(n+1)}}{(n+1)!} \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f^{(n+1)}(x) \\frac{(x-x)^{n+1}}{{(n+1)}!} - f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} - \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\\\ &= (-1) \\cdot \\left( f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} + \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\right) \\end{split} No mattern n is even or odd, (-1)^{n} (a-x)^n = (x-a)^n So f(x) = T_{n+1} + (-1)^{n+1} \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} dt Whereas the expression for f(x)\u2212T_n(x) in Theorem 1.3.3 is called the Lagrange form of the remainder, this exercise has derived the integral form of the remainder. Use the extreme value theorem and the intermediate value theorem to derive the Lagrange form of the remainder from the integral form. Proof : We want to estimate A = (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} Assume m \\leq f^{(n+1)}(t) \\leq M , and let B = (-1)^n \\int_{a}^{x} \\frac{(t-x)^n}{n!} dt = (-1)^n \\frac{(t-x)^{(n+1)}}{{(n+1)}!}\\Bigg|_{a}^{x} = (-1)^{n+1} \\frac{(a-x)^{(n+1)}}{{(n+1)}!} = \\frac{(x-a)^{(n+1)}}{{(n+1)}!} Note (-1)^n \\frac{(t-x)^n}{n!} \\geq 0, t \\in [a, x] , so m \\frac{(x-a)^{(n+1)}}{{(n+1)}!} \\leq (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} \\leq M \\frac{(x-a)^{(n+1)}}{{(n+1)}!} i.e. mB \\leq A \\leq MB Since f^{(n+1)} is continuous, with extreme value theorem, we can find f^{(n+1)}(t_m) = m, f^{(n+1)}(t_M) = M . Then with intermediate value theorem, we can find c , such that f^{(n+1)}(c) = A/B , i.e. (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1} \\square (b) Use the integral form of the remainder to show that \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] Proof : f^{(n+1)}(t) = \\frac{(-1)^n n!}{(1+t)^{n+1}} So \\begin{split} (-1)^n \\int_{0}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt &= (-1)^{2n} \\int_{0}^{x} \\frac{(-1)^n n!}{(1+t)^{n+1}} \\frac{(t-x)^n}{n!} dt \\\\ &= \\int_{0}^{x} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\end{split} Consider, x \\in (-1, -1/2] , and t \\geq x . t \\geq x > (2+t)x \\\\ \\Rightarrow \\\\ t > (2+t)x \\\\ \\Rightarrow \\\\ t - x > x + tx \\\\ \\Rightarrow \\\\ \\frac{t-x}{1+t} > x \\\\ So \\left| \\frac{t-x}{1+t} \\right| < |x| Then \\begin{split} \\left| \\int_{x}^{0} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\right| & \\leq \\int_{x}^{0} \\left| \\frac{(t-x)^n}{(1+t)^{n+1}} \\right| dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} \\left| \\frac{(t-x)}{(1+t)} \\right|^n dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} |x|^n dt\\\\ &= \\frac{|x|^{n+1}}{1+x} \\to 0 \\end{split} The book has proved the case for (-1/2, 1] . Then we can conclude \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] \\square","title":"1.3.6."},{"location":"ch01notes/","text":"Chapter 1 Results from One-Variable Calculus 1.1 The Real Number System Theorem 1.1.1 (Field axioms for ( R,+,\u00b7 )). (a1) Addition is associative (a2) 0 is an additive identity (a3) Existence of additive inverses (a4) Addition is commutative (m1) Multiplication is associative (m2) 1 is a multiplicative identity (m3) Existence of multiplicative inverses (m4) Multiplication is commutative (d1) Multiplication distributes over addition Theorem 1.1.2 (Order axioms). (o1) Trichotomy axiom: for every real number x , exactly one of the following conditions holds: x \\in \\mathbb{R}^+, x \\in \\mathbb{R}^-, x = 0. \\square (o2) Closure of positive numbers under addition: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also x+y \\in \\mathbb{R}^+ . \\square (o3) Closure of positive numbers under multiplication: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also xy \\in \\mathbb{R}^+ . \\square For all real numbers x and y , define x < y to mean y - x \\in \\mathbb{R}^+ We want to show this definition is the same as what we learned in \"Understanding Analysis\". \\Rightarrow We define this relation, if x < y of x = y then we say x \\leq y . (o1ua) Given any x, y , since either x - y \\in \\mathbb{R}^+, x - y \\in \\mathbb{R}^-, x - y = 0. Then either x \\leq y or y \\leq x . (o2ua) If x \\leq y , then x - y \\leq 0 . y \\leq x , then y - x \\leq 0 . So x = y . (o3ua) If x \\leq y , then x - y \\leq 0 . y - z \\leq 0 , then (x-y) + (y-z) = x - z \\leq 0 , i.e. x \\leq z . (o4ua) If y \\leq z , y - z \\leq 0 , then x + (y-z) \\leq x . so x + y \\leq x + z . (o5ua) It's automatic. \\Leftarrow We define the set to be \\mathbb{F}^+ = \\{ x : 0 \\leq x \\text{ and } x \\neq 0 \\} We want to showthe following: (o1) We want to show if x \\not\\in \\mathbb{F}^+ and x \\neq 0 , then -x \\in \\mathbb{F}^+ . Assume -x \\leq 0 , then -x + x \\leq 0 + x , so 0 \\leq x , we have a contradiction. So 0 \\leq -x . Since x \\neq 0 , then if -x = 0 , we have 0 = x + (-x) = x , we have a contradiction again, so -x \\neq 0 . Then x \\in \\mathbb{F}^+ . (o2) We want to show if x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also x+y \\in \\mathbb{F}^+ . 0 \\leq x, 0 \\leq y , then 0 + 0 \\leq x + 0 \\leq x + y . In addition, if x + y = 0 , then x = -y , since y \\in \\mathbb{F}^+ , then -y \\not\\in \\mathbb{F}^+ , i.e. x \\not\\in \\mathbb{F}^+ , we have a contradiction. (o3) We want to show x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also xy \\in \\mathbb{F}^+ . (o5ua) guarantees 0 \\leq x, 0 \\leq y then 0 \\leq xy . If xy = 0 , since y \\neq 0 , we can find y^{-1} such that x = xyy^{-1} = 0y^{-1} = 0 , we have a contradiction. \\square Another thing we can show is that no order can be made to a finite field. Proof : Assume we \\mathbb{F} is a finite field. We proved 1 \\in \\mathbb{F}^+ . Then consider the sequence 1,2,3,\\cdots Since \\mathbb{F} is finite, we must have for some k,n k = k+n , then we have n = 0 . On the other hand, n is the sum of n 1 s, so n \\in \\mathbb{F}^+ . We have a contradiction. \\square 1.3 Taylor\u2019s Theorem Let I \u2282 \\mathbb{R} be a nonempty open interval, and let a \u2208 I be any point. Let n be a nonnegative integer. Suppose that the function f : I \\to \\mathbb{R} has n continuous derivatives f, f', f'', \\cdots, f^{(n)} : I \\to \\mathbb{R} For every positive integer k and every x \u2208 \\mathbb{R} define a k-fold nested integral, I_k(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_k = a}^{x_{k-1}} d x_k \\cdots d x_2 d x_1 Then we have I_1(x) = \\int_{x_1 = a}^{x} d x_1 = x_1 \\Big|_{x_1 = a}^x = x - a I_2(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} d x_2 d x_1 = \\int_{x_1 = a}^{x} I_1(x_1) d x_1 = \\frac{1}{2} (x_1 - a)^2 \\Big|_{x_1 = a}^x = \\frac{1}{2} (x - a)^2 So in general I_k(x) = \\frac{1}{k!} (x - a)^k, k \\in \\mathbb{Z}^+. According to the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ f'(x_1) = f'(a) + \\int_{a}^{x_1} f''(x_2) d x_2 \\\\ So we have \\begin{split} f(x) &= T_0(x) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ &= T_0(x) + f'(a) I_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &= T_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &\\cdots \\\\ &= T_n(x) + \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\cdots d x_2 d x_1 \\\\ &= T_n(x) + R_n(x) \\end{split} Assume m \\leq f^{(n+1)}(x_{n+1}) \\leq M , then m I_1(x_n) \\leq \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\leq M I_1(x_n) and m I_2(x_{n-1}) \\leq \\int_{x_{n} = a}^{x_{n-1}} \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} d x_n \\leq M I_2(x_{n-1}) and so on. Eventually, we have m I_{n+1}(x) \\leq R_n(x) \\leq M I_{n+1}(x) i.e. m \\frac{(x-a)^{n+1}}{(n+1)!} \\leq R_n(x) \\leq M \\frac{(x-a)^{n+1}}{(n+1)!} If we let g(t) = f^{(n+1)}(t) \\frac{(x-a)^{n+1}}{(n+1)!}, t \\in [a, x] Then we have g(t_m) = m \\frac{(x-a)^{n+1}}{(n+1)!}, g(t_M) = M \\frac{(x-a)^{n+1}}{(n+1)!} and since g(t) is continuous, we can find c \\in [a, x] , such that g(c) = R_n(x) . \\square Next, we discuss the case for x < a . Consider \\tilde{f} : -I \\to \\mathbb{R} , \\tilde{f}(-x) = f(x) With chain rule, we can see \\tilde{f}^{(n)}(-x) = (-1)^n f^{(n)}(x) If x < a in I then \u2212x >\u2212a in \u2212I , and so we know by the version of Taylor\u2019s theorem that we have already proved that \\tilde{f}(-x) = \\tilde{T}_n(-x) + \\tilde{R}_n(-x) To get the \\tilde{T}_n(-x) , \\tilde{T}_n(-x) = \\sum_{k = 0}^{n} \\frac{\\tilde{f}^{(k)}(-a)}{k!} (-x - (-a))^k \\\\ = \\sum_{k = 0}^{n} (-1)^k \\frac{f^{(k)}(a)}{k!} (a-x)^k \\\\ = \\sum_{k = 0}^{n} \\frac{f^{(k)}(a)}{k!} (x-a)^k \\\\ and also \\tilde{R}_n(-x) = \\frac{\\tilde{f}^{(n+1)}(-c)}{(n+1)!} (-x - (-a))^{n+1} = (-1)^{n+1} \\frac{f^{(n+1)}(c)}{(n+1)!} (a-x)^{n+1} \\\\ = \\frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} So we proved when x < a . \\square Whereas our proof of Taylor\u2019s theorem relies primarily on the fundamental theorem of integral calculus a similar proof relies on repeated integration byparts(Exercise1.3.6) (Also see Jay Cumming's Real Analysis) many proofs rely instead on the mean value theorem. Also see Understanding Analysis. Our proof neatly uses three di\ufb00erent mathematical techniques for the three di\ufb00erent parts of the argument: To find the Taylor polynomial T_n(x) , we di\ufb00erentiated repeatedly, using a substitution at each step to determine a coe\ufb03cient. To get a precise (if unwieldy) expression for the remainder R_n(x) = f(x)\u2212 T_n(x) , we integrated repeatedly, usingthefundamental theorem of integral calculus at each step to produce a term of the Taylor polynomial. To express the remainder in a more convenient form, we used the extreme value theorem and then the intermediate value theorem once each. These foundational theorems are not results from calculus but (as we will discuss in Section 2.4) from an area of mathematics called topology.","title":"Chapter 01 Notes"},{"location":"ch01notes/#chapter-1-results-from-one-variable-calculus","text":"","title":"Chapter 1 Results from One-Variable Calculus"},{"location":"ch01notes/#11-the-real-number-system","text":"","title":"1.1 The Real Number System"},{"location":"ch01notes/#theorem-111-field-axioms-for-r","text":"(a1) Addition is associative (a2) 0 is an additive identity (a3) Existence of additive inverses (a4) Addition is commutative (m1) Multiplication is associative (m2) 1 is a multiplicative identity (m3) Existence of multiplicative inverses (m4) Multiplication is commutative (d1) Multiplication distributes over addition","title":"Theorem 1.1.1 (Field axioms for (R,+,\u00b7))."},{"location":"ch01notes/#theorem-112-order-axioms","text":"(o1) Trichotomy axiom: for every real number x , exactly one of the following conditions holds: x \\in \\mathbb{R}^+, x \\in \\mathbb{R}^-, x = 0. \\square (o2) Closure of positive numbers under addition: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also x+y \\in \\mathbb{R}^+ . \\square (o3) Closure of positive numbers under multiplication: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also xy \\in \\mathbb{R}^+ . \\square For all real numbers x and y , define x < y to mean y - x \\in \\mathbb{R}^+ We want to show this definition is the same as what we learned in \"Understanding Analysis\". \\Rightarrow We define this relation, if x < y of x = y then we say x \\leq y . (o1ua) Given any x, y , since either x - y \\in \\mathbb{R}^+, x - y \\in \\mathbb{R}^-, x - y = 0. Then either x \\leq y or y \\leq x . (o2ua) If x \\leq y , then x - y \\leq 0 . y \\leq x , then y - x \\leq 0 . So x = y . (o3ua) If x \\leq y , then x - y \\leq 0 . y - z \\leq 0 , then (x-y) + (y-z) = x - z \\leq 0 , i.e. x \\leq z . (o4ua) If y \\leq z , y - z \\leq 0 , then x + (y-z) \\leq x . so x + y \\leq x + z . (o5ua) It's automatic. \\Leftarrow We define the set to be \\mathbb{F}^+ = \\{ x : 0 \\leq x \\text{ and } x \\neq 0 \\} We want to showthe following: (o1) We want to show if x \\not\\in \\mathbb{F}^+ and x \\neq 0 , then -x \\in \\mathbb{F}^+ . Assume -x \\leq 0 , then -x + x \\leq 0 + x , so 0 \\leq x , we have a contradiction. So 0 \\leq -x . Since x \\neq 0 , then if -x = 0 , we have 0 = x + (-x) = x , we have a contradiction again, so -x \\neq 0 . Then x \\in \\mathbb{F}^+ . (o2) We want to show if x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also x+y \\in \\mathbb{F}^+ . 0 \\leq x, 0 \\leq y , then 0 + 0 \\leq x + 0 \\leq x + y . In addition, if x + y = 0 , then x = -y , since y \\in \\mathbb{F}^+ , then -y \\not\\in \\mathbb{F}^+ , i.e. x \\not\\in \\mathbb{F}^+ , we have a contradiction. (o3) We want to show x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also xy \\in \\mathbb{F}^+ . (o5ua) guarantees 0 \\leq x, 0 \\leq y then 0 \\leq xy . If xy = 0 , since y \\neq 0 , we can find y^{-1} such that x = xyy^{-1} = 0y^{-1} = 0 , we have a contradiction. \\square Another thing we can show is that no order can be made to a finite field. Proof : Assume we \\mathbb{F} is a finite field. We proved 1 \\in \\mathbb{F}^+ . Then consider the sequence 1,2,3,\\cdots Since \\mathbb{F} is finite, we must have for some k,n k = k+n , then we have n = 0 . On the other hand, n is the sum of n 1 s, so n \\in \\mathbb{F}^+ . We have a contradiction. \\square","title":"Theorem 1.1.2 (Order axioms)."},{"location":"ch01notes/#13-taylors-theorem","text":"Let I \u2282 \\mathbb{R} be a nonempty open interval, and let a \u2208 I be any point. Let n be a nonnegative integer. Suppose that the function f : I \\to \\mathbb{R} has n continuous derivatives f, f', f'', \\cdots, f^{(n)} : I \\to \\mathbb{R} For every positive integer k and every x \u2208 \\mathbb{R} define a k-fold nested integral, I_k(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_k = a}^{x_{k-1}} d x_k \\cdots d x_2 d x_1 Then we have I_1(x) = \\int_{x_1 = a}^{x} d x_1 = x_1 \\Big|_{x_1 = a}^x = x - a I_2(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} d x_2 d x_1 = \\int_{x_1 = a}^{x} I_1(x_1) d x_1 = \\frac{1}{2} (x_1 - a)^2 \\Big|_{x_1 = a}^x = \\frac{1}{2} (x - a)^2 So in general I_k(x) = \\frac{1}{k!} (x - a)^k, k \\in \\mathbb{Z}^+. According to the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ f'(x_1) = f'(a) + \\int_{a}^{x_1} f''(x_2) d x_2 \\\\ So we have \\begin{split} f(x) &= T_0(x) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ &= T_0(x) + f'(a) I_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &= T_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &\\cdots \\\\ &= T_n(x) + \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\cdots d x_2 d x_1 \\\\ &= T_n(x) + R_n(x) \\end{split} Assume m \\leq f^{(n+1)}(x_{n+1}) \\leq M , then m I_1(x_n) \\leq \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\leq M I_1(x_n) and m I_2(x_{n-1}) \\leq \\int_{x_{n} = a}^{x_{n-1}} \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} d x_n \\leq M I_2(x_{n-1}) and so on. Eventually, we have m I_{n+1}(x) \\leq R_n(x) \\leq M I_{n+1}(x) i.e. m \\frac{(x-a)^{n+1}}{(n+1)!} \\leq R_n(x) \\leq M \\frac{(x-a)^{n+1}}{(n+1)!} If we let g(t) = f^{(n+1)}(t) \\frac{(x-a)^{n+1}}{(n+1)!}, t \\in [a, x] Then we have g(t_m) = m \\frac{(x-a)^{n+1}}{(n+1)!}, g(t_M) = M \\frac{(x-a)^{n+1}}{(n+1)!} and since g(t) is continuous, we can find c \\in [a, x] , such that g(c) = R_n(x) . \\square Next, we discuss the case for x < a . Consider \\tilde{f} : -I \\to \\mathbb{R} , \\tilde{f}(-x) = f(x) With chain rule, we can see \\tilde{f}^{(n)}(-x) = (-1)^n f^{(n)}(x) If x < a in I then \u2212x >\u2212a in \u2212I , and so we know by the version of Taylor\u2019s theorem that we have already proved that \\tilde{f}(-x) = \\tilde{T}_n(-x) + \\tilde{R}_n(-x) To get the \\tilde{T}_n(-x) , \\tilde{T}_n(-x) = \\sum_{k = 0}^{n} \\frac{\\tilde{f}^{(k)}(-a)}{k!} (-x - (-a))^k \\\\ = \\sum_{k = 0}^{n} (-1)^k \\frac{f^{(k)}(a)}{k!} (a-x)^k \\\\ = \\sum_{k = 0}^{n} \\frac{f^{(k)}(a)}{k!} (x-a)^k \\\\ and also \\tilde{R}_n(-x) = \\frac{\\tilde{f}^{(n+1)}(-c)}{(n+1)!} (-x - (-a))^{n+1} = (-1)^{n+1} \\frac{f^{(n+1)}(c)}{(n+1)!} (a-x)^{n+1} \\\\ = \\frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} So we proved when x < a . \\square Whereas our proof of Taylor\u2019s theorem relies primarily on the fundamental theorem of integral calculus a similar proof relies on repeated integration byparts(Exercise1.3.6) (Also see Jay Cumming's Real Analysis) many proofs rely instead on the mean value theorem. Also see Understanding Analysis. Our proof neatly uses three di\ufb00erent mathematical techniques for the three di\ufb00erent parts of the argument: To find the Taylor polynomial T_n(x) , we di\ufb00erentiated repeatedly, using a substitution at each step to determine a coe\ufb03cient. To get a precise (if unwieldy) expression for the remainder R_n(x) = f(x)\u2212 T_n(x) , we integrated repeatedly, usingthefundamental theorem of integral calculus at each step to produce a term of the Taylor polynomial. To express the remainder in a more convenient form, we used the extreme value theorem and then the intermediate value theorem once each. These foundational theorems are not results from calculus but (as we will discuss in Section 2.4) from an area of mathematics called topology.","title":"1.3 Taylor\u2019s Theorem"},{"location":"ch02ex/","text":"Chapter 2 Euclidean Space 2.1 Algebra: Vectors 2.1.1. Write down any three specific nonzero vectors u , v , w from \\mathbb{R}^3 and any two specific nonzero scalars a, b from \\mathbb{R} . Compute u+v, aw, b(v+w), (a+b)u , u+v +w, abw , and the additive inverse to u . Solution : Let u = (1,0,0) \\\\ v = (0,1,0) \\\\ w = (0,0,1) \\\\ a = 1, b = -1 \\\\ u + v = (1, 1, 0) \\\\ aw = w = (0,0,1) \\\\ b(v + w) = (0,-1,-1) \\\\ (a+b)u = 0 \\\\ u+v+w = (1,1,1) \\\\ ab(w) = (0,0,-1) \\\\ 2.1.2. Working in \\mathbb{R}^2 , give a geometric proof that if we view the vectors x and y as arrows from 0 and form the parallelogram P with these arrows as two of its sides, then the diagonal z starting at 0 is the vector sum x + y viewed as an arrow. Proof : See the figure below. 2.1.3. Verify that \\mathbb{R}^n satisfies vector space axioms (A2), (A3), (D1). Proof : (A2) 0 is an additive identity 0 + x = (0, \\cdots, 0) + (x_1, \\cdots, x_n) \\\\ = (0 + x_1, \\cdots, 0 + x_n) \\\\ = (x_1, \\cdots, x_n) \\\\ = x \\square (A3) Existence of additive inverses Let x = (x_1, \\cdots, x_n) and y = (-x_1, \\cdots, -x_n) y + x = (-x_1 + x_1, \\cdots, -x_n + x_n) \\\\ = (0, \\cdots, 0) So y is the additive inverse of x . \\square (D1) Scalar multiplication distributes over scalar addition (a+b)x = (a+b) (x_1, \\cdots, x_n) \\\\ = ((a+b)x_1, \\cdots, (a+b)x_n) \\\\ = (ax_1 + bx_1, \\cdots, ax_n + bx_n) \\\\ = (ax_1, \\cdots, ax_n) + (bx_1, \\cdots, bx_n) \\\\ = ax + bx \\square 2.1.4. Are all the field axioms used in verifying that Euclidean space satisfies the vector space axioms? Solution : Let's check the vector space axioms one by one. (A1) Addition is associative uses (Fa1) (A2) 0 is an additive identity uses (Fa2) (A3) Existence of additive inverses (Fa3) (A4) Addition is commutative (Fa4) (M1) Scalar multiplication is associative (Fm1) (M2) 1 is a multiplicative identity (Fm2) (D1) Scalar multiplication distributes over scalar addition (Fd1) (D2) Scalar multiplication distributes over vector addition (Fd1) So we can see (Fm3) and (Fm4) are not used. \\square 2.1.5. Show that 0 is the unique additive identity in \\mathbb{R}^n . Show that each vector x \u2208 \\mathbb{R}^n has a unique additive inverse, which can therefore be denoted \u2212x . (And it follows that vector subtraction can now be defined, - : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^n, \\quad x - y = x + (-y) for all x, y \\in \\mathbb{R}^n . Show that 0x = 0 for all x \u2208 \\mathbb{R}^n . Proof : Assume x = (x_1, \\cdots, x_n) is a additive identity and y = (y_1, \\cdots, y_n) \\in \\mathbb{R}^n . Then x + y = y , i.e. x_1 + y_1 = y_1, \\cdots , x_n + y_n = y_n So x_1 = \\cdots = x_n = 0 , so x = 0 . And thus 0 is unique. Assume x = (x_1, \\cdots, x_n) and y (y_1, \\cdots, y_n) \\in \\mathbb{R}^n is x 's additive inverse. x + y = (x_1 + y_1, \\cdots, x_n + y_n) = 0 So y = (-x_1, \\cdots, -x_n) The additive inverse is unique. 0x = 0 (x_1, \\cdots, x_n) = (0x_1, \\cdots, 0x_n) = (0, \\cdots, 0) \\square 2.1.6. Repeat the previous exercise, but with \\mathbb{R}^n replaced by an arbitrary vector space V over a field F . (Work with the axioms.) 0 = 0' + 0 = 0 + 0' = 0' So 0 is unique. y = y + 0 = y + (x + y') = (y + x) + y' = 0 + y' = y' So the additive inverse is unique. 0x + 1x = (0 + 1) x = 1 x = x Then we can add -x on both side 0x + x + (-x) = 0x \\\\ \\Rightarrow \\\\ x + (-x) = 0 \\\\ \\Rightarrow \\\\ 0x = 0 \\square 2.1.7. Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.) Proof : I cannot figure it for now. The uniqueness of the additive identity. Assume 0 and 0' are 2 additive identities. Then 0' + 0 = 0 \\\\ 0 + 0' = 0' \\\\ 0' + 0' = 0' \\\\ 2.1.8. Let x and y be noncollinear vectors in \\mathbb{R}^3 . Give a geometric description of the set of all linear combinations of x and y . Solution : All the linear combinations of x and y form a plane that includes 0, x, y . \\square 2.1.9 Which of the following sets are bases of \\mathbb{R}^3 ? S_1 = \\{(1,0,0),(1,1,0),(1,1,1)\\} \\\\ S_2 = \\{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\\} \\\\ S_3 = \\{(1,1,0),(0,1,1)\\} \\\\ S_4 = \\{(1,1,0),(0,1,1),(1,0,-1)\\} \\\\ How many elements do you think a basis for \\mathbb{R}^n must have? Give (without proof) geometric descriptions of all bases of \\mathbb{R}^2 , of \\mathbb{R}^3 . Solution : For S_1 , given (x,y,z) , we have \\begin{cases} x &= a + b + c\\\\ y &= b + c \\\\ z &= c \\\\ \\end{cases} Then we can have \\begin{cases} a = x - y\\\\ b = y - z\\\\ c = z\\\\ \\end{cases} So S_1 is a base. For S_2 , it is note a base. For example (1,1,1) can be expressed by multiple ways. For S_3 \\begin{cases} x &= a \\\\ y &= a + b \\\\ z &= b \\\\ \\end{cases} Then we cannot represent (1,3,1) with S_3 . For S_4 , We have \\begin{cases} x &= a + c\\\\ y &= a + b \\\\ z &= b - c\\\\ \\end{cases} Then we have to have z = y - x , so we cannot represent (1,1,1) with S_4 . For \\mathbb{R}^n , it needs n element for a basis. For \\mathbb{R}^2 , if \\{f_1, f_2\\} is a basis, then geometrically, they should not be in one line. For \\mathbb{R}^3 , if \\{f_1, f_2, f_3\\} is a basis, then geometrically, they should not be in the same plane. \\square 2.1.10. Recall the field \\mathbb{C} of complex numbers. Define complex n -space \\mathbb{C}^n analogously to \\mathbb{R}^n : \\mathbb{C}^n = \\{ (z_1, \\cdots, z_n) : z_i \\in \\mathbb{C} \\text{ for } i = 1, \\cdots, n \\} and endow it with addition and scalar multiplication defined by the same formulas as for \\mathbb{R}^n . You may take for granted that under these definitions, \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{R} , and also \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{C} . That is, using language that was introduced briefly in this section, \\mathbb{C}^n can be viewed as a vector space over \\mathbb{R} and also, separately, as a vector space over \\mathbb{C} . Give a basis for each of these vector spaces. Solution : For the first one S_1 = \\{ (1,\\cdots,0), (i,\\cdots,0), \\cdots (0,\\cdots,1), (0,\\cdots,i) \\} For the second one S_2 = \\{ (1,\\cdots,0), \\cdots (0,\\cdots,1) \\} \\square 2.2 Geometry: Length and Angle 2.2.1 Let x = (\\frac{\\sqrt[]{3}}{2}, -\\frac{1}{2}, 0), y = (\\frac{1}{2}, \\frac{\\sqrt[]{3}}{2}, 1), z = (1, 1, 1) Compute \u27e8x,x\u27e9, \u27e8x,y\u27e9, \u27e8y,z\u27e9, |x|, |y|, |z|, \u03b8_{x,y}, \u03b8_{y,e_1}, \u03b8_{z,e_2} Solution : \u27e8x,x\u27e9 = \\frac{3}{4} + \\frac{1}{4} + 0 = 1 \\\\ \u27e8x,y\u27e9 = \\frac{\\sqrt[]{3}}{4} - \\frac{\\sqrt[]{3}}{4} + 0 = 0 \\\\ \u27e8y,z\u27e9 = \\frac{1}{2} + \\frac{\\sqrt[]{3}}{2} + 1 = \\frac{3 + \\sqrt[]{3}}{2} \\\\ |x| = 1 \\\\ |y| = \\sqrt[]{2} \\\\ |z| = \\sqrt[]{3} \\\\ \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x| |y|} = \\frac{0}{|x| |y|} = 0 \\\\ \u03b8_{y,e_1} = \\frac{\u27e8y,e_1\u27e9}{|y| |e_1|} = \\frac{1/2}{|x| |y|} = \\frac{1}{2 \\sqrt[]{2}} \\\\ \u03b8_{z,e_2} = \\frac{1}{\\sqrt[]{3}} \\square 2.2.2. Show that the points x = (2,\u22121,3,1), y = (4,2,1,4), z = (1,3,6,1) form the vertices of a triangle in \\mathbb{R}^4 with two equal angles. Solution : d(x,y) = |x - y| = \\sqrt[]{\u27e8x-y,x-y\u27e9} = \\sqrt[]{\u27e8(-2,-3,2,-3),(-2,-3,2,-3)\u27e9} = \\sqrt[]{4+9+4+9} = \\sqrt[]{26} \\\\ d(y,z) = |y-z| = \\sqrt[]{\u27e8y-z,y-z\u27e9} = \\sqrt[]{\u27e8(3,-1,-5,3),(3,-1,-5,3)\u27e9} = \\sqrt[]{9+1+25+9} = \\sqrt[]{44} \\\\ d(z,x) = |z-x| = \\sqrt[]{\u27e8z-x,z-x\u27e9} = \\sqrt[]{\u27e8(-1,4,3,0),(-1,4,3,0)\u27e9} = \\sqrt[]{1+16+9+0} = \\sqrt[]{26} \\\\ \\square 2.2.3. Explain why for all x \u2208 \\mathbb{R}^n , x = \\sum_{j=1}^{n} \u27e8x,e_j\u27e9e_j . Proof : Note \u27e8x,e_j\u27e9 = \u27e8(x_1, \\cdots, x_n),(0, \\cdots, 1, \\cdots, 0)\u27e9 = x_j \\\\ x_j e_j = (0, \\cdots, x_j, \\cdots, 0) \\\\ \\sum_{j=1}^{n} x_j e_j = (x_1, \\cdots, x_n) = x \\square 2.2.4. Prove the inner product properties. Proof : (IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 = x_1^2 + \\cdots + x_n^2 \\geq 0 It equals 0 only when x_1 = \\cdots = x_n = 0 . \\square (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n \u27e8x,y\u27e9 = x_1 y_1 + \\cdots + x_n y_n = y_1 x_1 + \\cdots + y_n x_n = \u27e8y,x\u27e9 \\square (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = (x_1 + x'_1) y_1 + \\cdots + (x_n + x'_n) y_n \\\\ = (x_1 y_1 + \\cdots + x_n y_n) + (x'_1 y_1 + \\cdots + x'_n y_n) \\\\ = \u27e8x,y\u27e9 + \u27e8x',y\u27e9 \u27e8ax,y\u27e9 = (ax_1) y_1 + \\cdots + (ax_n) y_n \\\\ = a (x_1 y_1 + \\cdots + x_n y_n) \\\\ = a \u27e8x,y\u27e9 \\square 2.2.5. Use the inner product properties and the definition of the modulus in terms of the inner product to prove the modulus properties. Proof : (Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ |x| = \\sqrt[]{\u27e8x,x\u27e9} \\geq 0 \u27e8x,x\u27e9 = 0 if and only if x = 0 . \\square (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n . \u27e8ax,ax\u27e9 = a \u27e8x,ax\u27e9 = a^2 \u27e8x,x\u27e9 So |ax| = \\sqrt[]{\u27e8ax,ax\u27e9} = \\sqrt[]{a^2 \u27e8x,x\u27e9} = |a||x| \\square 2.2.6. In the text, the modulus is defined in terms of the inner product. Prove that this can be turned around by showing that for every x,y \u2208 \\mathbb{R}^n , \u27e8x,y\u27e9 = \\frac{|x+y|^2 - |x-y|^2}{4}. Proof : |x+y|^2 = \u27e8x+y,x+y\u27e9 = \u27e8x,x\u27e9 + 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ |x-y|^2 = \u27e8x-y,x-y\u27e9 = \u27e8x,x\u27e9 - 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ So the equation holds. \\square 2.2.7. Prove the full triangle inequality: for every x,y \u2208 \\mathbb{R}^n , ||x| - |y|| \\leq |x \\pm y | \\leq |x| + |y| Do not do this by writing three more variants of the proof of the triangle inequality, but by substituting suitably into the basic triangle inequality, which is already proved. Proof : |x - y| \\leq |x| + |y| Note \\begin{split} |x-y| & = |x + (-y)| \\\\ & \\leq |x| + |-y| \\\\ & = |x| + |y| \\\\ \\end{split} 2. ||x| - |y|| \\leq |x + y| Note \\begin{split} |x| &= |(-y) + (x+y)| \\\\ & \\leq |-y| + |x+y| \\\\ & = |y| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |x| - |y| \\leq |x+y| And similarly \\begin{split} |y| &= |(-x) + (x+y)| \\\\ & \\leq |-x| + |x+y| \\\\ & = |x| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |y| - |x| \\leq |x+y| Combine these 2 together, we have ||x| - |y|| \\leq |x + y| 3. ||x| - |y|| \\leq |x - y| This is similar to 2. \\square 2.2.8. Let x = (x_1,...,x_n) \u2208 \\mathbb{R}^n . Prove the size bounds: for every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{j=1}^{n} |x_j|. (One approach is to start by noting that x_j= \u27e8x,e_j\u27e9 and recalling equation (2.1).) When can each \u2264 be an = ? Proof : From the hint: \\begin{split} |x_j| &= |\u27e8x,e_j\u27e9| \\\\ & \\leq |x||e_j| \\\\ & \\text{Cauchy-Schwarz inequality} \\\\ & = |x| \\cdot 1 \\\\ & = |x| \\end{split} This equality holds when x = a \\cdot e_j . Note \\begin{split} |x| &= \\left| \\sum_{j=1}^{n} x_j e_j \\right| \\\\ & \\leq \\sum_{j=1}^{n} \\left| x_j e_j \\right| \\\\ & \\text{Generalized triangle inequality}\\\\ & = \\sum_{j=1}^{n} | x_j | | e_j | \\\\ & \\text{The modulus is absolute-homogeneous}\\\\ & = \\sum_{j=1}^{n} | x_j | \\\\ \\end{split} This equality holds when only only one of x_j is not 0 . \\square 2.2.9. Prove the distance properties. Proof : (D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . Note d(x,y) = |x-y| \\geq 0 The equality holds only when x-y = 0 , i.e. x = y . \\square (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . Note: d(x,y) = |x-y| = |y-x| = d(y,x) \\square \\square (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n . Note: \\begin{split} d(x,z) &= |x-z| \\\\ &= |(x-y)+(y-z)| \\\\ & \\leq |x-y| + |y-z| \\\\ &= d(x,y) + d(y,z) \\end{split} \\square 2.2.10 Working in \\mathbb{R}^2 , depict the nonzero vectors x and y as arrows from the origin and depict x\u2212y as an arrow from the endpoint of y to the endpoint of x . Let \u03b8 denote the angle (in the usual geometric sense) between x and y . Use the law of cosines to show that \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} Proof Geometric proof: \\frac{\u27e8x,y\u27e9}{|x||y|} = \\frac{x_1 y_1 + x_2 y_2}{|x||y|} = \\frac{1}{|x|} \\left( \\frac{x_1 y_1 + x_2 y_2}{|y|} \\right) As shown in the figure below, XD \\perp OY , AC \\perp OY and XB \\perp AC . We just need to show \\frac{x_1 y_1 + x_2 y_2}{|y|} = OD Note \\frac{y_1}{y} = \\cos \\angle AOC \\\\ \\Rightarrow \\\\ \\frac{x_1 y_1}{|y|} = OC \\frac{y_2}{y} = \\cos \\angle AXB \\\\ \\Rightarrow \\\\ \\frac{x_2 y_2}{|y|} = CD Then we finished the proof. Proof with triangle equalities: x_1 = |x| \\cos \u03b8_x, x_2 = |x| \\sin \u03b8_x, y_1 = |y| \\cos \u03b8_y, y_2 = |y| \\sin \u03b8_y, Then \u27e8x,y\u27e9 = |x||y| \\cos \u03b8_x \\cos \u03b8_y + |x||y| \\sin \u03b8_x \\sin \u03b8_y \\\\ = |x||y| \\cos (\u03b8_x - \u03b8_y) = |x||y| \\cos \u03b8 This also proved. \\square 2.2.11. Prove that for every nonzero x \u2208 \\mathbb{R}^n , \\sum_{i = 1}^{n} \\cos ^2 \u03b8_{x, e_i} = 1 . Proof : \\cos ^2 \u03b8_{x, e_i} = \\sum_{i = 1}^{n} \\left( \\frac{\u27e8x,e_i\u27e9}{|x||e_i|} \\right)^2 \\\\ \\sum_{i = 1}^{n} \\frac{x_i^2}{|x|^2} \\\\ = \\frac{1}{|x|^2} \\sum_{i = 1}^{n} x_i^2 \\\\ = 1 \\square 2.2.12. Prove that two nonzero vectors x, y are orthogonal if and only if |x+y|^2 = |x|^2 +|y|^2. Proof : \\begin{split} |x+y|^2 = \u27e8x+y,x+y\u27e9 = |x|^2 + 2 \u27e8x,y\u27e9 + |y|^2 \\end{split} Then |x+y|^2 = |x|^2 +|y|^2 means \u27e8x,y\u27e9=0 . \\square 2.2.13. Use vectors in \\mathbb{R}^2 to show that the diagonals of a parallelogram are perpendicular if and only if the parallelogram is a rhombus. Proof : The 2 diagonals of the parallelogram are x+y, x-y . \u27e8x+y,x-y\u27e9 = |x|^2 - |y|^2 If x+y \\perp x-y , then |x|^2 - |y|^2=0 , i.e. |x| = |y| . On the other hand, if |x| = |y| , then \u27e8x+y,x-y\u27e9 . \\square 2.2.14. Use vectors to show that every angle inscribed in a semicircle is right. Let (x,y) on the semicircle, then x^2+y^2=1 . consider (x,y) - (-1,0) = (x+1,y) and (x,y) - (1,0) = (x-1,y) . Then \u27e8(x+1,y), (x-1, y)\u27e9 = x^2 - 1 + y^2 = 0 \\square 2.2.15. Let x and y be vectors, with x nonzero.Define the parallel component of y along x and the normal component of y to x to be y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x \\qquad \\text{and} \\qquad y_{(\\perp x)} = y - y_{(\\parallel x)} Answer: (a) Show that y = y_{(\\parallel x)} + y_{(\\perp x)} . show that y_{(\\parallel x)} is a scalar multiple of x . show that y_{(\\perp x)} is orthogonal to x . Show that the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. Draw an illustration. Proof : y_{(\\parallel x)} + y_{(\\perp x)} = \\\\ y_{(\\parallel x)} + (y - y_{(\\parallel x)}) = \\\\ y y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x Since \\frac{\u27e8x,y\u27e9}{|x|^2} is a scalar, then y_{(\\parallel x)} is a scalar multiple of x . \u27e8y - y_{(\\parallel x)},y_{(\\parallel x)}) = \\\\ \u27e8|x|^2 y - \u27e8x,y\u27e9x, \u27e8x,y\u27e9x\u27e9 \\\\ = |x|^2 \u27e8x,y\u27e9 \u27e8y,x\u27e9 - \u27e8x,y\u27e9^2 \u27e8x,x\u27e9 \\\\ = 0 So y_{(\\perp x)} is orthogonal to y_{(\\parallel x)} , thus is orthogonal to x . In general if x \\perp y and ax + by = 0, x,y \\neq 0 , then 0 = \u27e80, x\u27e9 = \u27e8ax + by,x\u27e9 = a \u27e8x,x\u27e9 + b \u27e8y,x\u27e9 = a \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ a = 0 For the same reason b = 0 . With this, the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. \\square (b) Show that |y|^2 = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 What theorem from classical geometry does this encompass? Proof : |y|^2 = \\\\ |y_{(\\parallel x)} + y_{(\\perp x)}|^2 = \\\\ |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 + 2 \u27e8y_{(\\parallel x)}, y_{(\\perp x)}\u27e9 \\\\ = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 \\\\ It's Pythagorean theorem or Gougu theorem. \\square (c) Explain why it follows from (b) that |y_{(\\parallel x)}|^2 \\leq |y|^2 with equality if and only if y is a scalar multiple of x . Use this inequality to give another proof of the Cauchy\u2013Schwarz inequality. This argument gives the geometric content of Cauchy\u2013Schwarz: the parallel component of one vector along another is at most as long as the original vector. Proof : This is because |y_{(\\perp x)}|^2 \\geq 0 . The equality holds when |y_{(\\perp x)}|^2 = 0 . In this case y = y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x To prove |\u27e8x,y\u27e9| \\leq |x||y| Note |\u27e8x,y\u27e9| = |\u27e8x,y_{(\\parallel x)}+y_{(\\perp x)}\u27e9| \\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9 + 0|\\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9| Now in general, if a is a scaler, then |\u27e8x,ax\u27e9| = |a \u27e8x,x\u27e9| = |a||x|^2 = |x| |ax| Then |\u27e8x,y_{(\\parallel x)}\u27e9| = |x||y_{(\\parallel x)}|| \\leq |x||y| \\square (d) The proof of the Cauchy\u2013Schwarz inequality in part (c) refers to parts (a) and (b), part (a) refers to orthogonality, orthogonality refers to an angle, and as explained in the text, the fact that angles make sense depends on the Cauchy\u2013Schwarz inequality. And so the proof in part (c) apparently relies on circular logic. Explain why the logic is in fact not circular. Solution : Note that when we define \\cos \u03b8 , it is this: \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} But here, we define y_{(\\parallel x)} as y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2}x If we think geometrically in \\mathbb{R}^2 or \\mathbb{R}^3 , the projection of y on x should be y \\cos \u03b8 . The unit length vector along x is \\frac{x}{|x|} . Also the length of y projection is |y \\cos \u03b8| = \\frac{\u27e8x,y\u27e9}{|x|} So it's reasonable to define y_{(\\parallel x)} this way. Now, since we does not rely on the fact that |\\cos \u03b8| \\leq 1 , it is not circular. \\square 2.2.16. Given nonzero vectors x_1,x_2,...,x_n in \\mathbb{R}^n , the Gram\u2013Schmidt process is to set \\begin{split} x_1' &= x_1 \\\\ x_2' &= x_2 - (x_2)_{(\\parallel x_1')}\\\\ x_3' &= x_3 - (x_3)_{(\\parallel x_2')} - (x_3)_{(\\parallel x_1')} \\\\ \\vdots \\\\ x_n' &= x_n - (x_n)_{(\\parallel x_{n-1}')} - \\cdots - (x_n)_{(\\parallel x_1')} \\\\ \\end{split} Answer: (a)What is the result of applying the Gram\u2013Schmidt process to the vectors x_1 = (1,0,0), x_2 = (1,1,0) , and x_3 = (1,1,1) ? Solution : x_1' = (1,0,0) \\\\ x_2' = (0,1,0) \\\\ x_3' = (0,0,1) \\\\ \\square (b) Returning to the general case, show that x_1',...,x_n' are pairwise orthogonal and that each x_j' has the form x_j'= a_{j,1} x_1 +a_{j,2}x_2 +\u00b7\u00b7\u00b7+a_{j,j\u22121}x_{j\u22121} +x_j. Thus every linear combination of the new {x_j'} is also a linear combination of the original {x_j} . The converse is also true and will be shown in Exercise 3.3.13. Proof : First, we want to prove a small lemma: Given x, y, z , if x \\perp y , then we want to show z_{(\\parallel x)} \\perp y Note \u27e8z_{(\\parallel x)},y\u27e9 = \u27e8\\frac{\u27e8z,x\u27e9}{|x|^2}x,y\u27e9 = \\frac{\u27e8z,x\u27e9}{|x|^2} \u27e8x,y\u27e9 = 0 Then we can use induction. And assume x_1',...,x_{k-1}' are orthogonal to each other. Assume 1 \\leq j \\leq k-1 , \u27e8x_j',x_{k}'\u27e9 = \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 - \\sum_{i \\neq j} \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 = 0 is proved in exercise 2.2.15. Also, since \u27e8x_j',x_i'\u27e9=0 for j \\neq i , then \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 = 0 as just proved in the lemma. So \u27e8x_j',x_{k}'\u27e9 = 0 . For the 2nd part, we can still use induction. And assume for x_1',...,x_{k-1}' , they are all linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Then x_k' = x_k - (x_k)_{(\\parallel x_{k-1}')} - \\cdots - (x_k)_{(\\parallel x_1')} Note (x_k)_{(\\parallel x_j')} is a scaler of x_j', j < k , thus a linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Therefore, x_k' is a linear combination of x_1,...,x_{k} . \\square 2.3 Analysis: Continuous Mappings 2.3.1. For A \u2282 \\mathbb{R}^n , partially verify that \\mathcal{M}(A,\\mathbb{R}^m) is a vector space over \\mathbb{R} by showing that it satisfies vector space axioms (A4) and (D1). Proof : (A4) Addition is commutative \\begin{align*} (f+g)(x) &= f(x) + g(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= g(x) + f(x) \\quad &\\text{by commutativity of \u201c+\u201d in }\\mathbb{R}^m\\\\ &= (g+f)(x) &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m) \\end{align*} Next, (D1) Scalar multiplication distributes over scalar addition \\begin{align*} ((a+b)f)(x) &= (a+b)f(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= af(x) + bf(x) \\quad &\\text{by D1 in }\\mathbb{R}^m\\\\ &= (af)(x) + (bf)(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= ((af) + (bf))(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ \\end{align*} \\square 2.3.2. Define multiplication * : \\mathcal{M}(A, \\mathbb{R}^m) \\times \\mathcal{M}(A, \\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A, \\mathbb{R}^m) Is \\mathcal{M}(A, \\mathbb{R}^m) a field with \u201c+\u201d from the section and this multiplication? Does it have a subspace that is a field? Solution Define: (f * g)(x) = f(x) * g(x) And f(x) = 1 is it's multiplication identity. However, this function does not have inverse. f(x) = \\begin{cases} 0 &\\text{if } x = 0 \\\\ 1 &\\text{if } x \\neq 0 \\\\ \\end{cases} Consider V = \\{f : f(x) = (r, \\cdots, r) \\in \\mathbb{R}^m \\text{ for all } x \\in A\\} This is a field. \\square 2.3.3. For A \u2282 \\mathbb{R}^n and m \u2208 \\mathbb{Z}^+ define a subspace of the space of mappings from A to \\mathbb{R}^m \\mathcal{C}(A, \\mathbb{R}^m) = \\{ f \\in \\mathcal{M}(A, \\mathbb{R}^m) : f \\text{ is continuous on } A \\}. Briefly explain how this section has shown that \\mathcal{C}(A, \\mathbb{R}^m) is a vector space. Solution : It's stated in Proposition 2.3.7 (Vector space properties of continuity). \\square 2.3.4. Define an inner product and a modulus on \\mathcal{C}([0,1], \\mathbb{R}) by $$ \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt, \\quad |f| = \\sqrt[]{\u27e8f,f\u27e9}. $$ Do the inner product properties (IP1),(IP2),and(IP3) (see Proposition2.2.2) hold for this inner product on \\mathcal{C}([0,1], \\mathbb{R}) How much of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) ? Express the Cauchy\u2013Schwarz inequality as a relation between integrals. Proof : (IP1) \u27e8f, f\u27e9 = \\int_{0}^{1} f(t)f(t)dt \\geq \\int_{0}^{1} 0dt = 0 It achieves 0 only when f(t) = 0 . (IP2) \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt = \\int_{0}^{1} g(t)f(t)dt = \u27e8g,f\u27e9 So IP2 holds. (IP3) First, we want to show \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh Based on the definition of integral, give a partition P on [0,1] . L(fg, P) + L(fh, P) \\leq L(fg+fh, P) \\leq U(fg+fh, P) \\leq U(fg, P) + L(fh, P) So we have \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh. Now \u27e8f,g+h\u27e9 = \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh = \u27e8f,g\u27e9 + \u27e8f,h\u27e9 Also \u27e8af,g\u27e9 = \\int_{0}^{1} (af)g = a(\\int_{0}^{1} fg) = a \u27e8f,g\u27e9 \\square All of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) Cauchy\u2013Schwarz inequality \\left| \\int_{0}^{1} f(t)g(t) dt \\right| \\leq \\sqrt[]{\\int_{0}^{1}f^2(t)dt} \\cdot \\sqrt[]{\\int_{0}^{1}g^2(t)dt} \\square 2.3.5 Use the definition of convergence and the componentwise nature of nullness to prove the componentwise nature of convergence. (The argument is short.) Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p = \\text{null} \\\\ \\text{definition of convergence} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j = \\text{null} \\\\ \\text{Lemma 2.3.2 (Componentwise nature of nullness).} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\\\ \\text{definition of convergence} \\\\ \\square 2.3.6. Use the definition of continuity and the componentwise nature of convergence to prove the componentwise nature of continuity. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square 2.3.7 Prove the persistence of continuity under composition. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square 2.3.8. Define f : \\mathbb{Q} \\longrightarrow \\mathbb{R} by the rule f(x) = \\begin{cases} 1 &\\text{if } x^2 < 2,\\\\ 0 &\\text{if } x^2 > 2.\\\\ \\end{cases} Solution : It is continuous. Given any p \\in \\mathbb{Q} , if p^2 < 2 , we can always find a U_{\\delta}(p) such that if q \\in U_{\\delta}(p) , then q^2 < 2 . Same thing when p^2 > 2 . \\square 2.3.9. Which of the following functions on \\mathbb{R}^2 can be defined continuously at 0 ? (a) f(x, y) = \\begin{cases} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution \\begin{split} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &= \\frac{x^2 - y^2}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\begin{split} \\frac{x^2 - y^2}{x^2 + y^2} &= \\frac{1 - m^2}{1 + m^2} \\end{split} So it cannot be defined continuously at 0 . \\square (b) g(x, y) = \\begin{cases} \\frac{x^2 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\frac{x^2 - y^3}{x^2 + y^2} &= 1 - \\frac{y^2 + y^3}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\frac{y^2 + y^3}{x^2 + y^2} \\\\ = \\frac{m^2 + m^3y}{1+m^2} \\rightarrow \\frac{m^2}{1+m^2} So it cannot be defined continuously at 0 . \\square (c) h(x, y) = \\begin{cases} \\frac{x^3 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\left| \\frac{x^3 - y^3}{x^2 + y^2} \\right| &= \\left| (x-y) \\frac{x^2 + xy + y^2}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{xy}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{|(x,y)||(x,y)|}{|(x,y)|^2} \\right| \\\\ & = 2 |x - y| \\\\ & \\leq 2 (|x| + |y|) \\rightarrow 0 \\end{split} So it can be defined continuously at 0. \\square (d) k(x, y) = \\begin{cases} \\frac{xy^2}{x^2 + y^6} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : When (x,y) approaches 0 with y = x , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^3}{x^2 + x^6} = \\frac{x}{1 + x^4} \\rightarrow 0 However, when (x,y) approaches 0 with y = \\sqrt[]{x} , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^2}{x^2 + x^3} = \\frac{1}{1 + x} \\rightarrow 1 So it cannot be defined continuously at 0 . \\square 2.3.10. Let f(x,y) = g(xy) , where g : \\mathbb{R} \u2192 \\mathbb{R} is continuous. Is f continuous? Proof : Let h(x,y) = xy , then h(x,y) is a continuous function. Then from Proposition 2.3.8 (Persistence of continuity under composition), f = g \\circ h . So f is also continuous. \\square 2.3.11. Let f,g \\in \\mathcal{M}(\\mathbb{R}^n, \\mathbb{R}) such that f +g and fg are continuous. Are f and g necessarily continuous? Proof : No, it's not necessary. Consider the case when n = 1 . f(x) = \\begin{cases} 0 &\\text{if } x \\in \\mathbb{Q}\\\\ 1 &\\text{if } x \\in \\mathbb{I}\\\\ \\end{cases} Then g(x) = 1 - f(x) . Then f+g = 1, fg = 0 are continuous. But f, g are not continuous. \\square","title":"Chapter 02 Exercises"},{"location":"ch02ex/#chapter-2-euclidean-space","text":"","title":"Chapter 2 Euclidean Space"},{"location":"ch02ex/#21-algebra-vectors","text":"","title":"2.1 Algebra: Vectors"},{"location":"ch02ex/#211","text":"Write down any three specific nonzero vectors u , v , w from \\mathbb{R}^3 and any two specific nonzero scalars a, b from \\mathbb{R} . Compute u+v, aw, b(v+w), (a+b)u , u+v +w, abw , and the additive inverse to u . Solution : Let u = (1,0,0) \\\\ v = (0,1,0) \\\\ w = (0,0,1) \\\\ a = 1, b = -1 \\\\ u + v = (1, 1, 0) \\\\ aw = w = (0,0,1) \\\\ b(v + w) = (0,-1,-1) \\\\ (a+b)u = 0 \\\\ u+v+w = (1,1,1) \\\\ ab(w) = (0,0,-1) \\\\","title":"2.1.1."},{"location":"ch02ex/#212","text":"Working in \\mathbb{R}^2 , give a geometric proof that if we view the vectors x and y as arrows from 0 and form the parallelogram P with these arrows as two of its sides, then the diagonal z starting at 0 is the vector sum x + y viewed as an arrow. Proof : See the figure below.","title":"2.1.2."},{"location":"ch02ex/#213","text":"Verify that \\mathbb{R}^n satisfies vector space axioms (A2), (A3), (D1). Proof : (A2) 0 is an additive identity 0 + x = (0, \\cdots, 0) + (x_1, \\cdots, x_n) \\\\ = (0 + x_1, \\cdots, 0 + x_n) \\\\ = (x_1, \\cdots, x_n) \\\\ = x \\square (A3) Existence of additive inverses Let x = (x_1, \\cdots, x_n) and y = (-x_1, \\cdots, -x_n) y + x = (-x_1 + x_1, \\cdots, -x_n + x_n) \\\\ = (0, \\cdots, 0) So y is the additive inverse of x . \\square (D1) Scalar multiplication distributes over scalar addition (a+b)x = (a+b) (x_1, \\cdots, x_n) \\\\ = ((a+b)x_1, \\cdots, (a+b)x_n) \\\\ = (ax_1 + bx_1, \\cdots, ax_n + bx_n) \\\\ = (ax_1, \\cdots, ax_n) + (bx_1, \\cdots, bx_n) \\\\ = ax + bx \\square","title":"2.1.3."},{"location":"ch02ex/#214","text":"Are all the field axioms used in verifying that Euclidean space satisfies the vector space axioms? Solution : Let's check the vector space axioms one by one. (A1) Addition is associative uses (Fa1) (A2) 0 is an additive identity uses (Fa2) (A3) Existence of additive inverses (Fa3) (A4) Addition is commutative (Fa4) (M1) Scalar multiplication is associative (Fm1) (M2) 1 is a multiplicative identity (Fm2) (D1) Scalar multiplication distributes over scalar addition (Fd1) (D2) Scalar multiplication distributes over vector addition (Fd1) So we can see (Fm3) and (Fm4) are not used. \\square","title":"2.1.4."},{"location":"ch02ex/#215","text":"Show that 0 is the unique additive identity in \\mathbb{R}^n . Show that each vector x \u2208 \\mathbb{R}^n has a unique additive inverse, which can therefore be denoted \u2212x . (And it follows that vector subtraction can now be defined, - : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^n, \\quad x - y = x + (-y) for all x, y \\in \\mathbb{R}^n . Show that 0x = 0 for all x \u2208 \\mathbb{R}^n . Proof : Assume x = (x_1, \\cdots, x_n) is a additive identity and y = (y_1, \\cdots, y_n) \\in \\mathbb{R}^n . Then x + y = y , i.e. x_1 + y_1 = y_1, \\cdots , x_n + y_n = y_n So x_1 = \\cdots = x_n = 0 , so x = 0 . And thus 0 is unique. Assume x = (x_1, \\cdots, x_n) and y (y_1, \\cdots, y_n) \\in \\mathbb{R}^n is x 's additive inverse. x + y = (x_1 + y_1, \\cdots, x_n + y_n) = 0 So y = (-x_1, \\cdots, -x_n) The additive inverse is unique. 0x = 0 (x_1, \\cdots, x_n) = (0x_1, \\cdots, 0x_n) = (0, \\cdots, 0) \\square","title":"2.1.5."},{"location":"ch02ex/#216","text":"Repeat the previous exercise, but with \\mathbb{R}^n replaced by an arbitrary vector space V over a field F . (Work with the axioms.) 0 = 0' + 0 = 0 + 0' = 0' So 0 is unique. y = y + 0 = y + (x + y') = (y + x) + y' = 0 + y' = y' So the additive inverse is unique. 0x + 1x = (0 + 1) x = 1 x = x Then we can add -x on both side 0x + x + (-x) = 0x \\\\ \\Rightarrow \\\\ x + (-x) = 0 \\\\ \\Rightarrow \\\\ 0x = 0 \\square","title":"2.1.6."},{"location":"ch02ex/#217","text":"Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.) Proof : I cannot figure it for now. The uniqueness of the additive identity. Assume 0 and 0' are 2 additive identities. Then 0' + 0 = 0 \\\\ 0 + 0' = 0' \\\\ 0' + 0' = 0' \\\\","title":"2.1.7."},{"location":"ch02ex/#218","text":"Let x and y be noncollinear vectors in \\mathbb{R}^3 . Give a geometric description of the set of all linear combinations of x and y . Solution : All the linear combinations of x and y form a plane that includes 0, x, y . \\square","title":"2.1.8."},{"location":"ch02ex/#219","text":"Which of the following sets are bases of \\mathbb{R}^3 ? S_1 = \\{(1,0,0),(1,1,0),(1,1,1)\\} \\\\ S_2 = \\{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\\} \\\\ S_3 = \\{(1,1,0),(0,1,1)\\} \\\\ S_4 = \\{(1,1,0),(0,1,1),(1,0,-1)\\} \\\\ How many elements do you think a basis for \\mathbb{R}^n must have? Give (without proof) geometric descriptions of all bases of \\mathbb{R}^2 , of \\mathbb{R}^3 . Solution : For S_1 , given (x,y,z) , we have \\begin{cases} x &= a + b + c\\\\ y &= b + c \\\\ z &= c \\\\ \\end{cases} Then we can have \\begin{cases} a = x - y\\\\ b = y - z\\\\ c = z\\\\ \\end{cases} So S_1 is a base. For S_2 , it is note a base. For example (1,1,1) can be expressed by multiple ways. For S_3 \\begin{cases} x &= a \\\\ y &= a + b \\\\ z &= b \\\\ \\end{cases} Then we cannot represent (1,3,1) with S_3 . For S_4 , We have \\begin{cases} x &= a + c\\\\ y &= a + b \\\\ z &= b - c\\\\ \\end{cases} Then we have to have z = y - x , so we cannot represent (1,1,1) with S_4 . For \\mathbb{R}^n , it needs n element for a basis. For \\mathbb{R}^2 , if \\{f_1, f_2\\} is a basis, then geometrically, they should not be in one line. For \\mathbb{R}^3 , if \\{f_1, f_2, f_3\\} is a basis, then geometrically, they should not be in the same plane. \\square","title":"2.1.9"},{"location":"ch02ex/#2110","text":"Recall the field \\mathbb{C} of complex numbers. Define complex n -space \\mathbb{C}^n analogously to \\mathbb{R}^n : \\mathbb{C}^n = \\{ (z_1, \\cdots, z_n) : z_i \\in \\mathbb{C} \\text{ for } i = 1, \\cdots, n \\} and endow it with addition and scalar multiplication defined by the same formulas as for \\mathbb{R}^n . You may take for granted that under these definitions, \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{R} , and also \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{C} . That is, using language that was introduced briefly in this section, \\mathbb{C}^n can be viewed as a vector space over \\mathbb{R} and also, separately, as a vector space over \\mathbb{C} . Give a basis for each of these vector spaces. Solution : For the first one S_1 = \\{ (1,\\cdots,0), (i,\\cdots,0), \\cdots (0,\\cdots,1), (0,\\cdots,i) \\} For the second one S_2 = \\{ (1,\\cdots,0), \\cdots (0,\\cdots,1) \\} \\square","title":"2.1.10."},{"location":"ch02ex/#22-geometry-length-and-angle","text":"","title":"2.2 Geometry: Length and Angle"},{"location":"ch02ex/#221","text":"Let x = (\\frac{\\sqrt[]{3}}{2}, -\\frac{1}{2}, 0), y = (\\frac{1}{2}, \\frac{\\sqrt[]{3}}{2}, 1), z = (1, 1, 1) Compute \u27e8x,x\u27e9, \u27e8x,y\u27e9, \u27e8y,z\u27e9, |x|, |y|, |z|, \u03b8_{x,y}, \u03b8_{y,e_1}, \u03b8_{z,e_2} Solution : \u27e8x,x\u27e9 = \\frac{3}{4} + \\frac{1}{4} + 0 = 1 \\\\ \u27e8x,y\u27e9 = \\frac{\\sqrt[]{3}}{4} - \\frac{\\sqrt[]{3}}{4} + 0 = 0 \\\\ \u27e8y,z\u27e9 = \\frac{1}{2} + \\frac{\\sqrt[]{3}}{2} + 1 = \\frac{3 + \\sqrt[]{3}}{2} \\\\ |x| = 1 \\\\ |y| = \\sqrt[]{2} \\\\ |z| = \\sqrt[]{3} \\\\ \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x| |y|} = \\frac{0}{|x| |y|} = 0 \\\\ \u03b8_{y,e_1} = \\frac{\u27e8y,e_1\u27e9}{|y| |e_1|} = \\frac{1/2}{|x| |y|} = \\frac{1}{2 \\sqrt[]{2}} \\\\ \u03b8_{z,e_2} = \\frac{1}{\\sqrt[]{3}} \\square","title":"2.2.1"},{"location":"ch02ex/#222","text":"Show that the points x = (2,\u22121,3,1), y = (4,2,1,4), z = (1,3,6,1) form the vertices of a triangle in \\mathbb{R}^4 with two equal angles. Solution : d(x,y) = |x - y| = \\sqrt[]{\u27e8x-y,x-y\u27e9} = \\sqrt[]{\u27e8(-2,-3,2,-3),(-2,-3,2,-3)\u27e9} = \\sqrt[]{4+9+4+9} = \\sqrt[]{26} \\\\ d(y,z) = |y-z| = \\sqrt[]{\u27e8y-z,y-z\u27e9} = \\sqrt[]{\u27e8(3,-1,-5,3),(3,-1,-5,3)\u27e9} = \\sqrt[]{9+1+25+9} = \\sqrt[]{44} \\\\ d(z,x) = |z-x| = \\sqrt[]{\u27e8z-x,z-x\u27e9} = \\sqrt[]{\u27e8(-1,4,3,0),(-1,4,3,0)\u27e9} = \\sqrt[]{1+16+9+0} = \\sqrt[]{26} \\\\ \\square","title":"2.2.2."},{"location":"ch02ex/#223","text":"Explain why for all x \u2208 \\mathbb{R}^n , x = \\sum_{j=1}^{n} \u27e8x,e_j\u27e9e_j . Proof : Note \u27e8x,e_j\u27e9 = \u27e8(x_1, \\cdots, x_n),(0, \\cdots, 1, \\cdots, 0)\u27e9 = x_j \\\\ x_j e_j = (0, \\cdots, x_j, \\cdots, 0) \\\\ \\sum_{j=1}^{n} x_j e_j = (x_1, \\cdots, x_n) = x \\square","title":"2.2.3."},{"location":"ch02ex/#224","text":"Prove the inner product properties. Proof : (IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 = x_1^2 + \\cdots + x_n^2 \\geq 0 It equals 0 only when x_1 = \\cdots = x_n = 0 . \\square (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n \u27e8x,y\u27e9 = x_1 y_1 + \\cdots + x_n y_n = y_1 x_1 + \\cdots + y_n x_n = \u27e8y,x\u27e9 \\square (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = (x_1 + x'_1) y_1 + \\cdots + (x_n + x'_n) y_n \\\\ = (x_1 y_1 + \\cdots + x_n y_n) + (x'_1 y_1 + \\cdots + x'_n y_n) \\\\ = \u27e8x,y\u27e9 + \u27e8x',y\u27e9 \u27e8ax,y\u27e9 = (ax_1) y_1 + \\cdots + (ax_n) y_n \\\\ = a (x_1 y_1 + \\cdots + x_n y_n) \\\\ = a \u27e8x,y\u27e9 \\square","title":"2.2.4."},{"location":"ch02ex/#225","text":"Use the inner product properties and the definition of the modulus in terms of the inner product to prove the modulus properties. Proof : (Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ |x| = \\sqrt[]{\u27e8x,x\u27e9} \\geq 0 \u27e8x,x\u27e9 = 0 if and only if x = 0 . \\square (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n . \u27e8ax,ax\u27e9 = a \u27e8x,ax\u27e9 = a^2 \u27e8x,x\u27e9 So |ax| = \\sqrt[]{\u27e8ax,ax\u27e9} = \\sqrt[]{a^2 \u27e8x,x\u27e9} = |a||x| \\square","title":"2.2.5."},{"location":"ch02ex/#226","text":"In the text, the modulus is defined in terms of the inner product. Prove that this can be turned around by showing that for every x,y \u2208 \\mathbb{R}^n , \u27e8x,y\u27e9 = \\frac{|x+y|^2 - |x-y|^2}{4}. Proof : |x+y|^2 = \u27e8x+y,x+y\u27e9 = \u27e8x,x\u27e9 + 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ |x-y|^2 = \u27e8x-y,x-y\u27e9 = \u27e8x,x\u27e9 - 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ So the equation holds. \\square","title":"2.2.6."},{"location":"ch02ex/#227","text":"Prove the full triangle inequality: for every x,y \u2208 \\mathbb{R}^n , ||x| - |y|| \\leq |x \\pm y | \\leq |x| + |y| Do not do this by writing three more variants of the proof of the triangle inequality, but by substituting suitably into the basic triangle inequality, which is already proved. Proof : |x - y| \\leq |x| + |y| Note \\begin{split} |x-y| & = |x + (-y)| \\\\ & \\leq |x| + |-y| \\\\ & = |x| + |y| \\\\ \\end{split} 2. ||x| - |y|| \\leq |x + y| Note \\begin{split} |x| &= |(-y) + (x+y)| \\\\ & \\leq |-y| + |x+y| \\\\ & = |y| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |x| - |y| \\leq |x+y| And similarly \\begin{split} |y| &= |(-x) + (x+y)| \\\\ & \\leq |-x| + |x+y| \\\\ & = |x| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |y| - |x| \\leq |x+y| Combine these 2 together, we have ||x| - |y|| \\leq |x + y| 3. ||x| - |y|| \\leq |x - y| This is similar to 2. \\square","title":"2.2.7."},{"location":"ch02ex/#228","text":"Let x = (x_1,...,x_n) \u2208 \\mathbb{R}^n . Prove the size bounds: for every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{j=1}^{n} |x_j|. (One approach is to start by noting that x_j= \u27e8x,e_j\u27e9 and recalling equation (2.1).) When can each \u2264 be an = ? Proof : From the hint: \\begin{split} |x_j| &= |\u27e8x,e_j\u27e9| \\\\ & \\leq |x||e_j| \\\\ & \\text{Cauchy-Schwarz inequality} \\\\ & = |x| \\cdot 1 \\\\ & = |x| \\end{split} This equality holds when x = a \\cdot e_j . Note \\begin{split} |x| &= \\left| \\sum_{j=1}^{n} x_j e_j \\right| \\\\ & \\leq \\sum_{j=1}^{n} \\left| x_j e_j \\right| \\\\ & \\text{Generalized triangle inequality}\\\\ & = \\sum_{j=1}^{n} | x_j | | e_j | \\\\ & \\text{The modulus is absolute-homogeneous}\\\\ & = \\sum_{j=1}^{n} | x_j | \\\\ \\end{split} This equality holds when only only one of x_j is not 0 . \\square","title":"2.2.8."},{"location":"ch02ex/#229","text":"Prove the distance properties. Proof : (D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . Note d(x,y) = |x-y| \\geq 0 The equality holds only when x-y = 0 , i.e. x = y . \\square (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . Note: d(x,y) = |x-y| = |y-x| = d(y,x) \\square \\square (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n . Note: \\begin{split} d(x,z) &= |x-z| \\\\ &= |(x-y)+(y-z)| \\\\ & \\leq |x-y| + |y-z| \\\\ &= d(x,y) + d(y,z) \\end{split} \\square","title":"2.2.9."},{"location":"ch02ex/#2210","text":"Working in \\mathbb{R}^2 , depict the nonzero vectors x and y as arrows from the origin and depict x\u2212y as an arrow from the endpoint of y to the endpoint of x . Let \u03b8 denote the angle (in the usual geometric sense) between x and y . Use the law of cosines to show that \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} Proof Geometric proof: \\frac{\u27e8x,y\u27e9}{|x||y|} = \\frac{x_1 y_1 + x_2 y_2}{|x||y|} = \\frac{1}{|x|} \\left( \\frac{x_1 y_1 + x_2 y_2}{|y|} \\right) As shown in the figure below, XD \\perp OY , AC \\perp OY and XB \\perp AC . We just need to show \\frac{x_1 y_1 + x_2 y_2}{|y|} = OD Note \\frac{y_1}{y} = \\cos \\angle AOC \\\\ \\Rightarrow \\\\ \\frac{x_1 y_1}{|y|} = OC \\frac{y_2}{y} = \\cos \\angle AXB \\\\ \\Rightarrow \\\\ \\frac{x_2 y_2}{|y|} = CD Then we finished the proof. Proof with triangle equalities: x_1 = |x| \\cos \u03b8_x, x_2 = |x| \\sin \u03b8_x, y_1 = |y| \\cos \u03b8_y, y_2 = |y| \\sin \u03b8_y, Then \u27e8x,y\u27e9 = |x||y| \\cos \u03b8_x \\cos \u03b8_y + |x||y| \\sin \u03b8_x \\sin \u03b8_y \\\\ = |x||y| \\cos (\u03b8_x - \u03b8_y) = |x||y| \\cos \u03b8 This also proved. \\square","title":"2.2.10"},{"location":"ch02ex/#2211","text":"Prove that for every nonzero x \u2208 \\mathbb{R}^n , \\sum_{i = 1}^{n} \\cos ^2 \u03b8_{x, e_i} = 1 . Proof : \\cos ^2 \u03b8_{x, e_i} = \\sum_{i = 1}^{n} \\left( \\frac{\u27e8x,e_i\u27e9}{|x||e_i|} \\right)^2 \\\\ \\sum_{i = 1}^{n} \\frac{x_i^2}{|x|^2} \\\\ = \\frac{1}{|x|^2} \\sum_{i = 1}^{n} x_i^2 \\\\ = 1 \\square","title":"2.2.11."},{"location":"ch02ex/#2212","text":"Prove that two nonzero vectors x, y are orthogonal if and only if |x+y|^2 = |x|^2 +|y|^2. Proof : \\begin{split} |x+y|^2 = \u27e8x+y,x+y\u27e9 = |x|^2 + 2 \u27e8x,y\u27e9 + |y|^2 \\end{split} Then |x+y|^2 = |x|^2 +|y|^2 means \u27e8x,y\u27e9=0 . \\square","title":"2.2.12."},{"location":"ch02ex/#2213","text":"Use vectors in \\mathbb{R}^2 to show that the diagonals of a parallelogram are perpendicular if and only if the parallelogram is a rhombus. Proof : The 2 diagonals of the parallelogram are x+y, x-y . \u27e8x+y,x-y\u27e9 = |x|^2 - |y|^2 If x+y \\perp x-y , then |x|^2 - |y|^2=0 , i.e. |x| = |y| . On the other hand, if |x| = |y| , then \u27e8x+y,x-y\u27e9 . \\square","title":"2.2.13."},{"location":"ch02ex/#2214","text":"Use vectors to show that every angle inscribed in a semicircle is right. Let (x,y) on the semicircle, then x^2+y^2=1 . consider (x,y) - (-1,0) = (x+1,y) and (x,y) - (1,0) = (x-1,y) . Then \u27e8(x+1,y), (x-1, y)\u27e9 = x^2 - 1 + y^2 = 0 \\square","title":"2.2.14."},{"location":"ch02ex/#2215","text":"Let x and y be vectors, with x nonzero.Define the parallel component of y along x and the normal component of y to x to be y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x \\qquad \\text{and} \\qquad y_{(\\perp x)} = y - y_{(\\parallel x)} Answer: (a) Show that y = y_{(\\parallel x)} + y_{(\\perp x)} . show that y_{(\\parallel x)} is a scalar multiple of x . show that y_{(\\perp x)} is orthogonal to x . Show that the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. Draw an illustration. Proof : y_{(\\parallel x)} + y_{(\\perp x)} = \\\\ y_{(\\parallel x)} + (y - y_{(\\parallel x)}) = \\\\ y y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x Since \\frac{\u27e8x,y\u27e9}{|x|^2} is a scalar, then y_{(\\parallel x)} is a scalar multiple of x . \u27e8y - y_{(\\parallel x)},y_{(\\parallel x)}) = \\\\ \u27e8|x|^2 y - \u27e8x,y\u27e9x, \u27e8x,y\u27e9x\u27e9 \\\\ = |x|^2 \u27e8x,y\u27e9 \u27e8y,x\u27e9 - \u27e8x,y\u27e9^2 \u27e8x,x\u27e9 \\\\ = 0 So y_{(\\perp x)} is orthogonal to y_{(\\parallel x)} , thus is orthogonal to x . In general if x \\perp y and ax + by = 0, x,y \\neq 0 , then 0 = \u27e80, x\u27e9 = \u27e8ax + by,x\u27e9 = a \u27e8x,x\u27e9 + b \u27e8y,x\u27e9 = a \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ a = 0 For the same reason b = 0 . With this, the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. \\square (b) Show that |y|^2 = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 What theorem from classical geometry does this encompass? Proof : |y|^2 = \\\\ |y_{(\\parallel x)} + y_{(\\perp x)}|^2 = \\\\ |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 + 2 \u27e8y_{(\\parallel x)}, y_{(\\perp x)}\u27e9 \\\\ = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 \\\\ It's Pythagorean theorem or Gougu theorem. \\square (c) Explain why it follows from (b) that |y_{(\\parallel x)}|^2 \\leq |y|^2 with equality if and only if y is a scalar multiple of x . Use this inequality to give another proof of the Cauchy\u2013Schwarz inequality. This argument gives the geometric content of Cauchy\u2013Schwarz: the parallel component of one vector along another is at most as long as the original vector. Proof : This is because |y_{(\\perp x)}|^2 \\geq 0 . The equality holds when |y_{(\\perp x)}|^2 = 0 . In this case y = y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x To prove |\u27e8x,y\u27e9| \\leq |x||y| Note |\u27e8x,y\u27e9| = |\u27e8x,y_{(\\parallel x)}+y_{(\\perp x)}\u27e9| \\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9 + 0|\\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9| Now in general, if a is a scaler, then |\u27e8x,ax\u27e9| = |a \u27e8x,x\u27e9| = |a||x|^2 = |x| |ax| Then |\u27e8x,y_{(\\parallel x)}\u27e9| = |x||y_{(\\parallel x)}|| \\leq |x||y| \\square (d) The proof of the Cauchy\u2013Schwarz inequality in part (c) refers to parts (a) and (b), part (a) refers to orthogonality, orthogonality refers to an angle, and as explained in the text, the fact that angles make sense depends on the Cauchy\u2013Schwarz inequality. And so the proof in part (c) apparently relies on circular logic. Explain why the logic is in fact not circular. Solution : Note that when we define \\cos \u03b8 , it is this: \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} But here, we define y_{(\\parallel x)} as y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2}x If we think geometrically in \\mathbb{R}^2 or \\mathbb{R}^3 , the projection of y on x should be y \\cos \u03b8 . The unit length vector along x is \\frac{x}{|x|} . Also the length of y projection is |y \\cos \u03b8| = \\frac{\u27e8x,y\u27e9}{|x|} So it's reasonable to define y_{(\\parallel x)} this way. Now, since we does not rely on the fact that |\\cos \u03b8| \\leq 1 , it is not circular. \\square","title":"2.2.15."},{"location":"ch02ex/#2216","text":"Given nonzero vectors x_1,x_2,...,x_n in \\mathbb{R}^n , the Gram\u2013Schmidt process is to set \\begin{split} x_1' &= x_1 \\\\ x_2' &= x_2 - (x_2)_{(\\parallel x_1')}\\\\ x_3' &= x_3 - (x_3)_{(\\parallel x_2')} - (x_3)_{(\\parallel x_1')} \\\\ \\vdots \\\\ x_n' &= x_n - (x_n)_{(\\parallel x_{n-1}')} - \\cdots - (x_n)_{(\\parallel x_1')} \\\\ \\end{split} Answer: (a)What is the result of applying the Gram\u2013Schmidt process to the vectors x_1 = (1,0,0), x_2 = (1,1,0) , and x_3 = (1,1,1) ? Solution : x_1' = (1,0,0) \\\\ x_2' = (0,1,0) \\\\ x_3' = (0,0,1) \\\\ \\square (b) Returning to the general case, show that x_1',...,x_n' are pairwise orthogonal and that each x_j' has the form x_j'= a_{j,1} x_1 +a_{j,2}x_2 +\u00b7\u00b7\u00b7+a_{j,j\u22121}x_{j\u22121} +x_j. Thus every linear combination of the new {x_j'} is also a linear combination of the original {x_j} . The converse is also true and will be shown in Exercise 3.3.13. Proof : First, we want to prove a small lemma: Given x, y, z , if x \\perp y , then we want to show z_{(\\parallel x)} \\perp y Note \u27e8z_{(\\parallel x)},y\u27e9 = \u27e8\\frac{\u27e8z,x\u27e9}{|x|^2}x,y\u27e9 = \\frac{\u27e8z,x\u27e9}{|x|^2} \u27e8x,y\u27e9 = 0 Then we can use induction. And assume x_1',...,x_{k-1}' are orthogonal to each other. Assume 1 \\leq j \\leq k-1 , \u27e8x_j',x_{k}'\u27e9 = \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 - \\sum_{i \\neq j} \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 = 0 is proved in exercise 2.2.15. Also, since \u27e8x_j',x_i'\u27e9=0 for j \\neq i , then \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 = 0 as just proved in the lemma. So \u27e8x_j',x_{k}'\u27e9 = 0 . For the 2nd part, we can still use induction. And assume for x_1',...,x_{k-1}' , they are all linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Then x_k' = x_k - (x_k)_{(\\parallel x_{k-1}')} - \\cdots - (x_k)_{(\\parallel x_1')} Note (x_k)_{(\\parallel x_j')} is a scaler of x_j', j < k , thus a linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Therefore, x_k' is a linear combination of x_1,...,x_{k} . \\square","title":"2.2.16."},{"location":"ch02ex/#23-analysis-continuous-mappings","text":"","title":"2.3 Analysis: Continuous Mappings"},{"location":"ch02ex/#231","text":"For A \u2282 \\mathbb{R}^n , partially verify that \\mathcal{M}(A,\\mathbb{R}^m) is a vector space over \\mathbb{R} by showing that it satisfies vector space axioms (A4) and (D1). Proof : (A4) Addition is commutative \\begin{align*} (f+g)(x) &= f(x) + g(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= g(x) + f(x) \\quad &\\text{by commutativity of \u201c+\u201d in }\\mathbb{R}^m\\\\ &= (g+f)(x) &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m) \\end{align*} Next, (D1) Scalar multiplication distributes over scalar addition \\begin{align*} ((a+b)f)(x) &= (a+b)f(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= af(x) + bf(x) \\quad &\\text{by D1 in }\\mathbb{R}^m\\\\ &= (af)(x) + (bf)(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= ((af) + (bf))(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ \\end{align*} \\square","title":"2.3.1."},{"location":"ch02ex/#232","text":"Define multiplication * : \\mathcal{M}(A, \\mathbb{R}^m) \\times \\mathcal{M}(A, \\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A, \\mathbb{R}^m) Is \\mathcal{M}(A, \\mathbb{R}^m) a field with \u201c+\u201d from the section and this multiplication? Does it have a subspace that is a field? Solution Define: (f * g)(x) = f(x) * g(x) And f(x) = 1 is it's multiplication identity. However, this function does not have inverse. f(x) = \\begin{cases} 0 &\\text{if } x = 0 \\\\ 1 &\\text{if } x \\neq 0 \\\\ \\end{cases} Consider V = \\{f : f(x) = (r, \\cdots, r) \\in \\mathbb{R}^m \\text{ for all } x \\in A\\} This is a field. \\square","title":"2.3.2."},{"location":"ch02ex/#233","text":"For A \u2282 \\mathbb{R}^n and m \u2208 \\mathbb{Z}^+ define a subspace of the space of mappings from A to \\mathbb{R}^m \\mathcal{C}(A, \\mathbb{R}^m) = \\{ f \\in \\mathcal{M}(A, \\mathbb{R}^m) : f \\text{ is continuous on } A \\}. Briefly explain how this section has shown that \\mathcal{C}(A, \\mathbb{R}^m) is a vector space. Solution : It's stated in Proposition 2.3.7 (Vector space properties of continuity). \\square","title":"2.3.3."},{"location":"ch02ex/#234","text":"Define an inner product and a modulus on \\mathcal{C}([0,1], \\mathbb{R}) by $$ \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt, \\quad |f| = \\sqrt[]{\u27e8f,f\u27e9}. $$ Do the inner product properties (IP1),(IP2),and(IP3) (see Proposition2.2.2) hold for this inner product on \\mathcal{C}([0,1], \\mathbb{R}) How much of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) ? Express the Cauchy\u2013Schwarz inequality as a relation between integrals. Proof : (IP1) \u27e8f, f\u27e9 = \\int_{0}^{1} f(t)f(t)dt \\geq \\int_{0}^{1} 0dt = 0 It achieves 0 only when f(t) = 0 . (IP2) \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt = \\int_{0}^{1} g(t)f(t)dt = \u27e8g,f\u27e9 So IP2 holds. (IP3) First, we want to show \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh Based on the definition of integral, give a partition P on [0,1] . L(fg, P) + L(fh, P) \\leq L(fg+fh, P) \\leq U(fg+fh, P) \\leq U(fg, P) + L(fh, P) So we have \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh. Now \u27e8f,g+h\u27e9 = \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh = \u27e8f,g\u27e9 + \u27e8f,h\u27e9 Also \u27e8af,g\u27e9 = \\int_{0}^{1} (af)g = a(\\int_{0}^{1} fg) = a \u27e8f,g\u27e9 \\square All of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) Cauchy\u2013Schwarz inequality \\left| \\int_{0}^{1} f(t)g(t) dt \\right| \\leq \\sqrt[]{\\int_{0}^{1}f^2(t)dt} \\cdot \\sqrt[]{\\int_{0}^{1}g^2(t)dt} \\square","title":"2.3.4."},{"location":"ch02ex/#235","text":"Use the definition of convergence and the componentwise nature of nullness to prove the componentwise nature of convergence. (The argument is short.) Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p = \\text{null} \\\\ \\text{definition of convergence} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j = \\text{null} \\\\ \\text{Lemma 2.3.2 (Componentwise nature of nullness).} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\\\ \\text{definition of convergence} \\\\ \\square","title":"2.3.5"},{"location":"ch02ex/#236","text":"Use the definition of continuity and the componentwise nature of convergence to prove the componentwise nature of continuity. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square","title":"2.3.6."},{"location":"ch02ex/#237","text":"Prove the persistence of continuity under composition. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square","title":"2.3.7"},{"location":"ch02ex/#238","text":"Define f : \\mathbb{Q} \\longrightarrow \\mathbb{R} by the rule f(x) = \\begin{cases} 1 &\\text{if } x^2 < 2,\\\\ 0 &\\text{if } x^2 > 2.\\\\ \\end{cases} Solution : It is continuous. Given any p \\in \\mathbb{Q} , if p^2 < 2 , we can always find a U_{\\delta}(p) such that if q \\in U_{\\delta}(p) , then q^2 < 2 . Same thing when p^2 > 2 . \\square","title":"2.3.8."},{"location":"ch02ex/#239","text":"Which of the following functions on \\mathbb{R}^2 can be defined continuously at 0 ? (a) f(x, y) = \\begin{cases} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution \\begin{split} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &= \\frac{x^2 - y^2}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\begin{split} \\frac{x^2 - y^2}{x^2 + y^2} &= \\frac{1 - m^2}{1 + m^2} \\end{split} So it cannot be defined continuously at 0 . \\square (b) g(x, y) = \\begin{cases} \\frac{x^2 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\frac{x^2 - y^3}{x^2 + y^2} &= 1 - \\frac{y^2 + y^3}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\frac{y^2 + y^3}{x^2 + y^2} \\\\ = \\frac{m^2 + m^3y}{1+m^2} \\rightarrow \\frac{m^2}{1+m^2} So it cannot be defined continuously at 0 . \\square (c) h(x, y) = \\begin{cases} \\frac{x^3 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\left| \\frac{x^3 - y^3}{x^2 + y^2} \\right| &= \\left| (x-y) \\frac{x^2 + xy + y^2}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{xy}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{|(x,y)||(x,y)|}{|(x,y)|^2} \\right| \\\\ & = 2 |x - y| \\\\ & \\leq 2 (|x| + |y|) \\rightarrow 0 \\end{split} So it can be defined continuously at 0. \\square (d) k(x, y) = \\begin{cases} \\frac{xy^2}{x^2 + y^6} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : When (x,y) approaches 0 with y = x , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^3}{x^2 + x^6} = \\frac{x}{1 + x^4} \\rightarrow 0 However, when (x,y) approaches 0 with y = \\sqrt[]{x} , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^2}{x^2 + x^3} = \\frac{1}{1 + x} \\rightarrow 1 So it cannot be defined continuously at 0 . \\square","title":"2.3.9."},{"location":"ch02ex/#2310","text":"Let f(x,y) = g(xy) , where g : \\mathbb{R} \u2192 \\mathbb{R} is continuous. Is f continuous? Proof : Let h(x,y) = xy , then h(x,y) is a continuous function. Then from Proposition 2.3.8 (Persistence of continuity under composition), f = g \\circ h . So f is also continuous. \\square","title":"2.3.10."},{"location":"ch02ex/#2311","text":"Let f,g \\in \\mathcal{M}(\\mathbb{R}^n, \\mathbb{R}) such that f +g and fg are continuous. Are f and g necessarily continuous? Proof : No, it's not necessary. Consider the case when n = 1 . f(x) = \\begin{cases} 0 &\\text{if } x \\in \\mathbb{Q}\\\\ 1 &\\text{if } x \\in \\mathbb{I}\\\\ \\end{cases} Then g(x) = 1 - f(x) . Then f+g = 1, fg = 0 are continuous. But f, g are not continuous. \\square","title":"2.3.11."},{"location":"ch02notes/","text":"Chapter 2 Euclidean Space 2.1 Algebra: Vectors Theorem 2.1.1 (Vector space axioms). (A1) Addition is associative (A2) 0 is an additive identity (A3) Existence of additive inverses (A4) Addition is commutative (M1) Scalar multiplication is associative (M2) 1 is a multiplicative identity (D1) Scalar multiplication distributes over scalar addition (D2) Scalar multiplication distributes over vector addition For n > 1 , \\mathbb{R}^n is not endowed with vector-by-vector multiplication. If the vector space axioms are satisfied with V and F replacing \\mathbb{R}^n and \\mathbb{R} then we say that V is a vector space over F . We can use intrinsic vector algebra to prove a result from Euclidean geometry, that the three medians of a triangle intersect. Let p = \\frac{x + y + z}{3} Rewrite it as p = x + \\frac{2}{3}(\\frac{y+z}{2} - x) Note that \\frac{y+z}{2} is the middle point of yz . Then \\frac{y+z}{2} - x is the median from x to yz , so p is on the median. Since x,y,z are symmetric, then p is on all 3 medians. \\square The standard basis of \\mathbb{R}^n is the set of vectors e_1 = (1, 0, \\cdots 0) \\\\ e_2 = (0, 1, \\cdots 0) \\\\ \\cdots \\\\ e_n = (0, 0, \\cdots 1) \\\\ So \\tag{2.1} x = \\sum_{i = 1}^{n}x_i e_i Definition 2.1.2 (Basis). A set of vectors \\{f_i\\} is a basis of \\mathbb{R}^n if every x \u2208 \\mathbb{R}^n is uniquely expressible as a linear combination of the f_i . \\square 2.2 Geometry: Length and Angle Definition 2.2.1 (Inner product). The inner product is a function from pairs of vectors to scalars, \\left\\langle , \\right\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R} defined by the formula \\left\\langle (x_1, \\cdots, x_n) , (y_1, \\cdots, y_n) \\right\\rangle = \\sum_{i = 1}^{n} x_i y_i Proposition 2.2.2 (Inner product properties). (IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = \u27e8x,y\u27e9 + \u27e8x',y\u27e9, \\quad \u27e8ax,y\u27e9 = a \u27e8x,y\u27e9 \\\\ \u27e8x,y+y'\u27e9 = \u27e8x,y\u27e9 + \u27e8x,y'\u27e9, \\quad \u27e8x,by\u27e9 = b \u27e8x,y\u27e9 for all a,b \u2208 \\mathbb{R} , x,x',y,y' \u2208 \\mathbb{R}^n . \\square Like the vector space axioms, the inner product properties are phrased intrinsically, although they need to be proved using coordinates. As mentioned in the previous section,intrinsic methods are neater and more conceptual than using coordinates. More importantly: The rest of the results of this section are proved by reference to the inner product properties, with no further reference to the inner product formula. Definition 2.2.3 (Modulus). The modulus (or absolute value) of a vector x \u2208 \\mathbb{R}^n is defined as |x| = \\sqrt[]{\u27e8x,x\u27e9}. Proposition 2.2.4 (Modulus properties). (Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n . Theorem 2.2.5 (Cauchy\u2013Schwarz inequality). For all x,y \u2208 \\mathbb{R}^n , |\u27e8x,y\u27e9| \\leq |x| |y| with equality if and only if one of x, y is a scalar multiple of the other. Note that the absolute value signs mean di\ufb00erent things on each side of the Cauchy\u2013Schwarz inequality. On the left side, the quantities x and y are vectors, their inner product \u27e8x,y\u27e9 is a scalar, and |\u27e8x,y\u27e9| is its scalar absolute value, while on the right side, |x| and |y| are the scalar absolute values of vectors, and |x||y| is their product. That is, the Cauchy\u2013Schwarz inequality says: The size of the product is at most the product of the sizes. The computation draws on the minutiae of the formulas for the inner product and the modulus, rather than using their properties. It is uninformative, making the Cauchy\u2013Schwarz inequality look like a low-level accident. It suggests that larger-scale mathematics is just a matter of bigger and bigger formulas. To prove the inequality in a way that is enlightening and general, we should work intrinsically, keeping the scalars \u27e8x,y\u27e9 and |x| and |y| notated in their concise forms, and we should use properties, not formulas. Proof . The result is clear when x = 0 , so assume x \\neq 0 . For every a \u2208 \\mathbb{R}^n , \\begin{split} 0 &\\leq \u27e8ax-y,ax-y\u27e9 \\quad \\\\ & \\quad \\text{by positive definiteness} \\\\ &= a\u27e8x,ax-y\u27e9 - \u27e8y,ax-y\u27e9 \\\\ & \\quad \\text{by linearity in the first variable} \\\\ &= a^2 \u27e8x,x\u27e9 - a \u27e8x,y\u27e9 - a \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by linearity in the second variable} \\\\ &= a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 \\\\ & \\quad \\text{by symmetry, definition of modulus.} \\end{split} Define f(a) = a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 Since f(a) is always nonnegative, so f has at most one root. Thus by the quadratic formula its discriminant is nonpositive, 4\u27e8x,y\u27e9^2 \u22124|x|^2|y|^2 \u2264 0 So |\u27e8x,y\u27e9| \\leq |x| |y| Equality holds exactly when the quadratic polynomial f(a) = |ax\u2212 y|^2 has a root a, i.e., exactly when y= ax for some a \u2208 \\mathbb{R}^n . \\square Theorem 2.2.6 (Triangle inequality). For all x,y \u2208 \\mathbb{R}^n , |x+y| \\leq |x| + |y|, with equality if and only if one of x, y is a nonnegative scalar multiple of the other. Proof : \\begin{split} \u27e8x+y,x+y\u27e9 \\\\ &= \u27e8x,x+y\u27e9 + \u27e8y,x+y\u27e9 \\\\ &= \u27e8x,x\u27e9 + \u27e8x,y\u27e9 + \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ &= \u27e8x,x\u27e9 + 2\u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ &\\leq \u27e8x,x\u27e9 + 2|x||y| + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by Cauchy\u2013Schwarz} \\\\ &= (|x| + |y|)^2 \\end{split} Equality holds exactly when \u27e8x,y\u27e9= |x||y| , or equivalently when |\u27e8x,y\u27e9|= |x||y| and \u27e8x,y\u27e9 \u2265 0 . These hold when one of x, y is a scalar multiple of the other and the scalar is nonnegative. \\square While the Cauchy\u2013Schwarz inequality says that the size of the product is at most the product of the sizes, the triangle inequality says: The size of the sum is at most the sum of the sizes. Proposition 2.2.7 (Size bounds). For every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{i=1}^{n} |x_i|. Distance Definition The modulus gives rise to a distance function on Rn that behaves as distance should. Define d: \\mathbb{R}^n \\times \\mathbb{R}^n \\xrightarrow{} \\mathbb{R} by d(x,y) = |y-x| Theorem 2.2.8 (Distance properties). (D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n . Angle Definition If x and y are nonzero vectors in \\mathbb{R}^n , define their angle \u03b8_{x,y} by the condition \\tag{2.2} \\cos \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x||y|}, \\quad 0 \\leq \u03b8_{x,y} \\leq \u03c0. In particular, two nonzero vectors x and y are orthogonal when \u27e8x,y\u27e9 = 0 . Thus 0 orthogonal to all vectors. three altitudes must meet We have q-y \\perp x \\quad \\text{and} \\quad q-x \\perp y And we want to show \\left\\{ \\begin{array}{lr} \u27e8q-y,x\u27e9 = 0 \\\\ \u27e8q-x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 We have \\left\\{ \\begin{array}{lr} \u27e8q,x\u27e9 - \u27e8y,x\u27e9 = 0 \\\\ \u27e8q,y\u27e9 - \u27e8x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 So we have \u27e8q,x\u27e9 = \u27e8y,x\u27e9 = \u27e8x,y\u27e9 = \u27e8q,y\u27e9 \\\\ \\Longrightarrow \\\\ \u27e8q,x-y\u27e9 = 0 \\square 2.3 Analysis: Continuous Mappings Mapping from \\mathbb{R}^n to \\mathbb{R}^m A mapping from \\mathbb{R}^n to \\mathbb{R}^m is some rule that assigns to each point x in \\mathbb{R}^n a point in \\mathbb{R}^m . Generally, mappings will be denoted by letters such as f, g, h . Mappings as a vector space For a given dimension n , a given set A \u2282 \\mathbb{R}^n , and a second dimension m , let \\mathcal{M}(A,\\mathbb{R}^m) denote the set of all mappings f : A \\rightarrow \\mathbb{R}^m . This set forms a vector space over \\mathbb{R} (whose points are functions) under the operations + : \\mathcal{M}(A,\\mathbb{R}^m) \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (f +g)(x) = f(x)+g(x) \\quad\\text{for all } x \u2208 A, and \\cdot : \\mathbb{R} \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (a \\cdot f )(x) = a \\cdot f(x) \\quad\\text{for all } x \u2208 A. Sequence in \\mathbb{R}^n Let A be a subset of \\mathbb{R}^n . A sequence in A is an infinite list of vectors \\{x_1,x_2,x_3,...\\} in A , often written \\{x_\u03bd\\} . Since a vector has n entries, each vector x_\u03bd in the sequence takes the form (x_{1,\u03bd},...,x_{n,\u03bd}) . Definition 2.3.1 (Null Sequence). The sequence (x_{\\nu }) in \\mathbb{R}^n is null if for every \u03b5 > 0 there exists some \u03bd_0 such that \\text{if } \u03bd > \u03bd_0 \\text{ then } |x_v| < \\epsilon. That is, a sequence is null if for every \u03b5 > 0 , all but finitely many terms of the sequence lie within distance \u03b5 of 0_n . If \\{x_\u03bd\\} is a null sequence and |y_\u03bd| \\leq |x_\u03bd| or all \u03bd then also \\{y_\u03bd\\} is null. \\{x_\u03bd\\} and \\{y_\u03bd\\} are null sequence, then |x_\u03bd + y_\u03bd| \\leq |x_\u03bd| + |y_\u03bd| , so \\{x_\u03bd + y_\u03bd\\} is null. \\{c x_\u03bd\\} is null seq, since |cx_\u03bd| = |c||x_\u03bd| So the set of null sequences in \\mathbb{R}^n forms a vector space. A vector sequence \\{x_\u03bd\\} is null if and only if the scalar sequence \\{|x_\u03bd|\\} is null. Lemma 2.3.2 (Componentwise nature of nullness). The vector sequence \\{(x_{1,\u03bd}, \\cdots, x_{n,\u03bd})\\} is null if and only if each of its component scalar sequences \\{x_{j,\u03bd}\\} (j \\in \\{1, \\cdots, n\\}) is null. Proof Use |x_{j,\u03bd}| \\leq |x_\u03bd| \\leq \\sum_{j=1}^{n} |x_{j,\u03bd}| \\square Definition 2.3.3 (Sequence convergence, sequence limit). Let A be a subset of \\mathbb{R}^n . Consider a sequence \\{x_\u03bd\\} in A and a point p \u2208 \\mathbb{R}^n . The sequence \\{x_\u03bd\\} converges to p (or has limit p ), written \\{x_\u03bd\\} \u2192 p , if the sequence \\{x_\u03bd\u2212p\\} is null. When the limit p is a point of A , the sequence \\{x_\u03bd\\} converges in A . A sequence can only converges to one limit. Proposition 2.3.4 (Linearity of convergence). Let \\{x_\u03bd\\} be a sequence in \\mathbb{R}^n converging to p , let \\{y_\u03bd\\} be a sequence in \\mathbb{R}^n converging to q , and let c be a scalar. Then the sequence \\{x_\u03bd +y_\u03bd\\} converges to p+q , and the sequence \\{c x_\u03bd\\} converges to c_p . Proof : \\begin{split} |(x_\u03bd + y_\u03bd) - (p + q)| &= |(x_\u03bd - p) + (y_\u03bd-q)| \\\\ &\\leq |x_\u03bd - p| + |y_\u03bd - q| \\end{split} So (x_\u03bd + y_\u03bd) - (p + q) is null. Then x_\u03bd + y_\u03bd \u2192 p+q . |c x_\u03bd - cp| = |c (x_\u03bd - p)| \\leq |c| |x_\u03bd - p| So c x_\u03bd - cp is null. Then c x_\u03bd \u2192 cp \\square Proposition 2.3.5 (Componentwise nature of convergence). The vector sequence \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} converges to the vector (p_1, \\cdots, p_n) if and only if each component scalar sequence \\{x_{j, \u03bd}\\} (j = 1, \\cdots, n) . converges to the scalar p_j . Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\square Definition 2.3.6 (Continuity). Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a mapping, and let p be a point of A . Then f is continuous at p if for every sequence \\{x_\u03bd\\} in A converging to p , the sequence f(x_\u03bd) converges to f(p) . The mapping f is continuous on A (or just continuous when A is clearly established) if it is continuous at each point p \u2208 A . Modulus function is continuous || : \\mathbb{R}^n \\longrightarrow \\mathbb{R} Proof : Assume \\{x_\u03bd\\} \\rightarrow p , then from exercise 2.2.7 ||x_\u03bd| - |p|| \\leq | x_\u03bd - p | Then |x_\u03bd| - |p| is null, i.e. \\{f(x_\u03bd)\\} \\rightarrow f(p) . So || is continuous. The inner product is continuous Given a \\in \\mathbb{R}^n , define T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad T(x) = \u27e8a,x\u27e9 T is continuous. From Cauchy-Schwarz inequality |\u27e8a,x\u27e9| \\leq |a||x| Given p \\in \\mathbb{R}^n |T(x_\u03bd) - T(p)| = |\u27e8a,x_v\u27e9 - \u27e8a,p\u27e9| = |\u27e8a,x_\u03bd - p\u27e9| \\leq |a| | x_\u03bd - p | Since we know |x_\u03bd - p| \\rightarrow 0 , we have |T(x_\u03bd) - T(p)| \\rightarrow 0 . \\square jth coordinate function map Consider the following mapping \\pi_j : \\mathbb{R}^n \\longrightarrow \\mathbb{R}, \\quad \\pi_j (x_1, \\cdots, x_n) = x_j Since \\pi_j(x) = \u27e8e_j,x\u27e9 , this mapping is continuous. Proposition 2.3.7 (Vector space properties of continuity). Let A be a subset of \\mathbb{R}^n , let f,g : A \\longrightarrow \\mathbb{R}^m be continuous mappings, and let c \\in \\mathbb{R} . Then the sum and the scalar multiple mappings f+g,cf : A \\longrightarrow \\mathbb{R}^m are continuous. Thus the set of continuous mappings from A to \\mathbb{R}^m forms a vector subspace of \\mathcal{M}(A, \\mathbb{R}^m) . Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . Then \\{f(x_\u03bd)\\} \\rightarrow f(p), \\{g(x_\u03bd)\\} \\rightarrow g(p) . Then from Proposition 2.3.4 (Linearity of convergence), \\{f(x_\u03bd) + g(x_\u03bd)\\} \\rightarrow f(p) + g(p) , i.e. \\{(f+g)(x_\u03bd)\\} \\rightarrow (f+g)(p) . So f+g is continuous. For the same reason cf is continuous. \\square Proposition 2.3.8 (Persistence of continuity under composition). Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let B be a superset of f(A) in \\mathbb{R}^m , and let g : B \\longrightarrow \\mathbb{R}^l be a continuous mapping. Then the composition mapping g \\circ f : A \\longrightarrow \\mathbb{R}^l is continuous. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square Theorem 2.3.9 (Componentwise nature of continuity). Let A \u2282 \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m have component functions f_1,...,f_m , and let p be a point in A . Then f \\text{ is continuous at } p \\Longleftrightarrow \\text{each } f_i \\text{ is continuous at } p. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square Some examples Consider f(x, y) = \\begin{cases} \\frac{2xy}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Can the constant b be specified to make f continuous at 0 ? Take a sequence \\{(x_\u03bd, y_\u03bd)\\} \\rightarrow 0 along the line y= mx . f(x_\u03bd, y_\u03bd) = \\\\ \\frac{2x_\u03bd y_\u03bd}{x_\u03bd^2 + y_\u03bd^2} = \\\\ \\frac{2m}{1 + m^2} hence f cannot be made continuous at 0 . \\square Now consider g(x, y) = \\begin{cases} \\frac{x^2y}{x^4 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} If (x,y) approaches to (0,0) with y = x , then g(x,y) = \\frac{x^3}{x^4 + x^2} = \\frac{x}{x^2 + 1} \\rightarrow 0 If (x,y) approaches to (0,0) with y = x^2 , then g(x,y) = \\frac{x^4}{x^4 + x^4} = \\frac{1}{2} hence f cannot be made continuous at 0 . \\square The size bounds to prove continuity Consider h(x, y) = \\begin{cases} \\frac{x^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Note from 2.2.7, we have |x| \\leq |(x,y)| , so \\frac{x^3}{x^2 + y^2} \\leq \\frac{|(x,y)|^3}{|(x,y)|^2} = |(x,y)| \\rightarrow 0 So setting b = 0 makes it continuous at 0 . Summary of the 3 examples The straight line test can prove that a limit does not exist, or it can determine the only candidate for the value of the limit, but it cannot prove that the candidate value is the limit. When the straight line test determines a candidate value of the limit, approaching along a curve can further support the candidate, or it can prove that the limit does not exist by determining a di\ufb00erent candidate as well. The size bounds can prove that a limit does exist, but they can only suggest that a limit does not exist. Proposition 2.3.10 (Persistence of inequality). Let A be a subset of \\mathbb{R}^n and let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let p be a point of A , let b be a point of \\mathbb{R}^m , and suppose that f(p) \\neq b . Then there exists some \\epsilon > 0 , such that \\text{for all } x \\in A \\text{such that } |x-p| < \\epsilon, f(x) \\neq b. Proof : Assume otherwise, then for \\nu \\in \\mathbb{N} , we can find x_\u03bd , such that | x_\u03bd - p | < 1/\u03bd and f(x_\u03bd) = b . Since f is continuous, then f(p) = b . We have a contradiction. \\square","title":"Chapter 02 Notes"},{"location":"ch02notes/#chapter-2-euclidean-space","text":"","title":"Chapter 2 Euclidean Space"},{"location":"ch02notes/#21-algebra-vectors","text":"","title":"2.1 Algebra: Vectors"},{"location":"ch02notes/#theorem-211-vector-space-axioms","text":"(A1) Addition is associative (A2) 0 is an additive identity (A3) Existence of additive inverses (A4) Addition is commutative (M1) Scalar multiplication is associative (M2) 1 is a multiplicative identity (D1) Scalar multiplication distributes over scalar addition (D2) Scalar multiplication distributes over vector addition For n > 1 , \\mathbb{R}^n is not endowed with vector-by-vector multiplication. If the vector space axioms are satisfied with V and F replacing \\mathbb{R}^n and \\mathbb{R} then we say that V is a vector space over F . We can use intrinsic vector algebra to prove a result from Euclidean geometry, that the three medians of a triangle intersect. Let p = \\frac{x + y + z}{3} Rewrite it as p = x + \\frac{2}{3}(\\frac{y+z}{2} - x) Note that \\frac{y+z}{2} is the middle point of yz . Then \\frac{y+z}{2} - x is the median from x to yz , so p is on the median. Since x,y,z are symmetric, then p is on all 3 medians. \\square The standard basis of \\mathbb{R}^n is the set of vectors e_1 = (1, 0, \\cdots 0) \\\\ e_2 = (0, 1, \\cdots 0) \\\\ \\cdots \\\\ e_n = (0, 0, \\cdots 1) \\\\ So \\tag{2.1} x = \\sum_{i = 1}^{n}x_i e_i","title":"Theorem 2.1.1 (Vector space axioms)."},{"location":"ch02notes/#definition-212-basis","text":"A set of vectors \\{f_i\\} is a basis of \\mathbb{R}^n if every x \u2208 \\mathbb{R}^n is uniquely expressible as a linear combination of the f_i . \\square","title":"Definition 2.1.2 (Basis)."},{"location":"ch02notes/#22-geometry-length-and-angle","text":"","title":"2.2 Geometry: Length and Angle"},{"location":"ch02notes/#definition-221-inner-product","text":"The inner product is a function from pairs of vectors to scalars, \\left\\langle , \\right\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R} defined by the formula \\left\\langle (x_1, \\cdots, x_n) , (y_1, \\cdots, y_n) \\right\\rangle = \\sum_{i = 1}^{n} x_i y_i","title":"Definition 2.2.1 (Inner product)."},{"location":"ch02notes/#proposition-222-inner-product-properties","text":"(IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = \u27e8x,y\u27e9 + \u27e8x',y\u27e9, \\quad \u27e8ax,y\u27e9 = a \u27e8x,y\u27e9 \\\\ \u27e8x,y+y'\u27e9 = \u27e8x,y\u27e9 + \u27e8x,y'\u27e9, \\quad \u27e8x,by\u27e9 = b \u27e8x,y\u27e9 for all a,b \u2208 \\mathbb{R} , x,x',y,y' \u2208 \\mathbb{R}^n . \\square Like the vector space axioms, the inner product properties are phrased intrinsically, although they need to be proved using coordinates. As mentioned in the previous section,intrinsic methods are neater and more conceptual than using coordinates. More importantly: The rest of the results of this section are proved by reference to the inner product properties, with no further reference to the inner product formula.","title":"Proposition 2.2.2 (Inner product properties)."},{"location":"ch02notes/#definition-223-modulus","text":"The modulus (or absolute value) of a vector x \u2208 \\mathbb{R}^n is defined as |x| = \\sqrt[]{\u27e8x,x\u27e9}.","title":"Definition 2.2.3 (Modulus)."},{"location":"ch02notes/#proposition-224-modulus-properties","text":"(Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n .","title":"Proposition 2.2.4 (Modulus properties)."},{"location":"ch02notes/#theorem-225-cauchyschwarz-inequality","text":"For all x,y \u2208 \\mathbb{R}^n , |\u27e8x,y\u27e9| \\leq |x| |y| with equality if and only if one of x, y is a scalar multiple of the other. Note that the absolute value signs mean di\ufb00erent things on each side of the Cauchy\u2013Schwarz inequality. On the left side, the quantities x and y are vectors, their inner product \u27e8x,y\u27e9 is a scalar, and |\u27e8x,y\u27e9| is its scalar absolute value, while on the right side, |x| and |y| are the scalar absolute values of vectors, and |x||y| is their product. That is, the Cauchy\u2013Schwarz inequality says: The size of the product is at most the product of the sizes. The computation draws on the minutiae of the formulas for the inner product and the modulus, rather than using their properties. It is uninformative, making the Cauchy\u2013Schwarz inequality look like a low-level accident. It suggests that larger-scale mathematics is just a matter of bigger and bigger formulas. To prove the inequality in a way that is enlightening and general, we should work intrinsically, keeping the scalars \u27e8x,y\u27e9 and |x| and |y| notated in their concise forms, and we should use properties, not formulas. Proof . The result is clear when x = 0 , so assume x \\neq 0 . For every a \u2208 \\mathbb{R}^n , \\begin{split} 0 &\\leq \u27e8ax-y,ax-y\u27e9 \\quad \\\\ & \\quad \\text{by positive definiteness} \\\\ &= a\u27e8x,ax-y\u27e9 - \u27e8y,ax-y\u27e9 \\\\ & \\quad \\text{by linearity in the first variable} \\\\ &= a^2 \u27e8x,x\u27e9 - a \u27e8x,y\u27e9 - a \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by linearity in the second variable} \\\\ &= a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 \\\\ & \\quad \\text{by symmetry, definition of modulus.} \\end{split} Define f(a) = a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 Since f(a) is always nonnegative, so f has at most one root. Thus by the quadratic formula its discriminant is nonpositive, 4\u27e8x,y\u27e9^2 \u22124|x|^2|y|^2 \u2264 0 So |\u27e8x,y\u27e9| \\leq |x| |y| Equality holds exactly when the quadratic polynomial f(a) = |ax\u2212 y|^2 has a root a, i.e., exactly when y= ax for some a \u2208 \\mathbb{R}^n . \\square","title":"Theorem 2.2.5 (Cauchy\u2013Schwarz inequality)."},{"location":"ch02notes/#theorem-226-triangle-inequality","text":"For all x,y \u2208 \\mathbb{R}^n , |x+y| \\leq |x| + |y|, with equality if and only if one of x, y is a nonnegative scalar multiple of the other. Proof : \\begin{split} \u27e8x+y,x+y\u27e9 \\\\ &= \u27e8x,x+y\u27e9 + \u27e8y,x+y\u27e9 \\\\ &= \u27e8x,x\u27e9 + \u27e8x,y\u27e9 + \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ &= \u27e8x,x\u27e9 + 2\u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ &\\leq \u27e8x,x\u27e9 + 2|x||y| + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by Cauchy\u2013Schwarz} \\\\ &= (|x| + |y|)^2 \\end{split} Equality holds exactly when \u27e8x,y\u27e9= |x||y| , or equivalently when |\u27e8x,y\u27e9|= |x||y| and \u27e8x,y\u27e9 \u2265 0 . These hold when one of x, y is a scalar multiple of the other and the scalar is nonnegative. \\square While the Cauchy\u2013Schwarz inequality says that the size of the product is at most the product of the sizes, the triangle inequality says: The size of the sum is at most the sum of the sizes.","title":"Theorem 2.2.6 (Triangle inequality)."},{"location":"ch02notes/#proposition-227-size-bounds","text":"For every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{i=1}^{n} |x_i|.","title":"Proposition 2.2.7 (Size bounds)."},{"location":"ch02notes/#distance-definition","text":"The modulus gives rise to a distance function on Rn that behaves as distance should. Define d: \\mathbb{R}^n \\times \\mathbb{R}^n \\xrightarrow{} \\mathbb{R} by d(x,y) = |y-x|","title":"Distance Definition"},{"location":"ch02notes/#theorem-228-distance-properties","text":"(D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n .","title":"Theorem 2.2.8 (Distance properties)."},{"location":"ch02notes/#angle-definition","text":"If x and y are nonzero vectors in \\mathbb{R}^n , define their angle \u03b8_{x,y} by the condition \\tag{2.2} \\cos \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x||y|}, \\quad 0 \\leq \u03b8_{x,y} \\leq \u03c0. In particular, two nonzero vectors x and y are orthogonal when \u27e8x,y\u27e9 = 0 . Thus 0 orthogonal to all vectors.","title":"Angle Definition"},{"location":"ch02notes/#three-altitudes-must-meet","text":"We have q-y \\perp x \\quad \\text{and} \\quad q-x \\perp y And we want to show \\left\\{ \\begin{array}{lr} \u27e8q-y,x\u27e9 = 0 \\\\ \u27e8q-x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 We have \\left\\{ \\begin{array}{lr} \u27e8q,x\u27e9 - \u27e8y,x\u27e9 = 0 \\\\ \u27e8q,y\u27e9 - \u27e8x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 So we have \u27e8q,x\u27e9 = \u27e8y,x\u27e9 = \u27e8x,y\u27e9 = \u27e8q,y\u27e9 \\\\ \\Longrightarrow \\\\ \u27e8q,x-y\u27e9 = 0 \\square","title":"three altitudes must meet"},{"location":"ch02notes/#23-analysis-continuous-mappings","text":"","title":"2.3 Analysis: Continuous Mappings"},{"location":"ch02notes/#mapping-from-mathbbrn-to-mathbbrm","text":"A mapping from \\mathbb{R}^n to \\mathbb{R}^m is some rule that assigns to each point x in \\mathbb{R}^n a point in \\mathbb{R}^m . Generally, mappings will be denoted by letters such as f, g, h .","title":"Mapping from \\mathbb{R}^n to \\mathbb{R}^m"},{"location":"ch02notes/#mappings-as-a-vector-space","text":"For a given dimension n , a given set A \u2282 \\mathbb{R}^n , and a second dimension m , let \\mathcal{M}(A,\\mathbb{R}^m) denote the set of all mappings f : A \\rightarrow \\mathbb{R}^m . This set forms a vector space over \\mathbb{R} (whose points are functions) under the operations + : \\mathcal{M}(A,\\mathbb{R}^m) \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (f +g)(x) = f(x)+g(x) \\quad\\text{for all } x \u2208 A, and \\cdot : \\mathbb{R} \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (a \\cdot f )(x) = a \\cdot f(x) \\quad\\text{for all } x \u2208 A.","title":"Mappings as a vector space"},{"location":"ch02notes/#sequence-in-mathbbrn","text":"Let A be a subset of \\mathbb{R}^n . A sequence in A is an infinite list of vectors \\{x_1,x_2,x_3,...\\} in A , often written \\{x_\u03bd\\} . Since a vector has n entries, each vector x_\u03bd in the sequence takes the form (x_{1,\u03bd},...,x_{n,\u03bd}) .","title":"Sequence in \\mathbb{R}^n"},{"location":"ch02notes/#definition-231-null-sequence","text":"The sequence (x_{\\nu }) in \\mathbb{R}^n is null if for every \u03b5 > 0 there exists some \u03bd_0 such that \\text{if } \u03bd > \u03bd_0 \\text{ then } |x_v| < \\epsilon. That is, a sequence is null if for every \u03b5 > 0 , all but finitely many terms of the sequence lie within distance \u03b5 of 0_n . If \\{x_\u03bd\\} is a null sequence and |y_\u03bd| \\leq |x_\u03bd| or all \u03bd then also \\{y_\u03bd\\} is null. \\{x_\u03bd\\} and \\{y_\u03bd\\} are null sequence, then |x_\u03bd + y_\u03bd| \\leq |x_\u03bd| + |y_\u03bd| , so \\{x_\u03bd + y_\u03bd\\} is null. \\{c x_\u03bd\\} is null seq, since |cx_\u03bd| = |c||x_\u03bd| So the set of null sequences in \\mathbb{R}^n forms a vector space. A vector sequence \\{x_\u03bd\\} is null if and only if the scalar sequence \\{|x_\u03bd|\\} is null.","title":"Definition 2.3.1 (Null Sequence)."},{"location":"ch02notes/#lemma-232-componentwise-nature-of-nullness","text":"The vector sequence \\{(x_{1,\u03bd}, \\cdots, x_{n,\u03bd})\\} is null if and only if each of its component scalar sequences \\{x_{j,\u03bd}\\} (j \\in \\{1, \\cdots, n\\}) is null. Proof Use |x_{j,\u03bd}| \\leq |x_\u03bd| \\leq \\sum_{j=1}^{n} |x_{j,\u03bd}| \\square","title":"Lemma 2.3.2 (Componentwise nature of nullness)."},{"location":"ch02notes/#definition-233-sequence-convergence-sequence-limit","text":"Let A be a subset of \\mathbb{R}^n . Consider a sequence \\{x_\u03bd\\} in A and a point p \u2208 \\mathbb{R}^n . The sequence \\{x_\u03bd\\} converges to p (or has limit p ), written \\{x_\u03bd\\} \u2192 p , if the sequence \\{x_\u03bd\u2212p\\} is null. When the limit p is a point of A , the sequence \\{x_\u03bd\\} converges in A . A sequence can only converges to one limit.","title":"Definition 2.3.3 (Sequence convergence, sequence limit)."},{"location":"ch02notes/#proposition-234-linearity-of-convergence","text":"Let \\{x_\u03bd\\} be a sequence in \\mathbb{R}^n converging to p , let \\{y_\u03bd\\} be a sequence in \\mathbb{R}^n converging to q , and let c be a scalar. Then the sequence \\{x_\u03bd +y_\u03bd\\} converges to p+q , and the sequence \\{c x_\u03bd\\} converges to c_p . Proof : \\begin{split} |(x_\u03bd + y_\u03bd) - (p + q)| &= |(x_\u03bd - p) + (y_\u03bd-q)| \\\\ &\\leq |x_\u03bd - p| + |y_\u03bd - q| \\end{split} So (x_\u03bd + y_\u03bd) - (p + q) is null. Then x_\u03bd + y_\u03bd \u2192 p+q . |c x_\u03bd - cp| = |c (x_\u03bd - p)| \\leq |c| |x_\u03bd - p| So c x_\u03bd - cp is null. Then c x_\u03bd \u2192 cp \\square","title":"Proposition 2.3.4 (Linearity of convergence)."},{"location":"ch02notes/#proposition-235-componentwise-nature-of-convergence","text":"The vector sequence \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} converges to the vector (p_1, \\cdots, p_n) if and only if each component scalar sequence \\{x_{j, \u03bd}\\} (j = 1, \\cdots, n) . converges to the scalar p_j . Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\square","title":"Proposition 2.3.5 (Componentwise nature of convergence)."},{"location":"ch02notes/#definition-236-continuity","text":"Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a mapping, and let p be a point of A . Then f is continuous at p if for every sequence \\{x_\u03bd\\} in A converging to p , the sequence f(x_\u03bd) converges to f(p) . The mapping f is continuous on A (or just continuous when A is clearly established) if it is continuous at each point p \u2208 A .","title":"Definition 2.3.6 (Continuity)."},{"location":"ch02notes/#modulus-function-is-continuous","text":"|| : \\mathbb{R}^n \\longrightarrow \\mathbb{R} Proof : Assume \\{x_\u03bd\\} \\rightarrow p , then from exercise 2.2.7 ||x_\u03bd| - |p|| \\leq | x_\u03bd - p | Then |x_\u03bd| - |p| is null, i.e. \\{f(x_\u03bd)\\} \\rightarrow f(p) . So || is continuous.","title":"Modulus function is continuous"},{"location":"ch02notes/#the-inner-product-is-continuous","text":"Given a \\in \\mathbb{R}^n , define T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad T(x) = \u27e8a,x\u27e9 T is continuous. From Cauchy-Schwarz inequality |\u27e8a,x\u27e9| \\leq |a||x| Given p \\in \\mathbb{R}^n |T(x_\u03bd) - T(p)| = |\u27e8a,x_v\u27e9 - \u27e8a,p\u27e9| = |\u27e8a,x_\u03bd - p\u27e9| \\leq |a| | x_\u03bd - p | Since we know |x_\u03bd - p| \\rightarrow 0 , we have |T(x_\u03bd) - T(p)| \\rightarrow 0 . \\square","title":"The inner product is continuous"},{"location":"ch02notes/#jth-coordinate-function-map","text":"Consider the following mapping \\pi_j : \\mathbb{R}^n \\longrightarrow \\mathbb{R}, \\quad \\pi_j (x_1, \\cdots, x_n) = x_j Since \\pi_j(x) = \u27e8e_j,x\u27e9 , this mapping is continuous.","title":"jth coordinate function map"},{"location":"ch02notes/#proposition-237-vector-space-properties-of-continuity","text":"Let A be a subset of \\mathbb{R}^n , let f,g : A \\longrightarrow \\mathbb{R}^m be continuous mappings, and let c \\in \\mathbb{R} . Then the sum and the scalar multiple mappings f+g,cf : A \\longrightarrow \\mathbb{R}^m are continuous. Thus the set of continuous mappings from A to \\mathbb{R}^m forms a vector subspace of \\mathcal{M}(A, \\mathbb{R}^m) . Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . Then \\{f(x_\u03bd)\\} \\rightarrow f(p), \\{g(x_\u03bd)\\} \\rightarrow g(p) . Then from Proposition 2.3.4 (Linearity of convergence), \\{f(x_\u03bd) + g(x_\u03bd)\\} \\rightarrow f(p) + g(p) , i.e. \\{(f+g)(x_\u03bd)\\} \\rightarrow (f+g)(p) . So f+g is continuous. For the same reason cf is continuous. \\square","title":"Proposition 2.3.7 (Vector space properties of continuity)."},{"location":"ch02notes/#proposition-238-persistence-of-continuity-under-composition","text":"Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let B be a superset of f(A) in \\mathbb{R}^m , and let g : B \\longrightarrow \\mathbb{R}^l be a continuous mapping. Then the composition mapping g \\circ f : A \\longrightarrow \\mathbb{R}^l is continuous. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square","title":"Proposition 2.3.8 (Persistence of continuity under composition)."},{"location":"ch02notes/#theorem-239-componentwise-nature-of-continuity","text":"Let A \u2282 \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m have component functions f_1,...,f_m , and let p be a point in A . Then f \\text{ is continuous at } p \\Longleftrightarrow \\text{each } f_i \\text{ is continuous at } p. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square","title":"Theorem 2.3.9 (Componentwise nature of continuity)."},{"location":"ch02notes/#some-examples","text":"Consider f(x, y) = \\begin{cases} \\frac{2xy}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Can the constant b be specified to make f continuous at 0 ? Take a sequence \\{(x_\u03bd, y_\u03bd)\\} \\rightarrow 0 along the line y= mx . f(x_\u03bd, y_\u03bd) = \\\\ \\frac{2x_\u03bd y_\u03bd}{x_\u03bd^2 + y_\u03bd^2} = \\\\ \\frac{2m}{1 + m^2} hence f cannot be made continuous at 0 . \\square Now consider g(x, y) = \\begin{cases} \\frac{x^2y}{x^4 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} If (x,y) approaches to (0,0) with y = x , then g(x,y) = \\frac{x^3}{x^4 + x^2} = \\frac{x}{x^2 + 1} \\rightarrow 0 If (x,y) approaches to (0,0) with y = x^2 , then g(x,y) = \\frac{x^4}{x^4 + x^4} = \\frac{1}{2} hence f cannot be made continuous at 0 . \\square","title":"Some examples"},{"location":"ch02notes/#the-size-bounds-to-prove-continuity","text":"Consider h(x, y) = \\begin{cases} \\frac{x^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Note from 2.2.7, we have |x| \\leq |(x,y)| , so \\frac{x^3}{x^2 + y^2} \\leq \\frac{|(x,y)|^3}{|(x,y)|^2} = |(x,y)| \\rightarrow 0 So setting b = 0 makes it continuous at 0 .","title":"The size bounds to prove continuity"},{"location":"ch02notes/#summary-of-the-3-examples","text":"The straight line test can prove that a limit does not exist, or it can determine the only candidate for the value of the limit, but it cannot prove that the candidate value is the limit. When the straight line test determines a candidate value of the limit, approaching along a curve can further support the candidate, or it can prove that the limit does not exist by determining a di\ufb00erent candidate as well. The size bounds can prove that a limit does exist, but they can only suggest that a limit does not exist.","title":"Summary of the 3 examples"},{"location":"ch02notes/#proposition-2310-persistence-of-inequality","text":"Let A be a subset of \\mathbb{R}^n and let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let p be a point of A , let b be a point of \\mathbb{R}^m , and suppose that f(p) \\neq b . Then there exists some \\epsilon > 0 , such that \\text{for all } x \\in A \\text{such that } |x-p| < \\epsilon, f(x) \\neq b. Proof : Assume otherwise, then for \\nu \\in \\mathbb{N} , we can find x_\u03bd , such that | x_\u03bd - p | < 1/\u03bd and f(x_\u03bd) = b . Since f is continuous, then f(p) = b . We have a contradiction. \\square","title":"Proposition 2.3.10 (Persistence of inequality)."},{"location":"unsolved/","text":"2.1.7. Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.)","title":"Unsolved Problems"},{"location":"unsolved/#217","text":"Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.)","title":"2.1.7."}]}