{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! This is my notes to the exercises of Professor Jerry Shurman's \"Calculus and Analysis in Euclidean Space\". You can find this book from the publisher here .","title":"Home"},{"location":"#welcome","text":"This is my notes to the exercises of Professor Jerry Shurman's \"Calculus and Analysis in Euclidean Space\". You can find this book from the publisher here .","title":"Welcome!"},{"location":"ch01ex/","text":"Chapter 1 Results from One-Variable Calculus 1.1 The Real Number System 1.1.1. Referring only to the field axioms, show that 0x = 0 for all x \u2208 R . Proof : Use (m2), (d1), (a2), (m2), we have 0x + x = 0x + 1x = (0 + 1)x = 1x = x Let y be the additive inverse of x , then (0x + x) + y = 0x + (x + y) = 0x + 0 = 0x \\\\ x + y = 0 So 0x = 0 \\square 1.1.2. Prove that in every ordered field, 1 is positive. Prove that the complex number field \\mathbb{C} cannot be made an ordered field. Proof : If 1 = 0 , then given x \\in \\mathbb{F} , x = 1x = 0x = 0 . Then \\mathbb{F} = \\{0\\} . Now we assume 1 \\neq 0 . If 1 \\not \\in \\mathbb{F}^+ , then -1 \\in \\mathbb{F}^+ . Then note (-1)(-1) + (-1)1 = (-1)((-1) + 1) = (-1)0 = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) + (-1) = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) = 1 So from (o3), 1 \\in \\mathbb{F}^+ , we have a contradiction. Then 1 \\in \\mathbb{F}^+ . For the second part. if i \\in \\mathbb{C}^+ , then i \\cdot i = -1 \\in \\mathbb{C}^- , we have a contradiction. if i \\in \\mathbb{C}^- , then (-i) \\cdot (-i) = i^2 = -1 \\in \\mathbb{C}^- , we have a contradiction again. \\square 1.1.3 Use a completeness property of the real number system to show that 2 has a positive square root. Proof : Let A = \\{x : x \\geq 0 \\text{ and } x^2 < 2\\} A is not empty, e.g. 1 \\in A . A is upper bounded, e.g. 2 is an upper bound. Now assume r is the least upper bound. If r^2 < 2 , then let d = 2 - r^2 . We can find n such that 2r/n + 1/n^2 < d , then (r + 1/n)^2 = r^2 + 2r/n + 1/n^2 < r^2 + d = 2 That means r < r+1/n \\in A , so r is not an upper bound, we have a contradiction. If r^2 > 2 , then let d = r^2 - 2 . We can find n such that 2r/n - 1/n^2 < d , then (r - 1/n)^2 = r^2 - 2r/n + 1/n^2 > r^2 - d = 2 That means r - 1/n is an upper bound of A . So r is not the least upper bound. Then we have to have r^2 = 2 . \\square 1.1.4. (a) Prove by induction that \\sum_{i=1}^{n} i^2 = \\frac{n(n+1)(2n+1)}{6} \\text{ for all } n \\in \\mathbb{Z}^+ Proof : When n = 1 , 1^2 = \\frac{1 \\cdot 2 \\cdot 3}{6} Assume when n = k this is correct \\begin{split} \\sum_{i=1}^{k+1} i^2 &= \\frac{k(k+1)(2k+1)}{6} + (k+1)^2 \\\\ &= \\frac{[k(2k+1) + 6(k+1)](k+1)}{6} \\\\ &= \\frac{(k+1)(k+2)(2(k+1)+1)}{6} \\\\ \\end{split} So when n = k + 1 , it's still correct. \\square (b) (Bernoulli\u2019s inequality) For every real number r \u2265 \u22121 , prove that (1+r)^n \\geq 1 + rn \\text{ for all } n \\in \\mathbb{N}. Proof : It's correct when n = 1 . (1+r)^{k+1} \\geq (1+rk)(1+r) = 1 + r(k+1) + r^2k \\geq 1 + r(k+1) So it's correct for all n \\in \\mathbb{N} . \\square (c) For what positive integers n is 2^n > n^3 ? Proof : Note when n = 10 2^{10} = 1024 > 1000 = 10^3 Assume 2^k > k^3, k \\geq 10 (k+1)^3 = (k^3 + 3k^2 + 3k + 1) < k^3 + 3k^2 + 3k^2 + 3k^2 \\\\ = k^3 + 9k^2 < k^3 + k^3 = 2 k^3 < 2 \\cdot 2^k = 2^{k+1} \\square 1.1.5. (a) Use the induction theorem to show that for every natural number m , the sum m+n and the product mn are again natural for every natural number n . Thus \\mathbb{N} is closed under addition and multiplication, and consequently so is \\mathbb{Z} . Proof : skip (b) Which of the field axioms continue to hold for the natural numbers? Solution : The following does not hold: (a3) Existence of additive inverses (m3) Existence of multiplicative inverses (c) Which of the field axioms continue to hold for the integers? The following does not hold: (m3) Existence of multiplicative inverses \\square 1.1.6. For every positive integer n , let \\mathbb{Z}/n\\mathbb{Z} denote the set \\{0,1,...,n\u22121\\} with the usual operations of addition and multiplication carried out taking remainders on division by n . That is, add and multiply in the usual fashion but subject to the additional condition that n = 0 . For example, in \\mathbb{Z}/5\\mathbb{Z} we have 2+4 = 1 and 2\u00b74 = 3 . For what values of n does \\mathbb{Z}/n\\mathbb{Z} form a field? Solution : If n is a composite number, and assume n = a \\cdot b, a > 1, b > 1 . If a has a multiplication inverse a' , then a \\cdot a' \\equiv 1 \\pmod n So a \\cdot a' - kn = 1 , but a | n and a | a , so a | 1 , we have a contradiction. So a does not have a multiplication inverse, then \\mathbb{Z}/n\\mathbb{Z} is not a field. If n is prime number, and 1 < a < n , then (a, n) = 1 . We can find p, q such that ap + nq = 1 . So ap \\equiv 1 \\pmod n Then p is the multiplication inverse of a . \\mathbb{Z}/n\\mathbb{Z} is a field. \\square 1.2 Foundational and Basic Theorems 1.2.1. Use the intermediate value theorem to show that 2 has a positive square root. 1.3 1.3.1. (a) Let n \u2208 \\mathbb{N} . What is the (2n + 1) st-degree Taylor polynomial T_{2n+1}(x) for the function f(x) = \\sin x at 0 ? (The reason for the strange indexing here is that every second term of the Taylor polynomial is 0 .) Prove that \\sin x is equal to the limit of T_{2n+1}(x) as n \u2192 \u221e , similarly to the argument in the text for e^x . Also find T_{2n}(x) for f(x) = \\cos x at 0 , and explain why the argument for \\sin shows that \\cos x is the limit of its even-degree Taylor polynomials as well. Solution T_{2n+1}(x) = \\frac{x}{1!} - \\frac{x^3}{3!} + + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots The Taylor remainder is |R_{2n+1}(x)| = \\left| \\frac{\\sin^{(2n+2)}(c) x^{2n+2}}{(2n+2)!} \\right| \\leq \\frac{x^{2n+2}}{(2n+2)!} \\rightarrow 0 For \\cos x T_{2n}(x) = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots |R_{2n}(x)| = \\left| \\frac{\\cos^{(2n+1)}(c) x^{2n+1}}{(2n+1)!} \\right| \\leq \\frac{x^{2n+1}}{(2n+1)!} \\rightarrow 0 \\square (b) Many years ago, the author\u2019s high-school physics textbook asserted, ba\ufb04ingly, that the approximation \\sin x \u2248 x is good for x up to 8^\u25e6 . Deconstruct. Solution : 8^\\circ in terms of radius is \\frac{\\pi }{2} \\times 8 / 90 \\approx 0.14 . And we have |R_1(x)| \\leq \\frac{x^2}{2} = \\frac{0.14^2}{2} \\approx 0.01 So the error with 8^\\circ is less than 1\\% . \\square 1.3.2. What is the n th-degree Taylor polynomial T_n(x) for the following functions at 0 ? (a) f(x) = \\arctan x . (This exercise is not just a matter of routine mechanics. One way to proceed involves the geometric series, and another makes use of the factorization 1 + x^2 = (1-ix)(1+ix).) Solution : Here are steps in Understanding Analysis First, we know \\arctan ' x = \\frac{1}{1 + x^2} Note the following geometric series converges in (-1, 1) . \\frac{1}{1 - x} = 1 + x + x^2 + \\cdots We replace x with -x^2 and get \\frac{1}{1 + x^2} = 1 - x^2 + x^4 - x^6 + \\cdots Now take anti-differentiation on both side: \\arctan x = x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7} + \\cdots This is the n th-degree Taylor polynomial T_n(x) for \\arctan x . \\square (b) f(x) = (1 + x)^\u03b1 where \u03b1 \u2208 \\mathbb{R} . (Although the answer can be written in a uniform way for all \u03b1 , it behaves di\ufb00erently when \u03b1 \u2208 \\mathbb{N} . Introduce the generalized binomial coe\ufb03cient symbol \\binom{\u03b1}{k} = \\frac{\u03b1(\u03b1-1)(\u03b1-2)\\cdots (\u03b1-k+1)}{k!}, k \\in \\mathbb{N} to help produce a tidy answer.) Solution : f'(x) = \u03b1(1+x)^{\u03b1-1} = \\binom{\u03b1}{1} \\\\ f''(x) = \u03b1(\u03b1-1)(1+x)^{\u03b1-2} = 2! \\binom{\u03b1}{2} \\\\ \\cdots \\\\ f^{n}(x) = \u03b1(\u03b1-1)\\cdots(\u03b1-n+1) (1+x)^{\u03b1-n} = n! \\binom{\u03b1}{n} \\\\ So T_n(x) = 1 + \\binom{\u03b1}{1} x + \\binom{\u03b1}{2} x^2 + \\cdots + \\binom{\u03b1}{n} x^n \\square 1.3.3. (a) Further tighten the numerical estimate of \\ln(1.1) from this section by reasoning as follows. As n increases, the Taylor polynomials T_n(0.1) add terms of decreasing magnitude and alternating sign. Therefore T_4(0.1) underestimates \\ln(1.1) . Now that we know this, it is useful to find the smallest possible value of the remainder (by setting c = 0.1 rather than c = 0 in the formula). Then \\ln(1.1) lies between T_4(0.1) plus this smallest possible remainder value and T_4(0.1) plus the largest possible remainder value, obtained in the section. Supply the numbers, and verify by machine that the tighter estimate of \\ln(1.1) is correct. Solution : T_n(x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\cdots = \\sum_{k=1}^{n} (-1)^{k-1} \\frac{x^k}{k}, And we also have R_n(x) = \\frac{(-1)^n x^{n+1}}{(1+c)^{n+1}(n+1)} And we can plug c = 0, 0.1, x = 0.1 into the above equation. So we have \\frac{0.1^5}{(1.1^5 \\cdot 5)} \\leq R_4(x) \\leq \\frac{0.1^5}{(1.0^5 \\cdot 5)} We calculate \\ln (1.1) = 0.09531018 , T_4(1.1) = 0.09530833 T_4(1.1) + \\frac{0.1^5}{(1.1^5 \\cdot 5)} = 0.09530957 Which is indeed than the lower bound obtained in the book, which is 0.09530633 . \\square (b) In Figure 1.1, identify the graphs of T_1 through T_5 and the graph of \\ln near x = 0 and near x = 2 . Solution : It's easy to see T_1 is a straight line. Now focus on x \\geq 0 . From part (a), we know T_1(x) > T_3(x) > T_5(x) > \\ln (x) and T_2(x) < T_4(x) < \\ln (x) . Then focus on x < 0 , notice T_{n}(x) - T_{n+1}(x) = (-1)^{n+1} \\frac{x^{n+1}}{n+1} No matter of n , it is always positive, so T_1(x) > T_2(x) > \\cdots > T_5(x) > \\ln (x) . \\square 1.3.4. Working by hand, use the third-degree Taylor polynomial for \\sin(x) at 0 to approximate a decimal representation of \\sin(0.1) . Also compute the decimal representation of an upper bound for the error of the approximation. Bound \\sin(0.1) between two decimal representations. Solution : T_3(x) = \\frac{x}{1} - \\frac{x^3}{3!} And also |R_3(x)| \\leq \\frac{x^4}{4!} T_3(0.1) = 0.09983333 \\\\ |R_3(0.1)| \\leq \\frac{0.1^4}{4!} = 0.00000417 \\\\ 0.09982916 \\leq \\sin (0.1) \\leq 0.0998375 With calculator, \\sin (0.1) \\approx 0.09983342 . 1.3.5. Use a second-degree Taylor polynomial to approximate \\sqrt[]{4.2} . Use Taylor\u2019s theorem to find a guaranteed accuracy of the approximation and thus to find upper and lower bounds for \\sqrt[]{4.2} . Solution : Consider f(x) = \\sqrt[]{4+x} \\begin{split} f'(x) &= \\frac{1}{2} (4+x)^{-\\frac{1}{2}} \\\\ f''(x) &= -\\frac{1}{2^2} (4+x) ^{-\\frac{3}{2}} \\\\ f^{(3)}(x) &= \\frac{1 \\cdot 3}{2^3} (4+x) ^{-\\frac{5}{2}} \\\\ \\end{split} So we have T_2(x) = f(0) + f'(0) x + \\frac{f''(0)}{2!}x^2 \\\\ = 2 + \\frac{x}{4} - \\frac{x^2}{64} The remainder is: |R_2(x)| = \\left| \\frac{f^{(3)}(c)}{3!}x^3 \\right| \\leq \\frac{f^{(3)}(0)}{3!}x^3 = \\frac{3x^3}{3!2^8} = \\frac{x^3}{512} Then we have T_2(0.2) = 2.049375 \\\\ |R_2(0.2)| = \\frac{0.2^3}{512} = 0.00001562 \\\\ \\sqrt[]{4.2} = 2.04939015 \\\\ Finally, we have 2.04935938 = T_2(0.2) - |R_2(0.2)| < \\sqrt[]{4.2} < T_2(0.2) + |R_2(0.2)| = 2.04939062 \\square 1.3.6. (a) Another proof of Taylor\u2019s Theorem uses the fundamental theorem of integral calculus once and then integrates by parts repeatedly. Begin with the hypotheses of Theorem 1.3.3, and let x \u2208 I . By the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(t) dt Let u = f'(t) and v = t - x , so that the integral is \\int_{a}^{x} u v' , and integrating by parts gives \\int_{a}^{x} u v' = uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v = f'(x)(x - x) - f'(a) (a - x) - \\int_{a}^{x} f''(t)(t-x) \\\\ = f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt So f(x) = f(a) + f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt Now let u = f''(t) and v = \\frac{1}{2} (t - x)^2 , \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f''(x) \\frac{1}{2} (x-x)^2 - f''(a) \\frac{1}{2}(a - x)^2 - \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt \\end{split} So f(x) = f(a) + f'(a) (x - a) + f''(a) \\frac{(x-a)^2}{2} + \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt Then we can use induction and assume f(x) = T_n(x) + (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt Let u = f^{(n+1)}(t), v = \\frac{(t-x)^{(n+1)}}{(n+1)!} \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f^{(n+1)}(x) \\frac{(x-x)^{n+1}}{{(n+1)}!} - f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} - \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\\\ &= (-1) \\cdot \\left( f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} + \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\right) \\end{split} No mattern n is even or odd, (-1)^{n} (a-x)^n = (x-a)^n So f(x) = T_{n+1} + (-1)^{n+1} \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} dt Whereas the expression for f(x)\u2212T_n(x) in Theorem 1.3.3 is called the Lagrange form of the remainder, this exercise has derived the integral form of the remainder. Use the extreme value theorem and the intermediate value theorem to derive the Lagrange form of the remainder from the integral form. Proof : We want to estimate A = (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} Assume m \\leq f^{(n+1)}(t) \\leq M , and let B = (-1)^n \\int_{a}^{x} \\frac{(t-x)^n}{n!} dt = (-1)^n \\frac{(t-x)^{(n+1)}}{{(n+1)}!}\\Bigg|_{a}^{x} = (-1)^{n+1} \\frac{(a-x)^{(n+1)}}{{(n+1)}!} = \\frac{(x-a)^{(n+1)}}{{(n+1)}!} Note (-1)^n \\frac{(t-x)^n}{n!} \\geq 0, t \\in [a, x] , so m \\frac{(x-a)^{(n+1)}}{{(n+1)}!} \\leq (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} \\leq M \\frac{(x-a)^{(n+1)}}{{(n+1)}!} i.e. mB \\leq A \\leq MB Since f^{(n+1)} is continuous, with extreme value theorem, we can find f^{(n+1)}(t_m) = m, f^{(n+1)}(t_M) = M . Then with intermediate value theorem, we can find c , such that f^{(n+1)}(c) = A/B , i.e. (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1} \\square (b) Use the integral form of the remainder to show that \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] Proof : f^{(n+1)}(t) = \\frac{(-1)^n n!}{(1+t)^{n+1}} So \\begin{split} (-1)^n \\int_{0}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt &= (-1)^{2n} \\int_{0}^{x} \\frac{(-1)^n n!}{(1+t)^{n+1}} \\frac{(t-x)^n}{n!} dt \\\\ &= \\int_{0}^{x} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\end{split} Consider, x \\in (-1, -1/2] , and t \\geq x . t \\geq x > (2+t)x \\\\ \\Rightarrow \\\\ t > (2+t)x \\\\ \\Rightarrow \\\\ t - x > x + tx \\\\ \\Rightarrow \\\\ \\frac{t-x}{1+t} > x \\\\ So \\left| \\frac{t-x}{1+t} \\right| < |x| Then \\begin{split} \\left| \\int_{x}^{0} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\right| & \\leq \\int_{x}^{0} \\left| \\frac{(t-x)^n}{(1+t)^{n+1}} \\right| dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} \\left| \\frac{(t-x)}{(1+t)} \\right|^n dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} |x|^n dt\\\\ &= \\frac{|x|^{n+1}}{1+x} \\to 0 \\end{split} The book has proved the case for (-1/2, 1] . Then we can conclude \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] \\square","title":"Chapter 01 Exercises"},{"location":"ch01ex/#chapter-1-results-from-one-variable-calculus","text":"","title":"Chapter 1 Results from One-Variable Calculus"},{"location":"ch01ex/#11-the-real-number-system","text":"","title":"1.1 The Real Number System"},{"location":"ch01ex/#111","text":"Referring only to the field axioms, show that 0x = 0 for all x \u2208 R . Proof : Use (m2), (d1), (a2), (m2), we have 0x + x = 0x + 1x = (0 + 1)x = 1x = x Let y be the additive inverse of x , then (0x + x) + y = 0x + (x + y) = 0x + 0 = 0x \\\\ x + y = 0 So 0x = 0 \\square","title":"1.1.1."},{"location":"ch01ex/#112","text":"Prove that in every ordered field, 1 is positive. Prove that the complex number field \\mathbb{C} cannot be made an ordered field. Proof : If 1 = 0 , then given x \\in \\mathbb{F} , x = 1x = 0x = 0 . Then \\mathbb{F} = \\{0\\} . Now we assume 1 \\neq 0 . If 1 \\not \\in \\mathbb{F}^+ , then -1 \\in \\mathbb{F}^+ . Then note (-1)(-1) + (-1)1 = (-1)((-1) + 1) = (-1)0 = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) + (-1) = 0 \\\\ \\Rightarrow \\\\ (-1)(-1) = 1 So from (o3), 1 \\in \\mathbb{F}^+ , we have a contradiction. Then 1 \\in \\mathbb{F}^+ . For the second part. if i \\in \\mathbb{C}^+ , then i \\cdot i = -1 \\in \\mathbb{C}^- , we have a contradiction. if i \\in \\mathbb{C}^- , then (-i) \\cdot (-i) = i^2 = -1 \\in \\mathbb{C}^- , we have a contradiction again. \\square","title":"1.1.2."},{"location":"ch01ex/#113","text":"Use a completeness property of the real number system to show that 2 has a positive square root. Proof : Let A = \\{x : x \\geq 0 \\text{ and } x^2 < 2\\} A is not empty, e.g. 1 \\in A . A is upper bounded, e.g. 2 is an upper bound. Now assume r is the least upper bound. If r^2 < 2 , then let d = 2 - r^2 . We can find n such that 2r/n + 1/n^2 < d , then (r + 1/n)^2 = r^2 + 2r/n + 1/n^2 < r^2 + d = 2 That means r < r+1/n \\in A , so r is not an upper bound, we have a contradiction. If r^2 > 2 , then let d = r^2 - 2 . We can find n such that 2r/n - 1/n^2 < d , then (r - 1/n)^2 = r^2 - 2r/n + 1/n^2 > r^2 - d = 2 That means r - 1/n is an upper bound of A . So r is not the least upper bound. Then we have to have r^2 = 2 . \\square","title":"1.1.3"},{"location":"ch01ex/#114","text":"(a) Prove by induction that \\sum_{i=1}^{n} i^2 = \\frac{n(n+1)(2n+1)}{6} \\text{ for all } n \\in \\mathbb{Z}^+ Proof : When n = 1 , 1^2 = \\frac{1 \\cdot 2 \\cdot 3}{6} Assume when n = k this is correct \\begin{split} \\sum_{i=1}^{k+1} i^2 &= \\frac{k(k+1)(2k+1)}{6} + (k+1)^2 \\\\ &= \\frac{[k(2k+1) + 6(k+1)](k+1)}{6} \\\\ &= \\frac{(k+1)(k+2)(2(k+1)+1)}{6} \\\\ \\end{split} So when n = k + 1 , it's still correct. \\square (b) (Bernoulli\u2019s inequality) For every real number r \u2265 \u22121 , prove that (1+r)^n \\geq 1 + rn \\text{ for all } n \\in \\mathbb{N}. Proof : It's correct when n = 1 . (1+r)^{k+1} \\geq (1+rk)(1+r) = 1 + r(k+1) + r^2k \\geq 1 + r(k+1) So it's correct for all n \\in \\mathbb{N} . \\square (c) For what positive integers n is 2^n > n^3 ? Proof : Note when n = 10 2^{10} = 1024 > 1000 = 10^3 Assume 2^k > k^3, k \\geq 10 (k+1)^3 = (k^3 + 3k^2 + 3k + 1) < k^3 + 3k^2 + 3k^2 + 3k^2 \\\\ = k^3 + 9k^2 < k^3 + k^3 = 2 k^3 < 2 \\cdot 2^k = 2^{k+1} \\square","title":"1.1.4."},{"location":"ch01ex/#115","text":"(a) Use the induction theorem to show that for every natural number m , the sum m+n and the product mn are again natural for every natural number n . Thus \\mathbb{N} is closed under addition and multiplication, and consequently so is \\mathbb{Z} . Proof : skip (b) Which of the field axioms continue to hold for the natural numbers? Solution : The following does not hold: (a3) Existence of additive inverses (m3) Existence of multiplicative inverses (c) Which of the field axioms continue to hold for the integers? The following does not hold: (m3) Existence of multiplicative inverses \\square","title":"1.1.5."},{"location":"ch01ex/#116","text":"For every positive integer n , let \\mathbb{Z}/n\\mathbb{Z} denote the set \\{0,1,...,n\u22121\\} with the usual operations of addition and multiplication carried out taking remainders on division by n . That is, add and multiply in the usual fashion but subject to the additional condition that n = 0 . For example, in \\mathbb{Z}/5\\mathbb{Z} we have 2+4 = 1 and 2\u00b74 = 3 . For what values of n does \\mathbb{Z}/n\\mathbb{Z} form a field? Solution : If n is a composite number, and assume n = a \\cdot b, a > 1, b > 1 . If a has a multiplication inverse a' , then a \\cdot a' \\equiv 1 \\pmod n So a \\cdot a' - kn = 1 , but a | n and a | a , so a | 1 , we have a contradiction. So a does not have a multiplication inverse, then \\mathbb{Z}/n\\mathbb{Z} is not a field. If n is prime number, and 1 < a < n , then (a, n) = 1 . We can find p, q such that ap + nq = 1 . So ap \\equiv 1 \\pmod n Then p is the multiplication inverse of a . \\mathbb{Z}/n\\mathbb{Z} is a field. \\square","title":"1.1.6."},{"location":"ch01ex/#12-foundational-and-basic-theorems","text":"","title":"1.2 Foundational and Basic Theorems"},{"location":"ch01ex/#121","text":"Use the intermediate value theorem to show that 2 has a positive square root.","title":"1.2.1."},{"location":"ch01ex/#13","text":"","title":"1.3"},{"location":"ch01ex/#131","text":"(a) Let n \u2208 \\mathbb{N} . What is the (2n + 1) st-degree Taylor polynomial T_{2n+1}(x) for the function f(x) = \\sin x at 0 ? (The reason for the strange indexing here is that every second term of the Taylor polynomial is 0 .) Prove that \\sin x is equal to the limit of T_{2n+1}(x) as n \u2192 \u221e , similarly to the argument in the text for e^x . Also find T_{2n}(x) for f(x) = \\cos x at 0 , and explain why the argument for \\sin shows that \\cos x is the limit of its even-degree Taylor polynomials as well. Solution T_{2n+1}(x) = \\frac{x}{1!} - \\frac{x^3}{3!} + + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots The Taylor remainder is |R_{2n+1}(x)| = \\left| \\frac{\\sin^{(2n+2)}(c) x^{2n+2}}{(2n+2)!} \\right| \\leq \\frac{x^{2n+2}}{(2n+2)!} \\rightarrow 0 For \\cos x T_{2n}(x) = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots |R_{2n}(x)| = \\left| \\frac{\\cos^{(2n+1)}(c) x^{2n+1}}{(2n+1)!} \\right| \\leq \\frac{x^{2n+1}}{(2n+1)!} \\rightarrow 0 \\square (b) Many years ago, the author\u2019s high-school physics textbook asserted, ba\ufb04ingly, that the approximation \\sin x \u2248 x is good for x up to 8^\u25e6 . Deconstruct. Solution : 8^\\circ in terms of radius is \\frac{\\pi }{2} \\times 8 / 90 \\approx 0.14 . And we have |R_1(x)| \\leq \\frac{x^2}{2} = \\frac{0.14^2}{2} \\approx 0.01 So the error with 8^\\circ is less than 1\\% . \\square","title":"1.3.1."},{"location":"ch01ex/#132","text":"What is the n th-degree Taylor polynomial T_n(x) for the following functions at 0 ? (a) f(x) = \\arctan x . (This exercise is not just a matter of routine mechanics. One way to proceed involves the geometric series, and another makes use of the factorization 1 + x^2 = (1-ix)(1+ix).) Solution : Here are steps in Understanding Analysis First, we know \\arctan ' x = \\frac{1}{1 + x^2} Note the following geometric series converges in (-1, 1) . \\frac{1}{1 - x} = 1 + x + x^2 + \\cdots We replace x with -x^2 and get \\frac{1}{1 + x^2} = 1 - x^2 + x^4 - x^6 + \\cdots Now take anti-differentiation on both side: \\arctan x = x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7} + \\cdots This is the n th-degree Taylor polynomial T_n(x) for \\arctan x . \\square (b) f(x) = (1 + x)^\u03b1 where \u03b1 \u2208 \\mathbb{R} . (Although the answer can be written in a uniform way for all \u03b1 , it behaves di\ufb00erently when \u03b1 \u2208 \\mathbb{N} . Introduce the generalized binomial coe\ufb03cient symbol \\binom{\u03b1}{k} = \\frac{\u03b1(\u03b1-1)(\u03b1-2)\\cdots (\u03b1-k+1)}{k!}, k \\in \\mathbb{N} to help produce a tidy answer.) Solution : f'(x) = \u03b1(1+x)^{\u03b1-1} = \\binom{\u03b1}{1} \\\\ f''(x) = \u03b1(\u03b1-1)(1+x)^{\u03b1-2} = 2! \\binom{\u03b1}{2} \\\\ \\cdots \\\\ f^{n}(x) = \u03b1(\u03b1-1)\\cdots(\u03b1-n+1) (1+x)^{\u03b1-n} = n! \\binom{\u03b1}{n} \\\\ So T_n(x) = 1 + \\binom{\u03b1}{1} x + \\binom{\u03b1}{2} x^2 + \\cdots + \\binom{\u03b1}{n} x^n \\square","title":"1.3.2."},{"location":"ch01ex/#133","text":"(a) Further tighten the numerical estimate of \\ln(1.1) from this section by reasoning as follows. As n increases, the Taylor polynomials T_n(0.1) add terms of decreasing magnitude and alternating sign. Therefore T_4(0.1) underestimates \\ln(1.1) . Now that we know this, it is useful to find the smallest possible value of the remainder (by setting c = 0.1 rather than c = 0 in the formula). Then \\ln(1.1) lies between T_4(0.1) plus this smallest possible remainder value and T_4(0.1) plus the largest possible remainder value, obtained in the section. Supply the numbers, and verify by machine that the tighter estimate of \\ln(1.1) is correct. Solution : T_n(x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\cdots = \\sum_{k=1}^{n} (-1)^{k-1} \\frac{x^k}{k}, And we also have R_n(x) = \\frac{(-1)^n x^{n+1}}{(1+c)^{n+1}(n+1)} And we can plug c = 0, 0.1, x = 0.1 into the above equation. So we have \\frac{0.1^5}{(1.1^5 \\cdot 5)} \\leq R_4(x) \\leq \\frac{0.1^5}{(1.0^5 \\cdot 5)} We calculate \\ln (1.1) = 0.09531018 , T_4(1.1) = 0.09530833 T_4(1.1) + \\frac{0.1^5}{(1.1^5 \\cdot 5)} = 0.09530957 Which is indeed than the lower bound obtained in the book, which is 0.09530633 . \\square (b) In Figure 1.1, identify the graphs of T_1 through T_5 and the graph of \\ln near x = 0 and near x = 2 . Solution : It's easy to see T_1 is a straight line. Now focus on x \\geq 0 . From part (a), we know T_1(x) > T_3(x) > T_5(x) > \\ln (x) and T_2(x) < T_4(x) < \\ln (x) . Then focus on x < 0 , notice T_{n}(x) - T_{n+1}(x) = (-1)^{n+1} \\frac{x^{n+1}}{n+1} No matter of n , it is always positive, so T_1(x) > T_2(x) > \\cdots > T_5(x) > \\ln (x) . \\square","title":"1.3.3."},{"location":"ch01ex/#134","text":"Working by hand, use the third-degree Taylor polynomial for \\sin(x) at 0 to approximate a decimal representation of \\sin(0.1) . Also compute the decimal representation of an upper bound for the error of the approximation. Bound \\sin(0.1) between two decimal representations. Solution : T_3(x) = \\frac{x}{1} - \\frac{x^3}{3!} And also |R_3(x)| \\leq \\frac{x^4}{4!} T_3(0.1) = 0.09983333 \\\\ |R_3(0.1)| \\leq \\frac{0.1^4}{4!} = 0.00000417 \\\\ 0.09982916 \\leq \\sin (0.1) \\leq 0.0998375 With calculator, \\sin (0.1) \\approx 0.09983342 .","title":"1.3.4."},{"location":"ch01ex/#135","text":"Use a second-degree Taylor polynomial to approximate \\sqrt[]{4.2} . Use Taylor\u2019s theorem to find a guaranteed accuracy of the approximation and thus to find upper and lower bounds for \\sqrt[]{4.2} . Solution : Consider f(x) = \\sqrt[]{4+x} \\begin{split} f'(x) &= \\frac{1}{2} (4+x)^{-\\frac{1}{2}} \\\\ f''(x) &= -\\frac{1}{2^2} (4+x) ^{-\\frac{3}{2}} \\\\ f^{(3)}(x) &= \\frac{1 \\cdot 3}{2^3} (4+x) ^{-\\frac{5}{2}} \\\\ \\end{split} So we have T_2(x) = f(0) + f'(0) x + \\frac{f''(0)}{2!}x^2 \\\\ = 2 + \\frac{x}{4} - \\frac{x^2}{64} The remainder is: |R_2(x)| = \\left| \\frac{f^{(3)}(c)}{3!}x^3 \\right| \\leq \\frac{f^{(3)}(0)}{3!}x^3 = \\frac{3x^3}{3!2^8} = \\frac{x^3}{512} Then we have T_2(0.2) = 2.049375 \\\\ |R_2(0.2)| = \\frac{0.2^3}{512} = 0.00001562 \\\\ \\sqrt[]{4.2} = 2.04939015 \\\\ Finally, we have 2.04935938 = T_2(0.2) - |R_2(0.2)| < \\sqrt[]{4.2} < T_2(0.2) + |R_2(0.2)| = 2.04939062 \\square","title":"1.3.5."},{"location":"ch01ex/#136","text":"(a) Another proof of Taylor\u2019s Theorem uses the fundamental theorem of integral calculus once and then integrates by parts repeatedly. Begin with the hypotheses of Theorem 1.3.3, and let x \u2208 I . By the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(t) dt Let u = f'(t) and v = t - x , so that the integral is \\int_{a}^{x} u v' , and integrating by parts gives \\int_{a}^{x} u v' = uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v = f'(x)(x - x) - f'(a) (a - x) - \\int_{a}^{x} f''(t)(t-x) \\\\ = f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt So f(x) = f(a) + f'(a) (x - a) - \\int_{a}^{x} f''(t)(t-x) dt Now let u = f''(t) and v = \\frac{1}{2} (t - x)^2 , \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f''(x) \\frac{1}{2} (x-x)^2 - f''(a) \\frac{1}{2}(a - x)^2 - \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt \\end{split} So f(x) = f(a) + f'(a) (x - a) + f''(a) \\frac{(x-a)^2}{2} + \\int_{a}^{x} f'''(t) \\frac{1}{2} (t-x)^2 dt Then we can use induction and assume f(x) = T_n(x) + (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt Let u = f^{(n+1)}(t), v = \\frac{(t-x)^{(n+1)}}{(n+1)!} \\begin{split} \\int_{a}^{x} u v' &= uv \\bigg|_{a}^{x} - \\int_{a}^{x} u'v \\\\ &= f^{(n+1)}(x) \\frac{(x-x)^{n+1}}{{(n+1)}!} - f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} - \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\\\ &= (-1) \\cdot \\left( f^{(n+1)}(a) \\frac{(a-x)^{n+1}}{{(n+1)}!} + \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} \\right) \\end{split} No mattern n is even or odd, (-1)^{n} (a-x)^n = (x-a)^n So f(x) = T_{n+1} + (-1)^{n+1} \\int_{a}^{x} f^{(n+2)}(t) \\frac{(t-x)^{(n+1)}}{(n+1)!} dt Whereas the expression for f(x)\u2212T_n(x) in Theorem 1.3.3 is called the Lagrange form of the remainder, this exercise has derived the integral form of the remainder. Use the extreme value theorem and the intermediate value theorem to derive the Lagrange form of the remainder from the integral form. Proof : We want to estimate A = (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} Assume m \\leq f^{(n+1)}(t) \\leq M , and let B = (-1)^n \\int_{a}^{x} \\frac{(t-x)^n}{n!} dt = (-1)^n \\frac{(t-x)^{(n+1)}}{{(n+1)}!}\\Bigg|_{a}^{x} = (-1)^{n+1} \\frac{(a-x)^{(n+1)}}{{(n+1)}!} = \\frac{(x-a)^{(n+1)}}{{(n+1)}!} Note (-1)^n \\frac{(t-x)^n}{n!} \\geq 0, t \\in [a, x] , so m \\frac{(x-a)^{(n+1)}}{{(n+1)}!} \\leq (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} \\leq M \\frac{(x-a)^{(n+1)}}{{(n+1)}!} i.e. mB \\leq A \\leq MB Since f^{(n+1)} is continuous, with extreme value theorem, we can find f^{(n+1)}(t_m) = m, f^{(n+1)}(t_M) = M . Then with intermediate value theorem, we can find c , such that f^{(n+1)}(c) = A/B , i.e. (-1)^n \\int_{a}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1} \\square (b) Use the integral form of the remainder to show that \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] Proof : f^{(n+1)}(t) = \\frac{(-1)^n n!}{(1+t)^{n+1}} So \\begin{split} (-1)^n \\int_{0}^{x} f^{(n+1)}(t) \\frac{(t-x)^n}{n!} dt &= (-1)^{2n} \\int_{0}^{x} \\frac{(-1)^n n!}{(1+t)^{n+1}} \\frac{(t-x)^n}{n!} dt \\\\ &= \\int_{0}^{x} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\end{split} Consider, x \\in (-1, -1/2] , and t \\geq x . t \\geq x > (2+t)x \\\\ \\Rightarrow \\\\ t > (2+t)x \\\\ \\Rightarrow \\\\ t - x > x + tx \\\\ \\Rightarrow \\\\ \\frac{t-x}{1+t} > x \\\\ So \\left| \\frac{t-x}{1+t} \\right| < |x| Then \\begin{split} \\left| \\int_{x}^{0} \\frac{(t-x)^n}{(1+t)^{n+1}} dt \\right| & \\leq \\int_{x}^{0} \\left| \\frac{(t-x)^n}{(1+t)^{n+1}} \\right| dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} \\left| \\frac{(t-x)}{(1+t)} \\right|^n dt\\\\ & \\leq \\frac{1}{1+x} \\int_{x}^{0} |x|^n dt\\\\ &= \\frac{|x|^{n+1}}{1+x} \\to 0 \\end{split} The book has proved the case for (-1/2, 1] . Then we can conclude \\ln (1+x) = T(x) \\text{ for } x \\in (-1, 1] \\square","title":"1.3.6."},{"location":"ch01notes/","text":"Chapter 1 Results from One-Variable Calculus 1.1 The Real Number System Theorem 1.1.1 (Field axioms for ( R,+,\u00b7 )). (a1) Addition is associative (a2) 0 is an additive identity (a3) Existence of additive inverses (a4) Addition is commutative (m1) Multiplication is associative (m2) 1 is a multiplicative identity (m3) Existence of multiplicative inverses (m4) Multiplication is commutative (d1) Multiplication distributes over addition Theorem 1.1.2 (Order axioms). (o1) Trichotomy axiom: for every real number x , exactly one of the following conditions holds: x \\in \\mathbb{R}^+, x \\in \\mathbb{R}^-, x = 0. \\square (o2) Closure of positive numbers under addition: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also x+y \\in \\mathbb{R}^+ . \\square (o3) Closure of positive numbers under multiplication: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also xy \\in \\mathbb{R}^+ . \\square For all real numbers x and y , define x < y to mean y - x \\in \\mathbb{R}^+ We want to show this definition is the same as what we learned in \"Understanding Analysis\". \\Rightarrow We define this relation, if x < y of x = y then we say x \\leq y . (o1ua) Given any x, y , since either x - y \\in \\mathbb{R}^+, x - y \\in \\mathbb{R}^-, x - y = 0. Then either x \\leq y or y \\leq x . (o2ua) If x \\leq y , then x - y \\leq 0 . y \\leq x , then y - x \\leq 0 . So x = y . (o3ua) If x \\leq y , then x - y \\leq 0 . y - z \\leq 0 , then (x-y) + (y-z) = x - z \\leq 0 , i.e. x \\leq z . (o4ua) If y \\leq z , y - z \\leq 0 , then x + (y-z) \\leq x . so x + y \\leq x + z . (o5ua) It's automatic. \\Leftarrow We define the set to be \\mathbb{F}^+ = \\{ x : 0 \\leq x \\text{ and } x \\neq 0 \\} We want to showthe following: (o1) We want to show if x \\not\\in \\mathbb{F}^+ and x \\neq 0 , then -x \\in \\mathbb{F}^+ . Assume -x \\leq 0 , then -x + x \\leq 0 + x , so 0 \\leq x , we have a contradiction. So 0 \\leq -x . Since x \\neq 0 , then if -x = 0 , we have 0 = x + (-x) = x , we have a contradiction again, so -x \\neq 0 . Then x \\in \\mathbb{F}^+ . (o2) We want to show if x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also x+y \\in \\mathbb{F}^+ . 0 \\leq x, 0 \\leq y , then 0 + 0 \\leq x + 0 \\leq x + y . In addition, if x + y = 0 , then x = -y , since y \\in \\mathbb{F}^+ , then -y \\not\\in \\mathbb{F}^+ , i.e. x \\not\\in \\mathbb{F}^+ , we have a contradiction. (o3) We want to show x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also xy \\in \\mathbb{F}^+ . (o5ua) guarantees 0 \\leq x, 0 \\leq y then 0 \\leq xy . If xy = 0 , since y \\neq 0 , we can find y^{-1} such that x = xyy^{-1} = 0y^{-1} = 0 , we have a contradiction. \\square Another thing we can show is that no order can be made to a finite field. Proof : Assume we \\mathbb{F} is a finite field. We proved 1 \\in \\mathbb{F}^+ . Then consider the sequence 1,2,3,\\cdots Since \\mathbb{F} is finite, we must have for some k,n k = k+n , then we have n = 0 . On the other hand, n is the sum of n 1 s, so n \\in \\mathbb{F}^+ . We have a contradiction. \\square 1.3 Taylor\u2019s Theorem Let I \u2282 \\mathbb{R} be a nonempty open interval, and let a \u2208 I be any point. Let n be a nonnegative integer. Suppose that the function f : I \\to \\mathbb{R} has n continuous derivatives f, f', f'', \\cdots, f^{(n)} : I \\to \\mathbb{R} For every positive integer k and every x \u2208 \\mathbb{R} define a k-fold nested integral, I_k(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_k = a}^{x_{k-1}} d x_k \\cdots d x_2 d x_1 Then we have I_1(x) = \\int_{x_1 = a}^{x} d x_1 = x_1 \\Big|_{x_1 = a}^x = x - a I_2(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} d x_2 d x_1 = \\int_{x_1 = a}^{x} I_1(x_1) d x_1 = \\frac{1}{2} (x_1 - a)^2 \\Big|_{x_1 = a}^x = \\frac{1}{2} (x - a)^2 So in general I_k(x) = \\frac{1}{k!} (x - a)^k, k \\in \\mathbb{Z}^+. According to the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ f'(x_1) = f'(a) + \\int_{a}^{x_1} f''(x_2) d x_2 \\\\ So we have \\begin{split} f(x) &= T_0(x) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ &= T_0(x) + f'(a) I_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &= T_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &\\cdots \\\\ &= T_n(x) + \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\cdots d x_2 d x_1 \\\\ &= T_n(x) + R_n(x) \\end{split} Assume m \\leq f^{(n+1)}(x_{n+1}) \\leq M , then m I_1(x_n) \\leq \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\leq M I_1(x_n) and m I_2(x_{n-1}) \\leq \\int_{x_{n} = a}^{x_{n-1}} \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} d x_n \\leq M I_2(x_{n-1}) and so on. Eventually, we have m I_{n+1}(x) \\leq R_n(x) \\leq M I_{n+1}(x) i.e. m \\frac{(x-a)^{n+1}}{(n+1)!} \\leq R_n(x) \\leq M \\frac{(x-a)^{n+1}}{(n+1)!} If we let g(t) = f^{(n+1)}(t) \\frac{(x-a)^{n+1}}{(n+1)!}, t \\in [a, x] Then we have g(t_m) = m \\frac{(x-a)^{n+1}}{(n+1)!}, g(t_M) = M \\frac{(x-a)^{n+1}}{(n+1)!} and since g(t) is continuous, we can find c \\in [a, x] , such that g(c) = R_n(x) . \\square Next, we discuss the case for x < a . Consider \\tilde{f} : -I \\to \\mathbb{R} , \\tilde{f}(-x) = f(x) With chain rule, we can see \\tilde{f}^{(n)}(-x) = (-1)^n f^{(n)}(x) If x < a in I then \u2212x >\u2212a in \u2212I , and so we know by the version of Taylor\u2019s theorem that we have already proved that \\tilde{f}(-x) = \\tilde{T}_n(-x) + \\tilde{R}_n(-x) To get the \\tilde{T}_n(-x) , \\tilde{T}_n(-x) = \\sum_{k = 0}^{n} \\frac{\\tilde{f}^{(k)}(-a)}{k!} (-x - (-a))^k \\\\ = \\sum_{k = 0}^{n} (-1)^k \\frac{f^{(k)}(a)}{k!} (a-x)^k \\\\ = \\sum_{k = 0}^{n} \\frac{f^{(k)}(a)}{k!} (x-a)^k \\\\ and also \\tilde{R}_n(-x) = \\frac{\\tilde{f}^{(n+1)}(-c)}{(n+1)!} (-x - (-a))^{n+1} = (-1)^{n+1} \\frac{f^{(n+1)}(c)}{(n+1)!} (a-x)^{n+1} \\\\ = \\frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} So we proved when x < a . \\square Whereas our proof of Taylor\u2019s theorem relies primarily on the fundamental theorem of integral calculus a similar proof relies on repeated integration byparts(Exercise1.3.6) (Also see Jay Cumming's Real Analysis) many proofs rely instead on the mean value theorem. Also see Understanding Analysis. Our proof neatly uses three di\ufb00erent mathematical techniques for the three di\ufb00erent parts of the argument: To find the Taylor polynomial T_n(x) , we di\ufb00erentiated repeatedly, using a substitution at each step to determine a coe\ufb03cient. To get a precise (if unwieldy) expression for the remainder R_n(x) = f(x)\u2212 T_n(x) , we integrated repeatedly, usingthefundamental theorem of integral calculus at each step to produce a term of the Taylor polynomial. To express the remainder in a more convenient form, we used the extreme value theorem and then the intermediate value theorem once each. These foundational theorems are not results from calculus but (as we will discuss in Section 2.4) from an area of mathematics called topology.","title":"Chapter 01 Notes"},{"location":"ch01notes/#chapter-1-results-from-one-variable-calculus","text":"","title":"Chapter 1 Results from One-Variable Calculus"},{"location":"ch01notes/#11-the-real-number-system","text":"","title":"1.1 The Real Number System"},{"location":"ch01notes/#theorem-111-field-axioms-for-r","text":"(a1) Addition is associative (a2) 0 is an additive identity (a3) Existence of additive inverses (a4) Addition is commutative (m1) Multiplication is associative (m2) 1 is a multiplicative identity (m3) Existence of multiplicative inverses (m4) Multiplication is commutative (d1) Multiplication distributes over addition","title":"Theorem 1.1.1 (Field axioms for (R,+,\u00b7))."},{"location":"ch01notes/#theorem-112-order-axioms","text":"(o1) Trichotomy axiom: for every real number x , exactly one of the following conditions holds: x \\in \\mathbb{R}^+, x \\in \\mathbb{R}^-, x = 0. \\square (o2) Closure of positive numbers under addition: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also x+y \\in \\mathbb{R}^+ . \\square (o3) Closure of positive numbers under multiplication: for all real numbers x and y , if x \\in \\mathbb{R}^+ , and y \\in \\mathbb{R}^+ , then also xy \\in \\mathbb{R}^+ . \\square For all real numbers x and y , define x < y to mean y - x \\in \\mathbb{R}^+ We want to show this definition is the same as what we learned in \"Understanding Analysis\". \\Rightarrow We define this relation, if x < y of x = y then we say x \\leq y . (o1ua) Given any x, y , since either x - y \\in \\mathbb{R}^+, x - y \\in \\mathbb{R}^-, x - y = 0. Then either x \\leq y or y \\leq x . (o2ua) If x \\leq y , then x - y \\leq 0 . y \\leq x , then y - x \\leq 0 . So x = y . (o3ua) If x \\leq y , then x - y \\leq 0 . y - z \\leq 0 , then (x-y) + (y-z) = x - z \\leq 0 , i.e. x \\leq z . (o4ua) If y \\leq z , y - z \\leq 0 , then x + (y-z) \\leq x . so x + y \\leq x + z . (o5ua) It's automatic. \\Leftarrow We define the set to be \\mathbb{F}^+ = \\{ x : 0 \\leq x \\text{ and } x \\neq 0 \\} We want to showthe following: (o1) We want to show if x \\not\\in \\mathbb{F}^+ and x \\neq 0 , then -x \\in \\mathbb{F}^+ . Assume -x \\leq 0 , then -x + x \\leq 0 + x , so 0 \\leq x , we have a contradiction. So 0 \\leq -x . Since x \\neq 0 , then if -x = 0 , we have 0 = x + (-x) = x , we have a contradiction again, so -x \\neq 0 . Then x \\in \\mathbb{F}^+ . (o2) We want to show if x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also x+y \\in \\mathbb{F}^+ . 0 \\leq x, 0 \\leq y , then 0 + 0 \\leq x + 0 \\leq x + y . In addition, if x + y = 0 , then x = -y , since y \\in \\mathbb{F}^+ , then -y \\not\\in \\mathbb{F}^+ , i.e. x \\not\\in \\mathbb{F}^+ , we have a contradiction. (o3) We want to show x \\in \\mathbb{F}^+ , and y \\in \\mathbb{F}^+ , then also xy \\in \\mathbb{F}^+ . (o5ua) guarantees 0 \\leq x, 0 \\leq y then 0 \\leq xy . If xy = 0 , since y \\neq 0 , we can find y^{-1} such that x = xyy^{-1} = 0y^{-1} = 0 , we have a contradiction. \\square Another thing we can show is that no order can be made to a finite field. Proof : Assume we \\mathbb{F} is a finite field. We proved 1 \\in \\mathbb{F}^+ . Then consider the sequence 1,2,3,\\cdots Since \\mathbb{F} is finite, we must have for some k,n k = k+n , then we have n = 0 . On the other hand, n is the sum of n 1 s, so n \\in \\mathbb{F}^+ . We have a contradiction. \\square","title":"Theorem 1.1.2 (Order axioms)."},{"location":"ch01notes/#13-taylors-theorem","text":"Let I \u2282 \\mathbb{R} be a nonempty open interval, and let a \u2208 I be any point. Let n be a nonnegative integer. Suppose that the function f : I \\to \\mathbb{R} has n continuous derivatives f, f', f'', \\cdots, f^{(n)} : I \\to \\mathbb{R} For every positive integer k and every x \u2208 \\mathbb{R} define a k-fold nested integral, I_k(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_k = a}^{x_{k-1}} d x_k \\cdots d x_2 d x_1 Then we have I_1(x) = \\int_{x_1 = a}^{x} d x_1 = x_1 \\Big|_{x_1 = a}^x = x - a I_2(x) = \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} d x_2 d x_1 = \\int_{x_1 = a}^{x} I_1(x_1) d x_1 = \\frac{1}{2} (x_1 - a)^2 \\Big|_{x_1 = a}^x = \\frac{1}{2} (x - a)^2 So in general I_k(x) = \\frac{1}{k!} (x - a)^k, k \\in \\mathbb{Z}^+. According to the fundamental theorem, f(x) = f(a) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ f'(x_1) = f'(a) + \\int_{a}^{x_1} f''(x_2) d x_2 \\\\ So we have \\begin{split} f(x) &= T_0(x) + \\int_{a}^{x} f'(x_1) d x_1 \\\\ &= T_0(x) + f'(a) I_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &= T_1(x) + \\int_{a}^{x} \\int_{a}^{x_1} f''(x_2) d x_2 d x_1 \\\\ &\\cdots \\\\ &= T_n(x) + \\int_{x_1 = a}^{x} \\int_{x_2 = a}^{x_1} \\cdots \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\cdots d x_2 d x_1 \\\\ &= T_n(x) + R_n(x) \\end{split} Assume m \\leq f^{(n+1)}(x_{n+1}) \\leq M , then m I_1(x_n) \\leq \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} \\leq M I_1(x_n) and m I_2(x_{n-1}) \\leq \\int_{x_{n} = a}^{x_{n-1}} \\int_{x_{n+1} = a}^{x_{n}} f^{(n+1)}(x_{n+1}) d x_{n+1} d x_n \\leq M I_2(x_{n-1}) and so on. Eventually, we have m I_{n+1}(x) \\leq R_n(x) \\leq M I_{n+1}(x) i.e. m \\frac{(x-a)^{n+1}}{(n+1)!} \\leq R_n(x) \\leq M \\frac{(x-a)^{n+1}}{(n+1)!} If we let g(t) = f^{(n+1)}(t) \\frac{(x-a)^{n+1}}{(n+1)!}, t \\in [a, x] Then we have g(t_m) = m \\frac{(x-a)^{n+1}}{(n+1)!}, g(t_M) = M \\frac{(x-a)^{n+1}}{(n+1)!} and since g(t) is continuous, we can find c \\in [a, x] , such that g(c) = R_n(x) . \\square Next, we discuss the case for x < a . Consider \\tilde{f} : -I \\to \\mathbb{R} , \\tilde{f}(-x) = f(x) With chain rule, we can see \\tilde{f}^{(n)}(-x) = (-1)^n f^{(n)}(x) If x < a in I then \u2212x >\u2212a in \u2212I , and so we know by the version of Taylor\u2019s theorem that we have already proved that \\tilde{f}(-x) = \\tilde{T}_n(-x) + \\tilde{R}_n(-x) To get the \\tilde{T}_n(-x) , \\tilde{T}_n(-x) = \\sum_{k = 0}^{n} \\frac{\\tilde{f}^{(k)}(-a)}{k!} (-x - (-a))^k \\\\ = \\sum_{k = 0}^{n} (-1)^k \\frac{f^{(k)}(a)}{k!} (a-x)^k \\\\ = \\sum_{k = 0}^{n} \\frac{f^{(k)}(a)}{k!} (x-a)^k \\\\ and also \\tilde{R}_n(-x) = \\frac{\\tilde{f}^{(n+1)}(-c)}{(n+1)!} (-x - (-a))^{n+1} = (-1)^{n+1} \\frac{f^{(n+1)}(c)}{(n+1)!} (a-x)^{n+1} \\\\ = \\frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} So we proved when x < a . \\square Whereas our proof of Taylor\u2019s theorem relies primarily on the fundamental theorem of integral calculus a similar proof relies on repeated integration byparts(Exercise1.3.6) (Also see Jay Cumming's Real Analysis) many proofs rely instead on the mean value theorem. Also see Understanding Analysis. Our proof neatly uses three di\ufb00erent mathematical techniques for the three di\ufb00erent parts of the argument: To find the Taylor polynomial T_n(x) , we di\ufb00erentiated repeatedly, using a substitution at each step to determine a coe\ufb03cient. To get a precise (if unwieldy) expression for the remainder R_n(x) = f(x)\u2212 T_n(x) , we integrated repeatedly, usingthefundamental theorem of integral calculus at each step to produce a term of the Taylor polynomial. To express the remainder in a more convenient form, we used the extreme value theorem and then the intermediate value theorem once each. These foundational theorems are not results from calculus but (as we will discuss in Section 2.4) from an area of mathematics called topology.","title":"1.3 Taylor\u2019s Theorem"},{"location":"ch02ex/","text":"Chapter 2 Euclidean Space 2.1 Algebra: Vectors 2.1.1. Write down any three specific nonzero vectors u , v , w from \\mathbb{R}^3 and any two specific nonzero scalars a, b from \\mathbb{R} . Compute u+v, aw, b(v+w), (a+b)u , u+v +w, abw , and the additive inverse to u . Solution : Let u = (1,0,0) \\\\ v = (0,1,0) \\\\ w = (0,0,1) \\\\ a = 1, b = -1 \\\\ u + v = (1, 1, 0) \\\\ aw = w = (0,0,1) \\\\ b(v + w) = (0,-1,-1) \\\\ (a+b)u = 0 \\\\ u+v+w = (1,1,1) \\\\ ab(w) = (0,0,-1) \\\\ 2.1.2. Working in \\mathbb{R}^2 , give a geometric proof that if we view the vectors x and y as arrows from 0 and form the parallelogram P with these arrows as two of its sides, then the diagonal z starting at 0 is the vector sum x + y viewed as an arrow. Proof : See the figure below. 2.1.3. Verify that \\mathbb{R}^n satisfies vector space axioms (A2), (A3), (D1). Proof : (A2) 0 is an additive identity 0 + x = (0, \\cdots, 0) + (x_1, \\cdots, x_n) \\\\ = (0 + x_1, \\cdots, 0 + x_n) \\\\ = (x_1, \\cdots, x_n) \\\\ = x \\square (A3) Existence of additive inverses Let x = (x_1, \\cdots, x_n) and y = (-x_1, \\cdots, -x_n) y + x = (-x_1 + x_1, \\cdots, -x_n + x_n) \\\\ = (0, \\cdots, 0) So y is the additive inverse of x . \\square (D1) Scalar multiplication distributes over scalar addition (a+b)x = (a+b) (x_1, \\cdots, x_n) \\\\ = ((a+b)x_1, \\cdots, (a+b)x_n) \\\\ = (ax_1 + bx_1, \\cdots, ax_n + bx_n) \\\\ = (ax_1, \\cdots, ax_n) + (bx_1, \\cdots, bx_n) \\\\ = ax + bx \\square 2.1.4. Are all the field axioms used in verifying that Euclidean space satisfies the vector space axioms? Solution : Let's check the vector space axioms one by one. (A1) Addition is associative uses (Fa1) (A2) 0 is an additive identity uses (Fa2) (A3) Existence of additive inverses (Fa3) (A4) Addition is commutative (Fa4) (M1) Scalar multiplication is associative (Fm1) (M2) 1 is a multiplicative identity (Fm2) (D1) Scalar multiplication distributes over scalar addition (Fd1) (D2) Scalar multiplication distributes over vector addition (Fd1) So we can see (Fm3) and (Fm4) are not used. \\square 2.1.5. Show that 0 is the unique additive identity in \\mathbb{R}^n . Show that each vector x \u2208 \\mathbb{R}^n has a unique additive inverse, which can therefore be denoted \u2212x . (And it follows that vector subtraction can now be defined, - : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^n, \\quad x - y = x + (-y) for all x, y \\in \\mathbb{R}^n . Show that 0x = 0 for all x \u2208 \\mathbb{R}^n . Proof : Assume x = (x_1, \\cdots, x_n) is a additive identity and y = (y_1, \\cdots, y_n) \\in \\mathbb{R}^n . Then x + y = y , i.e. x_1 + y_1 = y_1, \\cdots , x_n + y_n = y_n So x_1 = \\cdots = x_n = 0 , so x = 0 . And thus 0 is unique. Assume x = (x_1, \\cdots, x_n) and y (y_1, \\cdots, y_n) \\in \\mathbb{R}^n is x 's additive inverse. x + y = (x_1 + y_1, \\cdots, x_n + y_n) = 0 So y = (-x_1, \\cdots, -x_n) The additive inverse is unique. 0x = 0 (x_1, \\cdots, x_n) = (0x_1, \\cdots, 0x_n) = (0, \\cdots, 0) \\square 2.1.6. Repeat the previous exercise, but with \\mathbb{R}^n replaced by an arbitrary vector space V over a field F . (Work with the axioms.) 0 = 0' + 0 = 0 + 0' = 0' So 0 is unique. y = y + 0 = y + (x + y') = (y + x) + y' = 0 + y' = y' So the additive inverse is unique. 0x + 1x = (0 + 1) x = 1 x = x Then we can add -x on both side 0x + x + (-x) = 0x \\\\ \\Rightarrow \\\\ x + (-x) = 0 \\\\ \\Rightarrow \\\\ 0x = 0 \\square 2.1.7. Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.) Proof : I cannot figure it for now. The uniqueness of the additive identity. Assume 0 and 0' are 2 additive identities. Then 0' + 0 = 0 \\\\ 0 + 0' = 0' \\\\ 0' + 0' = 0' \\\\ 2.1.8. Let x and y be noncollinear vectors in \\mathbb{R}^3 . Give a geometric description of the set of all linear combinations of x and y . Solution : All the linear combinations of x and y form a plane that includes 0, x, y . \\square 2.1.9 Which of the following sets are bases of \\mathbb{R}^3 ? S_1 = \\{(1,0,0),(1,1,0),(1,1,1)\\} \\\\ S_2 = \\{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\\} \\\\ S_3 = \\{(1,1,0),(0,1,1)\\} \\\\ S_4 = \\{(1,1,0),(0,1,1),(1,0,-1)\\} \\\\ How many elements do you think a basis for \\mathbb{R}^n must have? Give (without proof) geometric descriptions of all bases of \\mathbb{R}^2 , of \\mathbb{R}^3 . Solution : For S_1 , given (x,y,z) , we have \\begin{cases} x &= a + b + c\\\\ y &= b + c \\\\ z &= c \\\\ \\end{cases} Then we can have \\begin{cases} a = x - y\\\\ b = y - z\\\\ c = z\\\\ \\end{cases} So S_1 is a base. For S_2 , it is note a base. For example (1,1,1) can be expressed by multiple ways. For S_3 \\begin{cases} x &= a \\\\ y &= a + b \\\\ z &= b \\\\ \\end{cases} Then we cannot represent (1,3,1) with S_3 . For S_4 , We have \\begin{cases} x &= a + c\\\\ y &= a + b \\\\ z &= b - c\\\\ \\end{cases} Then we have to have z = y - x , so we cannot represent (1,1,1) with S_4 . For \\mathbb{R}^n , it needs n element for a basis. For \\mathbb{R}^2 , if \\{f_1, f_2\\} is a basis, then geometrically, they should not be in one line. For \\mathbb{R}^3 , if \\{f_1, f_2, f_3\\} is a basis, then geometrically, they should not be in the same plane. \\square 2.1.10. Recall the field \\mathbb{C} of complex numbers. Define complex n -space \\mathbb{C}^n analogously to \\mathbb{R}^n : \\mathbb{C}^n = \\{ (z_1, \\cdots, z_n) : z_i \\in \\mathbb{C} \\text{ for } i = 1, \\cdots, n \\} and endow it with addition and scalar multiplication defined by the same formulas as for \\mathbb{R}^n . You may take for granted that under these definitions, \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{R} , and also \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{C} . That is, using language that was introduced briefly in this section, \\mathbb{C}^n can be viewed as a vector space over \\mathbb{R} and also, separately, as a vector space over \\mathbb{C} . Give a basis for each of these vector spaces. Solution : For the first one S_1 = \\{ (1,\\cdots,0), (i,\\cdots,0), \\cdots (0,\\cdots,1), (0,\\cdots,i) \\} For the second one S_2 = \\{ (1,\\cdots,0), \\cdots (0,\\cdots,1) \\} \\square 2.2 Geometry: Length and Angle 2.2.1 Let x = (\\frac{\\sqrt[]{3}}{2}, -\\frac{1}{2}, 0), y = (\\frac{1}{2}, \\frac{\\sqrt[]{3}}{2}, 1), z = (1, 1, 1) Compute \u27e8x,x\u27e9, \u27e8x,y\u27e9, \u27e8y,z\u27e9, |x|, |y|, |z|, \u03b8_{x,y}, \u03b8_{y,e_1}, \u03b8_{z,e_2} Solution : \u27e8x,x\u27e9 = \\frac{3}{4} + \\frac{1}{4} + 0 = 1 \\\\ \u27e8x,y\u27e9 = \\frac{\\sqrt[]{3}}{4} - \\frac{\\sqrt[]{3}}{4} + 0 = 0 \\\\ \u27e8y,z\u27e9 = \\frac{1}{2} + \\frac{\\sqrt[]{3}}{2} + 1 = \\frac{3 + \\sqrt[]{3}}{2} \\\\ |x| = 1 \\\\ |y| = \\sqrt[]{2} \\\\ |z| = \\sqrt[]{3} \\\\ \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x| |y|} = \\frac{0}{|x| |y|} = 0 \\\\ \u03b8_{y,e_1} = \\frac{\u27e8y,e_1\u27e9}{|y| |e_1|} = \\frac{1/2}{|x| |y|} = \\frac{1}{2 \\sqrt[]{2}} \\\\ \u03b8_{z,e_2} = \\frac{1}{\\sqrt[]{3}} \\square 2.2.2. Show that the points x = (2,\u22121,3,1), y = (4,2,1,4), z = (1,3,6,1) form the vertices of a triangle in \\mathbb{R}^4 with two equal angles. Solution : d(x,y) = |x - y| = \\sqrt[]{\u27e8x-y,x-y\u27e9} = \\sqrt[]{\u27e8(-2,-3,2,-3),(-2,-3,2,-3)\u27e9} = \\sqrt[]{4+9+4+9} = \\sqrt[]{26} \\\\ d(y,z) = |y-z| = \\sqrt[]{\u27e8y-z,y-z\u27e9} = \\sqrt[]{\u27e8(3,-1,-5,3),(3,-1,-5,3)\u27e9} = \\sqrt[]{9+1+25+9} = \\sqrt[]{44} \\\\ d(z,x) = |z-x| = \\sqrt[]{\u27e8z-x,z-x\u27e9} = \\sqrt[]{\u27e8(-1,4,3,0),(-1,4,3,0)\u27e9} = \\sqrt[]{1+16+9+0} = \\sqrt[]{26} \\\\ \\square 2.2.3. Explain why for all x \u2208 \\mathbb{R}^n , x = \\sum_{j=1}^{n} \u27e8x,e_j\u27e9e_j . Proof : Note \u27e8x,e_j\u27e9 = \u27e8(x_1, \\cdots, x_n),(0, \\cdots, 1, \\cdots, 0)\u27e9 = x_j \\\\ x_j e_j = (0, \\cdots, x_j, \\cdots, 0) \\\\ \\sum_{j=1}^{n} x_j e_j = (x_1, \\cdots, x_n) = x \\square 2.2.4. Prove the inner product properties. Proof : (IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 = x_1^2 + \\cdots + x_n^2 \\geq 0 It equals 0 only when x_1 = \\cdots = x_n = 0 . \\square (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n \u27e8x,y\u27e9 = x_1 y_1 + \\cdots + x_n y_n = y_1 x_1 + \\cdots + y_n x_n = \u27e8y,x\u27e9 \\square (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = (x_1 + x'_1) y_1 + \\cdots + (x_n + x'_n) y_n \\\\ = (x_1 y_1 + \\cdots + x_n y_n) + (x'_1 y_1 + \\cdots + x'_n y_n) \\\\ = \u27e8x,y\u27e9 + \u27e8x',y\u27e9 \u27e8ax,y\u27e9 = (ax_1) y_1 + \\cdots + (ax_n) y_n \\\\ = a (x_1 y_1 + \\cdots + x_n y_n) \\\\ = a \u27e8x,y\u27e9 \\square 2.2.5. Use the inner product properties and the definition of the modulus in terms of the inner product to prove the modulus properties. Proof : (Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ |x| = \\sqrt[]{\u27e8x,x\u27e9} \\geq 0 \u27e8x,x\u27e9 = 0 if and only if x = 0 . \\square (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n . \u27e8ax,ax\u27e9 = a \u27e8x,ax\u27e9 = a^2 \u27e8x,x\u27e9 So |ax| = \\sqrt[]{\u27e8ax,ax\u27e9} = \\sqrt[]{a^2 \u27e8x,x\u27e9} = |a||x| \\square 2.2.6. In the text, the modulus is defined in terms of the inner product. Prove that this can be turned around by showing that for every x,y \u2208 \\mathbb{R}^n , \u27e8x,y\u27e9 = \\frac{|x+y|^2 - |x-y|^2}{4}. Proof : |x+y|^2 = \u27e8x+y,x+y\u27e9 = \u27e8x,x\u27e9 + 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ |x-y|^2 = \u27e8x-y,x-y\u27e9 = \u27e8x,x\u27e9 - 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ So the equation holds. \\square 2.2.7. Prove the full triangle inequality: for every x,y \u2208 \\mathbb{R}^n , ||x| - |y|| \\leq |x \\pm y | \\leq |x| + |y| Do not do this by writing three more variants of the proof of the triangle inequality, but by substituting suitably into the basic triangle inequality, which is already proved. Proof : |x - y| \\leq |x| + |y| Note \\begin{split} |x-y| & = |x + (-y)| \\\\ & \\leq |x| + |-y| \\\\ & = |x| + |y| \\\\ \\end{split} 2. ||x| - |y|| \\leq |x + y| Note \\begin{split} |x| &= |(-y) + (x+y)| \\\\ & \\leq |-y| + |x+y| \\\\ & = |y| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |x| - |y| \\leq |x+y| And similarly \\begin{split} |y| &= |(-x) + (x+y)| \\\\ & \\leq |-x| + |x+y| \\\\ & = |x| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |y| - |x| \\leq |x+y| Combine these 2 together, we have ||x| - |y|| \\leq |x + y| 3. ||x| - |y|| \\leq |x - y| This is similar to 2. \\square 2.2.8. Let x = (x_1,...,x_n) \u2208 \\mathbb{R}^n . Prove the size bounds: for every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{j=1}^{n} |x_j|. (One approach is to start by noting that x_j= \u27e8x,e_j\u27e9 and recalling equation (2.1).) When can each \u2264 be an = ? Proof : From the hint: \\begin{split} |x_j| &= |\u27e8x,e_j\u27e9| \\\\ & \\leq |x||e_j| \\\\ & \\text{Cauchy-Schwarz inequality} \\\\ & = |x| \\cdot 1 \\\\ & = |x| \\end{split} This equality holds when x = a \\cdot e_j . Note \\begin{split} |x| &= \\left| \\sum_{j=1}^{n} x_j e_j \\right| \\\\ & \\leq \\sum_{j=1}^{n} \\left| x_j e_j \\right| \\\\ & \\text{Generalized triangle inequality}\\\\ & = \\sum_{j=1}^{n} | x_j | | e_j | \\\\ & \\text{The modulus is absolute-homogeneous}\\\\ & = \\sum_{j=1}^{n} | x_j | \\\\ \\end{split} This equality holds when only only one of x_j is not 0 . \\square 2.2.9. Prove the distance properties. Proof : (D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . Note d(x,y) = |x-y| \\geq 0 The equality holds only when x-y = 0 , i.e. x = y . \\square (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . Note: d(x,y) = |x-y| = |y-x| = d(y,x) \\square \\square (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n . Note: \\begin{split} d(x,z) &= |x-z| \\\\ &= |(x-y)+(y-z)| \\\\ & \\leq |x-y| + |y-z| \\\\ &= d(x,y) + d(y,z) \\end{split} \\square 2.2.10 Working in \\mathbb{R}^2 , depict the nonzero vectors x and y as arrows from the origin and depict x\u2212y as an arrow from the endpoint of y to the endpoint of x . Let \u03b8 denote the angle (in the usual geometric sense) between x and y . Use the law of cosines to show that \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} Proof Geometric proof: \\frac{\u27e8x,y\u27e9}{|x||y|} = \\frac{x_1 y_1 + x_2 y_2}{|x||y|} = \\frac{1}{|x|} \\left( \\frac{x_1 y_1 + x_2 y_2}{|y|} \\right) As shown in the figure below, XD \\perp OY , AC \\perp OY and XB \\perp AC . We just need to show \\frac{x_1 y_1 + x_2 y_2}{|y|} = OD Note \\frac{y_1}{y} = \\cos \\angle AOC \\\\ \\Rightarrow \\\\ \\frac{x_1 y_1}{|y|} = OC \\frac{y_2}{y} = \\cos \\angle AXB \\\\ \\Rightarrow \\\\ \\frac{x_2 y_2}{|y|} = CD Then we finished the proof. Proof with triangle equalities: x_1 = |x| \\cos \u03b8_x, x_2 = |x| \\sin \u03b8_x, y_1 = |y| \\cos \u03b8_y, y_2 = |y| \\sin \u03b8_y, Then \u27e8x,y\u27e9 = |x||y| \\cos \u03b8_x \\cos \u03b8_y + |x||y| \\sin \u03b8_x \\sin \u03b8_y \\\\ = |x||y| \\cos (\u03b8_x - \u03b8_y) = |x||y| \\cos \u03b8 This also proved. \\square 2.2.11. Prove that for every nonzero x \u2208 \\mathbb{R}^n , \\sum_{i = 1}^{n} \\cos ^2 \u03b8_{x, e_i} = 1 . Proof : \\cos ^2 \u03b8_{x, e_i} = \\sum_{i = 1}^{n} \\left( \\frac{\u27e8x,e_i\u27e9}{|x||e_i|} \\right)^2 \\\\ \\sum_{i = 1}^{n} \\frac{x_i^2}{|x|^2} \\\\ = \\frac{1}{|x|^2} \\sum_{i = 1}^{n} x_i^2 \\\\ = 1 \\square 2.2.12. Prove that two nonzero vectors x, y are orthogonal if and only if |x+y|^2 = |x|^2 +|y|^2. Proof : \\begin{split} |x+y|^2 = \u27e8x+y,x+y\u27e9 = |x|^2 + 2 \u27e8x,y\u27e9 + |y|^2 \\end{split} Then |x+y|^2 = |x|^2 +|y|^2 means \u27e8x,y\u27e9=0 . \\square 2.2.13. Use vectors in \\mathbb{R}^2 to show that the diagonals of a parallelogram are perpendicular if and only if the parallelogram is a rhombus. Proof : The 2 diagonals of the parallelogram are x+y, x-y . \u27e8x+y,x-y\u27e9 = |x|^2 - |y|^2 If x+y \\perp x-y , then |x|^2 - |y|^2=0 , i.e. |x| = |y| . On the other hand, if |x| = |y| , then \u27e8x+y,x-y\u27e9 . \\square 2.2.14. Use vectors to show that every angle inscribed in a semicircle is right. Let (x,y) on the semicircle, then x^2+y^2=1 . consider (x,y) - (-1,0) = (x+1,y) and (x,y) - (1,0) = (x-1,y) . Then \u27e8(x+1,y), (x-1, y)\u27e9 = x^2 - 1 + y^2 = 0 \\square 2.2.15. Let x and y be vectors, with x nonzero.Define the parallel component of y along x and the normal component of y to x to be y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x \\qquad \\text{and} \\qquad y_{(\\perp x)} = y - y_{(\\parallel x)} Answer: (a) Show that y = y_{(\\parallel x)} + y_{(\\perp x)} . show that y_{(\\parallel x)} is a scalar multiple of x . show that y_{(\\perp x)} is orthogonal to x . Show that the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. Draw an illustration. Proof : y_{(\\parallel x)} + y_{(\\perp x)} = \\\\ y_{(\\parallel x)} + (y - y_{(\\parallel x)}) = \\\\ y y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x Since \\frac{\u27e8x,y\u27e9}{|x|^2} is a scalar, then y_{(\\parallel x)} is a scalar multiple of x . \u27e8y - y_{(\\parallel x)},y_{(\\parallel x)}) = \\\\ \u27e8|x|^2 y - \u27e8x,y\u27e9x, \u27e8x,y\u27e9x\u27e9 \\\\ = |x|^2 \u27e8x,y\u27e9 \u27e8y,x\u27e9 - \u27e8x,y\u27e9^2 \u27e8x,x\u27e9 \\\\ = 0 So y_{(\\perp x)} is orthogonal to y_{(\\parallel x)} , thus is orthogonal to x . In general if x \\perp y and ax + by = 0, x,y \\neq 0 , then 0 = \u27e80, x\u27e9 = \u27e8ax + by,x\u27e9 = a \u27e8x,x\u27e9 + b \u27e8y,x\u27e9 = a \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ a = 0 For the same reason b = 0 . With this, the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. \\square (b) Show that |y|^2 = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 What theorem from classical geometry does this encompass? Proof : |y|^2 = \\\\ |y_{(\\parallel x)} + y_{(\\perp x)}|^2 = \\\\ |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 + 2 \u27e8y_{(\\parallel x)}, y_{(\\perp x)}\u27e9 \\\\ = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 \\\\ It's Pythagorean theorem or Gougu theorem. \\square (c) Explain why it follows from (b) that |y_{(\\parallel x)}|^2 \\leq |y|^2 with equality if and only if y is a scalar multiple of x . Use this inequality to give another proof of the Cauchy\u2013Schwarz inequality. This argument gives the geometric content of Cauchy\u2013Schwarz: the parallel component of one vector along another is at most as long as the original vector. Proof : This is because |y_{(\\perp x)}|^2 \\geq 0 . The equality holds when |y_{(\\perp x)}|^2 = 0 . In this case y = y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x To prove |\u27e8x,y\u27e9| \\leq |x||y| Note |\u27e8x,y\u27e9| = |\u27e8x,y_{(\\parallel x)}+y_{(\\perp x)}\u27e9| \\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9 + 0|\\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9| Now in general, if a is a scaler, then |\u27e8x,ax\u27e9| = |a \u27e8x,x\u27e9| = |a||x|^2 = |x| |ax| Then |\u27e8x,y_{(\\parallel x)}\u27e9| = |x||y_{(\\parallel x)}|| \\leq |x||y| \\square (d) The proof of the Cauchy\u2013Schwarz inequality in part (c) refers to parts (a) and (b), part (a) refers to orthogonality, orthogonality refers to an angle, and as explained in the text, the fact that angles make sense depends on the Cauchy\u2013Schwarz inequality. And so the proof in part (c) apparently relies on circular logic. Explain why the logic is in fact not circular. Solution : Note that when we define \\cos \u03b8 , it is this: \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} But here, we define y_{(\\parallel x)} as y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2}x If we think geometrically in \\mathbb{R}^2 or \\mathbb{R}^3 , the projection of y on x should be y \\cos \u03b8 . The unit length vector along x is \\frac{x}{|x|} . Also the length of y projection is |y \\cos \u03b8| = \\frac{\u27e8x,y\u27e9}{|x|} So it's reasonable to define y_{(\\parallel x)} this way. Now, since we does not rely on the fact that |\\cos \u03b8| \\leq 1 , it is not circular. \\square 2.2.16. Given nonzero vectors x_1,x_2,...,x_n in \\mathbb{R}^n , the Gram\u2013Schmidt process is to set \\begin{split} x_1' &= x_1 \\\\ x_2' &= x_2 - (x_2)_{(\\parallel x_1')}\\\\ x_3' &= x_3 - (x_3)_{(\\parallel x_2')} - (x_3)_{(\\parallel x_1')} \\\\ \\vdots \\\\ x_n' &= x_n - (x_n)_{(\\parallel x_{n-1}')} - \\cdots - (x_n)_{(\\parallel x_1')} \\\\ \\end{split} Answer: (a)What is the result of applying the Gram\u2013Schmidt process to the vectors x_1 = (1,0,0), x_2 = (1,1,0) , and x_3 = (1,1,1) ? Solution : x_1' = (1,0,0) \\\\ x_2' = (0,1,0) \\\\ x_3' = (0,0,1) \\\\ \\square (b) Returning to the general case, show that x_1',...,x_n' are pairwise orthogonal and that each x_j' has the form x_j'= a_{j,1} x_1 +a_{j,2}x_2 +\u00b7\u00b7\u00b7+a_{j,j\u22121}x_{j\u22121} +x_j. Thus every linear combination of the new {x_j'} is also a linear combination of the original {x_j} . The converse is also true and will be shown in Exercise 3.3.13. Proof : First, we want to prove a small lemma: Given x, y, z , if x \\perp y , then we want to show z_{(\\parallel x)} \\perp y Note \u27e8z_{(\\parallel x)},y\u27e9 = \u27e8\\frac{\u27e8z,x\u27e9}{|x|^2}x,y\u27e9 = \\frac{\u27e8z,x\u27e9}{|x|^2} \u27e8x,y\u27e9 = 0 Then we can use induction. And assume x_1',...,x_{k-1}' are orthogonal to each other. Assume 1 \\leq j \\leq k-1 , \u27e8x_j',x_{k}'\u27e9 = \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 - \\sum_{i \\neq j} \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 = 0 is proved in exercise 2.2.15. Also, since \u27e8x_j',x_i'\u27e9=0 for j \\neq i , then \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 = 0 as just proved in the lemma. So \u27e8x_j',x_{k}'\u27e9 = 0 . For the 2nd part, we can still use induction. And assume for x_1',...,x_{k-1}' , they are all linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Then x_k' = x_k - (x_k)_{(\\parallel x_{k-1}')} - \\cdots - (x_k)_{(\\parallel x_1')} Note (x_k)_{(\\parallel x_j')} is a scaler of x_j', j < k , thus a linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Therefore, x_k' is a linear combination of x_1,...,x_{k} . \\square 2.3 Analysis: Continuous Mappings 2.3.1. For A \u2282 \\mathbb{R}^n , partially verify that \\mathcal{M}(A,\\mathbb{R}^m) is a vector space over \\mathbb{R} by showing that it satisfies vector space axioms (A4) and (D1). Proof : (A4) Addition is commutative \\begin{align*} (f+g)(x) &= f(x) + g(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= g(x) + f(x) \\quad &\\text{by commutativity of \u201c+\u201d in }\\mathbb{R}^m\\\\ &= (g+f)(x) &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m) \\end{align*} Next, (D1) Scalar multiplication distributes over scalar addition \\begin{align*} ((a+b)f)(x) &= (a+b)f(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= af(x) + bf(x) \\quad &\\text{by D1 in }\\mathbb{R}^m\\\\ &= (af)(x) + (bf)(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= ((af) + (bf))(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ \\end{align*} \\square 2.3.2. Define multiplication * : \\mathcal{M}(A, \\mathbb{R}^m) \\times \\mathcal{M}(A, \\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A, \\mathbb{R}^m) Is \\mathcal{M}(A, \\mathbb{R}^m) a field with \u201c+\u201d from the section and this multiplication? Does it have a subspace that is a field? Solution Define: (f * g)(x) = f(x) * g(x) And f(x) = 1 is it's multiplication identity. However, this function does not have inverse. f(x) = \\begin{cases} 0 &\\text{if } x = 0 \\\\ 1 &\\text{if } x \\neq 0 \\\\ \\end{cases} Consider V = \\{f : f(x) = (r, \\cdots, r) \\in \\mathbb{R}^m \\text{ for all } x \\in A\\} This is a field. \\square 2.3.3. For A \u2282 \\mathbb{R}^n and m \u2208 \\mathbb{Z}^+ define a subspace of the space of mappings from A to \\mathbb{R}^m \\mathcal{C}(A, \\mathbb{R}^m) = \\{ f \\in \\mathcal{M}(A, \\mathbb{R}^m) : f \\text{ is continuous on } A \\}. Briefly explain how this section has shown that \\mathcal{C}(A, \\mathbb{R}^m) is a vector space. Solution : It's stated in Proposition 2.3.7 (Vector space properties of continuity). \\square 2.3.4. Define an inner product and a modulus on \\mathcal{C}([0,1], \\mathbb{R}) by \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt, \\quad |f| = \\sqrt[]{\u27e8f,f\u27e9}. Do the inner product properties (IP1),(IP2),and(IP3) (see Proposition2.2.2) hold for this inner product on \\mathcal{C}([0,1], \\mathbb{R}) How much of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) ? Express the Cauchy\u2013Schwarz inequality as a relation between integrals. Proof : (IP1) \u27e8f, f\u27e9 = \\int_{0}^{1} f(t)f(t)dt \\geq \\int_{0}^{1} 0dt = 0 It achieves 0 only when f(t) = 0 . (IP2) \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt = \\int_{0}^{1} g(t)f(t)dt = \u27e8g,f\u27e9 So IP2 holds. (IP3) First, we want to show \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh Based on the definition of integral, give a partition P on [0,1] . L(fg, P) + L(fh, P) \\leq L(fg+fh, P) \\leq U(fg+fh, P) \\leq U(fg, P) + L(fh, P) So we have \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh. Now \u27e8f,g+h\u27e9 = \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh = \u27e8f,g\u27e9 + \u27e8f,h\u27e9 Also \u27e8af,g\u27e9 = \\int_{0}^{1} (af)g = a(\\int_{0}^{1} fg) = a \u27e8f,g\u27e9 \\square All of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) Cauchy\u2013Schwarz inequality \\left| \\int_{0}^{1} f(t)g(t) dt \\right| \\leq \\sqrt[]{\\int_{0}^{1}f^2(t)dt} \\cdot \\sqrt[]{\\int_{0}^{1}g^2(t)dt} \\square 2.3.5 Use the definition of convergence and the componentwise nature of nullness to prove the componentwise nature of convergence. (The argument is short.) Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p = \\text{null} \\\\ \\text{definition of convergence} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j = \\text{null} \\\\ \\text{Lemma 2.3.2 (Componentwise nature of nullness).} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\\\ \\text{definition of convergence} \\\\ \\square 2.3.6. Use the definition of continuity and the componentwise nature of convergence to prove the componentwise nature of continuity. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square 2.3.7 Prove the persistence of continuity under composition. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square 2.3.8. Define f : \\mathbb{Q} \\longrightarrow \\mathbb{R} by the rule f(x) = \\begin{cases} 1 &\\text{if } x^2 < 2,\\\\ 0 &\\text{if } x^2 > 2.\\\\ \\end{cases} Solution : It is continuous. Given any p \\in \\mathbb{Q} , if p^2 < 2 , we can always find a U_{\\delta}(p) such that if q \\in U_{\\delta}(p) , then q^2 < 2 . Same thing when p^2 > 2 . \\square 2.3.9. Which of the following functions on \\mathbb{R}^2 can be defined continuously at 0 ? (a) f(x, y) = \\begin{cases} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution \\begin{split} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &= \\frac{x^2 - y^2}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\begin{split} \\frac{x^2 - y^2}{x^2 + y^2} &= \\frac{1 - m^2}{1 + m^2} \\end{split} So it cannot be defined continuously at 0 . \\square (b) g(x, y) = \\begin{cases} \\frac{x^2 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\frac{x^2 - y^3}{x^2 + y^2} &= 1 - \\frac{y^2 + y^3}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\frac{y^2 + y^3}{x^2 + y^2} \\\\ = \\frac{m^2 + m^3y}{1+m^2} \\rightarrow \\frac{m^2}{1+m^2} So it cannot be defined continuously at 0 . \\square (c) h(x, y) = \\begin{cases} \\frac{x^3 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\left| \\frac{x^3 - y^3}{x^2 + y^2} \\right| &= \\left| (x-y) \\frac{x^2 + xy + y^2}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{xy}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{|(x,y)||(x,y)|}{|(x,y)|^2} \\right| \\\\ & = 2 |x - y| \\\\ & \\leq 2 (|x| + |y|) \\rightarrow 0 \\end{split} So it can be defined continuously at 0. \\square (d) k(x, y) = \\begin{cases} \\frac{xy^2}{x^2 + y^6} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : When (x,y) approaches 0 with y = x , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^3}{x^2 + x^6} = \\frac{x}{1 + x^4} \\rightarrow 0 However, when (x,y) approaches 0 with y = \\sqrt[]{x} , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^2}{x^2 + x^3} = \\frac{1}{1 + x} \\rightarrow 1 So it cannot be defined continuously at 0 . \\square 2.3.10. Let f(x,y) = g(xy) , where g : \\mathbb{R} \u2192 \\mathbb{R} is continuous. Is f continuous? Proof : Let h(x,y) = xy , then h(x,y) is a continuous function. Then from Proposition 2.3.8 (Persistence of continuity under composition), f = g \\circ h . So f is also continuous. \\square 2.3.11. Let f,g \\in \\mathcal{M}(\\mathbb{R}^n, \\mathbb{R}) such that f +g and fg are continuous. Are f and g necessarily continuous? Proof : No, it's not necessary. Consider the case when n = 1 . f(x) = \\begin{cases} 0 &\\text{if } x \\in \\mathbb{Q}\\\\ 1 &\\text{if } x \\in \\mathbb{I}\\\\ \\end{cases} Then g(x) = 1 - f(x) . Then f+g = 1, fg = 0 are continuous. But f, g are not continuous. \\square 2.4 Topology: Compact Sets and Continuity 2.4.1. Are the following subsets of \\mathbb{R}^n closed, bounded, compact? (a) B(0,1) : not closed, bounded, not compact. (b) \\{(x,y) \\in \\mathbb{R}^2 : y - x^2 = 0\\} : closed, not bounded, not compact. (c) \\{(x,y,z) \\in \\mathbb{R}^3: x^2 +y^2 +z^2 \u22121 = 0 \\} closed, bounded, compact. (d) \\{x : f(x) = 0_m\\} , where f \\in \\mathcal{M}(\\mathbb{R}^n, \\mathbb{R}^m) is continuous (this generalizes (b) and (c)), Solution : Consider \\{x_\u03bd\\} \\rightarrow p , since f is continuous, \\{f(x_\u03bd)\\} \\rightarrow f(p) . So f(p) = 0 . So it's closed. It can be either bounded or unbounded, so either compact or not compact. (f) \\{(x_1,...,x_n) : x_1 +\u00b7\u00b7\u00b7+x_n > 0\\} . not closed, not bounded, not compact. \\square 2.4.2. Give a set A \u2282 \\mathbb{R}^n and limit point b of A such that b \\not\\in A . Give a set A \u2282 \\mathbb{R}^n and a point a \u2208 A such that a is not a limit point of A . Solution : Consider n = 1 , let A = (0, 1) , b = 0 . Let A = \\mathbb{N}, a = 0 . \\square 2.4.3. Let A be a closed subset of \\mathbb{R}^n and let f \u2208 \\mathcal{M}(A, \\mathbb{R}^m) . Define the graph of f to be G(f) = \\{(a, f(a)) : a \\in A\\} a subset of \\mathbb{R}^{n+m} . Show that if f is continuous then its graph is closed. Proof : Given \\{x_\u03bd\\} \\rightarrow p , \\{x_\u03bd\\} \\in G(f) . Let x_\u03bd = (a_\u03bd, f(a_\u03bd)) . Then from Proposition 2.3.5 (Componentwise nature of convergence), \\{a_\u03bd\\} \\rightarrow p_0 , and since f is continuous \\{f(a_\u03bd)\\} \\rightarrow f(p_0) . Also since A is closed, so p_0 \\in A . So p = (p_0, f(p_0) \\in G(f) . Then G(f) is closed. \\square 2.4.4. Prove the closed set properties: (1) the empty set \u2205 and the full space \\mathbb{R}^n are closed subsets of \\mathbb{R}^n ; (2) every intersection of closed sets is closed; (3) every finite union of closed sets is closed. Proof : (1) \u2205 does not have limit point, so \u2205 contains all it's limit point, which is nothing. \\mathbb{R}^n 's limit points are in \\mathbb{R}^n . So \\mathbb{R}^n is closed. (2) Let C = \\cap C_i , p is a limit point of C . So we can find a \\{x_\u03bd\\} \\rightarrow p , where x_\u03bd \\in C . Then x_\u03bd \\in C_i . So p \\in C_i . Then p \\in C . (3) Let C = \\cup_{i=1}^{n} C_i , p is a limit point of C . So we can find a \\{x_\u03bd\\} \\rightarrow p , where x_\u03bd \\in C . Then we can find at least one i , such that there are infinitely many x_\u03bd \\in C_i . It is a subsequence of \\{x_\u03bd\\} and also convergences to p . So p \\in C_i \\subseteq C . \\square 2.4.5. Prove that every ball B(p,\u03b5) is bounded in \\mathbb{R}^n . Proof : Assume x \\in B(p,\u03b5) , then |x| \\leq |x-p| + |p| < \\epsilon + p \\square 2.4.6. Show that A is a bounded subset of \\mathbb{R}^n if and only if for each j \u2208 \\{1,...,n\\} , the j th coordinates of its points form a bounded subset of \\mathbb{R} . Proof : We will need Proposition 2.2.7 (Size bounds) A \\text{ is a bounded } \\\\ \\Longleftrightarrow \\\\ |x| < R, \\forall x \\in A \\\\ \\Longleftrightarrow \\\\ |x_j| < R \\\\ \\Longleftrightarrow \\\\ j\\text{th coordinates of its points form a bounded subset of} \\mathbb{R} 2.4.7. Show by example that a closed set need not satisfy the sequential characterization of bounded sets, and that a bounded set need not satisfy the sequential characterization of closed sets. Solutions: Consider A = [0, +\\infty) which is a closed set. Let \\{x_\u03bd\\} = \\{1, 2, 3, \\cdots\\} , then it does not have a subsequence that convergences. Consider A = (0, 1) which is bounded. Let \\{x_\u03bd\\} = \\{1, 1/2, 1/3, \\cdots\\} , we have \\{x_\u03bd\\} \\rightarrow 0 \\not\\in A . \\square 2.4.8. Show by example that the continuous image of a closed set need not be closed, that the continuous image of a closed set need not be bounded, that the continuous image of a bounded set need not be closed, and that the continuous image of a bounded set need not be bounded. Solution : Consider f(x) = \\frac{1}{x}, x \\in [1, +\\infty) . f(C) = (0, 1] which is not closed. Consider f(x) = x, x \\in [1, +\\infty) . f(C) = [1, +\\infty) which is not bounded. Consider f(x) = x, x \\in (0, 1) . f(C) = (0, 1) which is not closed. Consider f(x) = 1/x, x \\in (0, 1) . f(C) = [1, +\\infty) which is not bounded. 2.4.9. A subset A of \\mathbb{R}^n is called discrete if each of its points is isolated. (Recall that the term isolated was defined in this section.) Show or take for granted the (perhaps surprising at first) fact that every mapping whose domain is discrete must be continuous. Is discreteness a topological property? That is, need the continuous image of a discrete set be discrete? Solution : First we show a function f : A \\longrightarrow \\mathbb{R}^n is continuous at any point a \\in A . Give any \\{x_\u03bd\\} \\rightarrow a , since a is isolated, we must have x_\u03bd = a for \u03bd > \\Nu , then \\{f(x_\u03bd)\\} \\rightarrow f(a) , which means f is continuous. Consider f(x) = \\begin{cases} 0 &\\text{if } x = 0\\\\ 1/x &\\text{otherwise}\\\\ \\end{cases} and let A = \\mathbb{N} . So 0 is in f(A) , and 0 is not isolated. \\square 2.4.10 A subset A of \\mathbb{R}^n is called path-connected if for every two points x,y \u2208 A , there is a continuous mapping \u03b3 : [0,1] \\longrightarrow A such that \u03b3(0) = x and \u03b3(1) = y . (This \u03b3 is the path that connects x and y .) Draw a picture to illustrate the definition of a path-connected set. Prove that path-connectedness is a topological property. Proof : Let f : A \\longrightarrow \\mathbb{R}^m is a continuous map. Given x, y \\in A , we want to show f(x), f(y) is also path-connected. From Proposition 2.3.8 (Persistence of continuity under composition), f \\circ \u03b3 : [0,1] \\longrightarrow \\mathbb{R}^m is a continuous map and (f \\circ \u03b3)(0) = f(x) \\\\ (f \\circ \u03b3)(1) = f(y) \\\\ So f(x), f(y) is also path-connected. \\square","title":"Chapter 02 Exercises"},{"location":"ch02ex/#chapter-2-euclidean-space","text":"","title":"Chapter 2 Euclidean Space"},{"location":"ch02ex/#21-algebra-vectors","text":"","title":"2.1 Algebra: Vectors"},{"location":"ch02ex/#211","text":"Write down any three specific nonzero vectors u , v , w from \\mathbb{R}^3 and any two specific nonzero scalars a, b from \\mathbb{R} . Compute u+v, aw, b(v+w), (a+b)u , u+v +w, abw , and the additive inverse to u . Solution : Let u = (1,0,0) \\\\ v = (0,1,0) \\\\ w = (0,0,1) \\\\ a = 1, b = -1 \\\\ u + v = (1, 1, 0) \\\\ aw = w = (0,0,1) \\\\ b(v + w) = (0,-1,-1) \\\\ (a+b)u = 0 \\\\ u+v+w = (1,1,1) \\\\ ab(w) = (0,0,-1) \\\\","title":"2.1.1."},{"location":"ch02ex/#212","text":"Working in \\mathbb{R}^2 , give a geometric proof that if we view the vectors x and y as arrows from 0 and form the parallelogram P with these arrows as two of its sides, then the diagonal z starting at 0 is the vector sum x + y viewed as an arrow. Proof : See the figure below.","title":"2.1.2."},{"location":"ch02ex/#213","text":"Verify that \\mathbb{R}^n satisfies vector space axioms (A2), (A3), (D1). Proof : (A2) 0 is an additive identity 0 + x = (0, \\cdots, 0) + (x_1, \\cdots, x_n) \\\\ = (0 + x_1, \\cdots, 0 + x_n) \\\\ = (x_1, \\cdots, x_n) \\\\ = x \\square (A3) Existence of additive inverses Let x = (x_1, \\cdots, x_n) and y = (-x_1, \\cdots, -x_n) y + x = (-x_1 + x_1, \\cdots, -x_n + x_n) \\\\ = (0, \\cdots, 0) So y is the additive inverse of x . \\square (D1) Scalar multiplication distributes over scalar addition (a+b)x = (a+b) (x_1, \\cdots, x_n) \\\\ = ((a+b)x_1, \\cdots, (a+b)x_n) \\\\ = (ax_1 + bx_1, \\cdots, ax_n + bx_n) \\\\ = (ax_1, \\cdots, ax_n) + (bx_1, \\cdots, bx_n) \\\\ = ax + bx \\square","title":"2.1.3."},{"location":"ch02ex/#214","text":"Are all the field axioms used in verifying that Euclidean space satisfies the vector space axioms? Solution : Let's check the vector space axioms one by one. (A1) Addition is associative uses (Fa1) (A2) 0 is an additive identity uses (Fa2) (A3) Existence of additive inverses (Fa3) (A4) Addition is commutative (Fa4) (M1) Scalar multiplication is associative (Fm1) (M2) 1 is a multiplicative identity (Fm2) (D1) Scalar multiplication distributes over scalar addition (Fd1) (D2) Scalar multiplication distributes over vector addition (Fd1) So we can see (Fm3) and (Fm4) are not used. \\square","title":"2.1.4."},{"location":"ch02ex/#215","text":"Show that 0 is the unique additive identity in \\mathbb{R}^n . Show that each vector x \u2208 \\mathbb{R}^n has a unique additive inverse, which can therefore be denoted \u2212x . (And it follows that vector subtraction can now be defined, - : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^n, \\quad x - y = x + (-y) for all x, y \\in \\mathbb{R}^n . Show that 0x = 0 for all x \u2208 \\mathbb{R}^n . Proof : Assume x = (x_1, \\cdots, x_n) is a additive identity and y = (y_1, \\cdots, y_n) \\in \\mathbb{R}^n . Then x + y = y , i.e. x_1 + y_1 = y_1, \\cdots , x_n + y_n = y_n So x_1 = \\cdots = x_n = 0 , so x = 0 . And thus 0 is unique. Assume x = (x_1, \\cdots, x_n) and y (y_1, \\cdots, y_n) \\in \\mathbb{R}^n is x 's additive inverse. x + y = (x_1 + y_1, \\cdots, x_n + y_n) = 0 So y = (-x_1, \\cdots, -x_n) The additive inverse is unique. 0x = 0 (x_1, \\cdots, x_n) = (0x_1, \\cdots, 0x_n) = (0, \\cdots, 0) \\square","title":"2.1.5."},{"location":"ch02ex/#216","text":"Repeat the previous exercise, but with \\mathbb{R}^n replaced by an arbitrary vector space V over a field F . (Work with the axioms.) 0 = 0' + 0 = 0 + 0' = 0' So 0 is unique. y = y + 0 = y + (x + y') = (y + x) + y' = 0 + y' = y' So the additive inverse is unique. 0x + 1x = (0 + 1) x = 1 x = x Then we can add -x on both side 0x + x + (-x) = 0x \\\\ \\Rightarrow \\\\ x + (-x) = 0 \\\\ \\Rightarrow \\\\ 0x = 0 \\square","title":"2.1.6."},{"location":"ch02ex/#217","text":"Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.) Proof : I cannot figure it for now. The uniqueness of the additive identity. Assume 0 and 0' are 2 additive identities. Then 0' + 0 = 0 \\\\ 0 + 0' = 0' \\\\ 0' + 0' = 0' \\\\","title":"2.1.7."},{"location":"ch02ex/#218","text":"Let x and y be noncollinear vectors in \\mathbb{R}^3 . Give a geometric description of the set of all linear combinations of x and y . Solution : All the linear combinations of x and y form a plane that includes 0, x, y . \\square","title":"2.1.8."},{"location":"ch02ex/#219","text":"Which of the following sets are bases of \\mathbb{R}^3 ? S_1 = \\{(1,0,0),(1,1,0),(1,1,1)\\} \\\\ S_2 = \\{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\\} \\\\ S_3 = \\{(1,1,0),(0,1,1)\\} \\\\ S_4 = \\{(1,1,0),(0,1,1),(1,0,-1)\\} \\\\ How many elements do you think a basis for \\mathbb{R}^n must have? Give (without proof) geometric descriptions of all bases of \\mathbb{R}^2 , of \\mathbb{R}^3 . Solution : For S_1 , given (x,y,z) , we have \\begin{cases} x &= a + b + c\\\\ y &= b + c \\\\ z &= c \\\\ \\end{cases} Then we can have \\begin{cases} a = x - y\\\\ b = y - z\\\\ c = z\\\\ \\end{cases} So S_1 is a base. For S_2 , it is note a base. For example (1,1,1) can be expressed by multiple ways. For S_3 \\begin{cases} x &= a \\\\ y &= a + b \\\\ z &= b \\\\ \\end{cases} Then we cannot represent (1,3,1) with S_3 . For S_4 , We have \\begin{cases} x &= a + c\\\\ y &= a + b \\\\ z &= b - c\\\\ \\end{cases} Then we have to have z = y - x , so we cannot represent (1,1,1) with S_4 . For \\mathbb{R}^n , it needs n element for a basis. For \\mathbb{R}^2 , if \\{f_1, f_2\\} is a basis, then geometrically, they should not be in one line. For \\mathbb{R}^3 , if \\{f_1, f_2, f_3\\} is a basis, then geometrically, they should not be in the same plane. \\square","title":"2.1.9"},{"location":"ch02ex/#2110","text":"Recall the field \\mathbb{C} of complex numbers. Define complex n -space \\mathbb{C}^n analogously to \\mathbb{R}^n : \\mathbb{C}^n = \\{ (z_1, \\cdots, z_n) : z_i \\in \\mathbb{C} \\text{ for } i = 1, \\cdots, n \\} and endow it with addition and scalar multiplication defined by the same formulas as for \\mathbb{R}^n . You may take for granted that under these definitions, \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{R} , and also \\mathbb{C}^n satisfies the vector space axioms with scalar multiplication by scalars from \\mathbb{C} . That is, using language that was introduced briefly in this section, \\mathbb{C}^n can be viewed as a vector space over \\mathbb{R} and also, separately, as a vector space over \\mathbb{C} . Give a basis for each of these vector spaces. Solution : For the first one S_1 = \\{ (1,\\cdots,0), (i,\\cdots,0), \\cdots (0,\\cdots,1), (0,\\cdots,i) \\} For the second one S_2 = \\{ (1,\\cdots,0), \\cdots (0,\\cdots,1) \\} \\square","title":"2.1.10."},{"location":"ch02ex/#22-geometry-length-and-angle","text":"","title":"2.2 Geometry: Length and Angle"},{"location":"ch02ex/#221","text":"Let x = (\\frac{\\sqrt[]{3}}{2}, -\\frac{1}{2}, 0), y = (\\frac{1}{2}, \\frac{\\sqrt[]{3}}{2}, 1), z = (1, 1, 1) Compute \u27e8x,x\u27e9, \u27e8x,y\u27e9, \u27e8y,z\u27e9, |x|, |y|, |z|, \u03b8_{x,y}, \u03b8_{y,e_1}, \u03b8_{z,e_2} Solution : \u27e8x,x\u27e9 = \\frac{3}{4} + \\frac{1}{4} + 0 = 1 \\\\ \u27e8x,y\u27e9 = \\frac{\\sqrt[]{3}}{4} - \\frac{\\sqrt[]{3}}{4} + 0 = 0 \\\\ \u27e8y,z\u27e9 = \\frac{1}{2} + \\frac{\\sqrt[]{3}}{2} + 1 = \\frac{3 + \\sqrt[]{3}}{2} \\\\ |x| = 1 \\\\ |y| = \\sqrt[]{2} \\\\ |z| = \\sqrt[]{3} \\\\ \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x| |y|} = \\frac{0}{|x| |y|} = 0 \\\\ \u03b8_{y,e_1} = \\frac{\u27e8y,e_1\u27e9}{|y| |e_1|} = \\frac{1/2}{|x| |y|} = \\frac{1}{2 \\sqrt[]{2}} \\\\ \u03b8_{z,e_2} = \\frac{1}{\\sqrt[]{3}} \\square","title":"2.2.1"},{"location":"ch02ex/#222","text":"Show that the points x = (2,\u22121,3,1), y = (4,2,1,4), z = (1,3,6,1) form the vertices of a triangle in \\mathbb{R}^4 with two equal angles. Solution : d(x,y) = |x - y| = \\sqrt[]{\u27e8x-y,x-y\u27e9} = \\sqrt[]{\u27e8(-2,-3,2,-3),(-2,-3,2,-3)\u27e9} = \\sqrt[]{4+9+4+9} = \\sqrt[]{26} \\\\ d(y,z) = |y-z| = \\sqrt[]{\u27e8y-z,y-z\u27e9} = \\sqrt[]{\u27e8(3,-1,-5,3),(3,-1,-5,3)\u27e9} = \\sqrt[]{9+1+25+9} = \\sqrt[]{44} \\\\ d(z,x) = |z-x| = \\sqrt[]{\u27e8z-x,z-x\u27e9} = \\sqrt[]{\u27e8(-1,4,3,0),(-1,4,3,0)\u27e9} = \\sqrt[]{1+16+9+0} = \\sqrt[]{26} \\\\ \\square","title":"2.2.2."},{"location":"ch02ex/#223","text":"Explain why for all x \u2208 \\mathbb{R}^n , x = \\sum_{j=1}^{n} \u27e8x,e_j\u27e9e_j . Proof : Note \u27e8x,e_j\u27e9 = \u27e8(x_1, \\cdots, x_n),(0, \\cdots, 1, \\cdots, 0)\u27e9 = x_j \\\\ x_j e_j = (0, \\cdots, x_j, \\cdots, 0) \\\\ \\sum_{j=1}^{n} x_j e_j = (x_1, \\cdots, x_n) = x \\square","title":"2.2.3."},{"location":"ch02ex/#224","text":"Prove the inner product properties. Proof : (IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 = x_1^2 + \\cdots + x_n^2 \\geq 0 It equals 0 only when x_1 = \\cdots = x_n = 0 . \\square (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n \u27e8x,y\u27e9 = x_1 y_1 + \\cdots + x_n y_n = y_1 x_1 + \\cdots + y_n x_n = \u27e8y,x\u27e9 \\square (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = (x_1 + x'_1) y_1 + \\cdots + (x_n + x'_n) y_n \\\\ = (x_1 y_1 + \\cdots + x_n y_n) + (x'_1 y_1 + \\cdots + x'_n y_n) \\\\ = \u27e8x,y\u27e9 + \u27e8x',y\u27e9 \u27e8ax,y\u27e9 = (ax_1) y_1 + \\cdots + (ax_n) y_n \\\\ = a (x_1 y_1 + \\cdots + x_n y_n) \\\\ = a \u27e8x,y\u27e9 \\square","title":"2.2.4."},{"location":"ch02ex/#225","text":"Use the inner product properties and the definition of the modulus in terms of the inner product to prove the modulus properties. Proof : (Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ |x| = \\sqrt[]{\u27e8x,x\u27e9} \\geq 0 \u27e8x,x\u27e9 = 0 if and only if x = 0 . \\square (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n . \u27e8ax,ax\u27e9 = a \u27e8x,ax\u27e9 = a^2 \u27e8x,x\u27e9 So |ax| = \\sqrt[]{\u27e8ax,ax\u27e9} = \\sqrt[]{a^2 \u27e8x,x\u27e9} = |a||x| \\square","title":"2.2.5."},{"location":"ch02ex/#226","text":"In the text, the modulus is defined in terms of the inner product. Prove that this can be turned around by showing that for every x,y \u2208 \\mathbb{R}^n , \u27e8x,y\u27e9 = \\frac{|x+y|^2 - |x-y|^2}{4}. Proof : |x+y|^2 = \u27e8x+y,x+y\u27e9 = \u27e8x,x\u27e9 + 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ |x-y|^2 = \u27e8x-y,x-y\u27e9 = \u27e8x,x\u27e9 - 2 \u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ So the equation holds. \\square","title":"2.2.6."},{"location":"ch02ex/#227","text":"Prove the full triangle inequality: for every x,y \u2208 \\mathbb{R}^n , ||x| - |y|| \\leq |x \\pm y | \\leq |x| + |y| Do not do this by writing three more variants of the proof of the triangle inequality, but by substituting suitably into the basic triangle inequality, which is already proved. Proof : |x - y| \\leq |x| + |y| Note \\begin{split} |x-y| & = |x + (-y)| \\\\ & \\leq |x| + |-y| \\\\ & = |x| + |y| \\\\ \\end{split} 2. ||x| - |y|| \\leq |x + y| Note \\begin{split} |x| &= |(-y) + (x+y)| \\\\ & \\leq |-y| + |x+y| \\\\ & = |y| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |x| - |y| \\leq |x+y| And similarly \\begin{split} |y| &= |(-x) + (x+y)| \\\\ & \\leq |-x| + |x+y| \\\\ & = |x| + |x+y| \\\\ \\end{split} \\\\ \\Rightarrow \\\\ |y| - |x| \\leq |x+y| Combine these 2 together, we have ||x| - |y|| \\leq |x + y| 3. ||x| - |y|| \\leq |x - y| This is similar to 2. \\square","title":"2.2.7."},{"location":"ch02ex/#228","text":"Let x = (x_1,...,x_n) \u2208 \\mathbb{R}^n . Prove the size bounds: for every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{j=1}^{n} |x_j|. (One approach is to start by noting that x_j= \u27e8x,e_j\u27e9 and recalling equation (2.1).) When can each \u2264 be an = ? Proof : From the hint: \\begin{split} |x_j| &= |\u27e8x,e_j\u27e9| \\\\ & \\leq |x||e_j| \\\\ & \\text{Cauchy-Schwarz inequality} \\\\ & = |x| \\cdot 1 \\\\ & = |x| \\end{split} This equality holds when x = a \\cdot e_j . Note \\begin{split} |x| &= \\left| \\sum_{j=1}^{n} x_j e_j \\right| \\\\ & \\leq \\sum_{j=1}^{n} \\left| x_j e_j \\right| \\\\ & \\text{Generalized triangle inequality}\\\\ & = \\sum_{j=1}^{n} | x_j | | e_j | \\\\ & \\text{The modulus is absolute-homogeneous}\\\\ & = \\sum_{j=1}^{n} | x_j | \\\\ \\end{split} This equality holds when only only one of x_j is not 0 . \\square","title":"2.2.8."},{"location":"ch02ex/#229","text":"Prove the distance properties. Proof : (D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . Note d(x,y) = |x-y| \\geq 0 The equality holds only when x-y = 0 , i.e. x = y . \\square (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . Note: d(x,y) = |x-y| = |y-x| = d(y,x) \\square \\square (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n . Note: \\begin{split} d(x,z) &= |x-z| \\\\ &= |(x-y)+(y-z)| \\\\ & \\leq |x-y| + |y-z| \\\\ &= d(x,y) + d(y,z) \\end{split} \\square","title":"2.2.9."},{"location":"ch02ex/#2210","text":"Working in \\mathbb{R}^2 , depict the nonzero vectors x and y as arrows from the origin and depict x\u2212y as an arrow from the endpoint of y to the endpoint of x . Let \u03b8 denote the angle (in the usual geometric sense) between x and y . Use the law of cosines to show that \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} Proof Geometric proof: \\frac{\u27e8x,y\u27e9}{|x||y|} = \\frac{x_1 y_1 + x_2 y_2}{|x||y|} = \\frac{1}{|x|} \\left( \\frac{x_1 y_1 + x_2 y_2}{|y|} \\right) As shown in the figure below, XD \\perp OY , AC \\perp OY and XB \\perp AC . We just need to show \\frac{x_1 y_1 + x_2 y_2}{|y|} = OD Note \\frac{y_1}{y} = \\cos \\angle AOC \\\\ \\Rightarrow \\\\ \\frac{x_1 y_1}{|y|} = OC \\frac{y_2}{y} = \\cos \\angle AXB \\\\ \\Rightarrow \\\\ \\frac{x_2 y_2}{|y|} = CD Then we finished the proof. Proof with triangle equalities: x_1 = |x| \\cos \u03b8_x, x_2 = |x| \\sin \u03b8_x, y_1 = |y| \\cos \u03b8_y, y_2 = |y| \\sin \u03b8_y, Then \u27e8x,y\u27e9 = |x||y| \\cos \u03b8_x \\cos \u03b8_y + |x||y| \\sin \u03b8_x \\sin \u03b8_y \\\\ = |x||y| \\cos (\u03b8_x - \u03b8_y) = |x||y| \\cos \u03b8 This also proved. \\square","title":"2.2.10"},{"location":"ch02ex/#2211","text":"Prove that for every nonzero x \u2208 \\mathbb{R}^n , \\sum_{i = 1}^{n} \\cos ^2 \u03b8_{x, e_i} = 1 . Proof : \\cos ^2 \u03b8_{x, e_i} = \\sum_{i = 1}^{n} \\left( \\frac{\u27e8x,e_i\u27e9}{|x||e_i|} \\right)^2 \\\\ \\sum_{i = 1}^{n} \\frac{x_i^2}{|x|^2} \\\\ = \\frac{1}{|x|^2} \\sum_{i = 1}^{n} x_i^2 \\\\ = 1 \\square","title":"2.2.11."},{"location":"ch02ex/#2212","text":"Prove that two nonzero vectors x, y are orthogonal if and only if |x+y|^2 = |x|^2 +|y|^2. Proof : \\begin{split} |x+y|^2 = \u27e8x+y,x+y\u27e9 = |x|^2 + 2 \u27e8x,y\u27e9 + |y|^2 \\end{split} Then |x+y|^2 = |x|^2 +|y|^2 means \u27e8x,y\u27e9=0 . \\square","title":"2.2.12."},{"location":"ch02ex/#2213","text":"Use vectors in \\mathbb{R}^2 to show that the diagonals of a parallelogram are perpendicular if and only if the parallelogram is a rhombus. Proof : The 2 diagonals of the parallelogram are x+y, x-y . \u27e8x+y,x-y\u27e9 = |x|^2 - |y|^2 If x+y \\perp x-y , then |x|^2 - |y|^2=0 , i.e. |x| = |y| . On the other hand, if |x| = |y| , then \u27e8x+y,x-y\u27e9 . \\square","title":"2.2.13."},{"location":"ch02ex/#2214","text":"Use vectors to show that every angle inscribed in a semicircle is right. Let (x,y) on the semicircle, then x^2+y^2=1 . consider (x,y) - (-1,0) = (x+1,y) and (x,y) - (1,0) = (x-1,y) . Then \u27e8(x+1,y), (x-1, y)\u27e9 = x^2 - 1 + y^2 = 0 \\square","title":"2.2.14."},{"location":"ch02ex/#2215","text":"Let x and y be vectors, with x nonzero.Define the parallel component of y along x and the normal component of y to x to be y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x \\qquad \\text{and} \\qquad y_{(\\perp x)} = y - y_{(\\parallel x)} Answer: (a) Show that y = y_{(\\parallel x)} + y_{(\\perp x)} . show that y_{(\\parallel x)} is a scalar multiple of x . show that y_{(\\perp x)} is orthogonal to x . Show that the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. Draw an illustration. Proof : y_{(\\parallel x)} + y_{(\\perp x)} = \\\\ y_{(\\parallel x)} + (y - y_{(\\parallel x)}) = \\\\ y y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x Since \\frac{\u27e8x,y\u27e9}{|x|^2} is a scalar, then y_{(\\parallel x)} is a scalar multiple of x . \u27e8y - y_{(\\parallel x)},y_{(\\parallel x)}) = \\\\ \u27e8|x|^2 y - \u27e8x,y\u27e9x, \u27e8x,y\u27e9x\u27e9 \\\\ = |x|^2 \u27e8x,y\u27e9 \u27e8y,x\u27e9 - \u27e8x,y\u27e9^2 \u27e8x,x\u27e9 \\\\ = 0 So y_{(\\perp x)} is orthogonal to y_{(\\parallel x)} , thus is orthogonal to x . In general if x \\perp y and ax + by = 0, x,y \\neq 0 , then 0 = \u27e80, x\u27e9 = \u27e8ax + by,x\u27e9 = a \u27e8x,x\u27e9 + b \u27e8y,x\u27e9 = a \u27e8x,x\u27e9 \\geq 0 \\\\ \\Rightarrow \\\\ a = 0 For the same reason b = 0 . With this, the decomposition of y as a sum of vectors parallel and perpendicular to x is unique. \\square (b) Show that |y|^2 = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 What theorem from classical geometry does this encompass? Proof : |y|^2 = \\\\ |y_{(\\parallel x)} + y_{(\\perp x)}|^2 = \\\\ |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 + 2 \u27e8y_{(\\parallel x)}, y_{(\\perp x)}\u27e9 \\\\ = |y_{(\\parallel x)}|^2 + |y_{(\\perp x)}|^2 \\\\ It's Pythagorean theorem or Gougu theorem. \\square (c) Explain why it follows from (b) that |y_{(\\parallel x)}|^2 \\leq |y|^2 with equality if and only if y is a scalar multiple of x . Use this inequality to give another proof of the Cauchy\u2013Schwarz inequality. This argument gives the geometric content of Cauchy\u2013Schwarz: the parallel component of one vector along another is at most as long as the original vector. Proof : This is because |y_{(\\perp x)}|^2 \\geq 0 . The equality holds when |y_{(\\perp x)}|^2 = 0 . In this case y = y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2} x To prove |\u27e8x,y\u27e9| \\leq |x||y| Note |\u27e8x,y\u27e9| = |\u27e8x,y_{(\\parallel x)}+y_{(\\perp x)}\u27e9| \\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9 + 0|\\\\ = |\u27e8x,y_{(\\parallel x)}\u27e9| Now in general, if a is a scaler, then |\u27e8x,ax\u27e9| = |a \u27e8x,x\u27e9| = |a||x|^2 = |x| |ax| Then |\u27e8x,y_{(\\parallel x)}\u27e9| = |x||y_{(\\parallel x)}|| \\leq |x||y| \\square (d) The proof of the Cauchy\u2013Schwarz inequality in part (c) refers to parts (a) and (b), part (a) refers to orthogonality, orthogonality refers to an angle, and as explained in the text, the fact that angles make sense depends on the Cauchy\u2013Schwarz inequality. And so the proof in part (c) apparently relies on circular logic. Explain why the logic is in fact not circular. Solution : Note that when we define \\cos \u03b8 , it is this: \\cos \u03b8 = \\frac{\u27e8x,y\u27e9}{|x||y|} But here, we define y_{(\\parallel x)} as y_{(\\parallel x)} = \\frac{\u27e8x,y\u27e9}{|x|^2}x If we think geometrically in \\mathbb{R}^2 or \\mathbb{R}^3 , the projection of y on x should be y \\cos \u03b8 . The unit length vector along x is \\frac{x}{|x|} . Also the length of y projection is |y \\cos \u03b8| = \\frac{\u27e8x,y\u27e9}{|x|} So it's reasonable to define y_{(\\parallel x)} this way. Now, since we does not rely on the fact that |\\cos \u03b8| \\leq 1 , it is not circular. \\square","title":"2.2.15."},{"location":"ch02ex/#2216","text":"Given nonzero vectors x_1,x_2,...,x_n in \\mathbb{R}^n , the Gram\u2013Schmidt process is to set \\begin{split} x_1' &= x_1 \\\\ x_2' &= x_2 - (x_2)_{(\\parallel x_1')}\\\\ x_3' &= x_3 - (x_3)_{(\\parallel x_2')} - (x_3)_{(\\parallel x_1')} \\\\ \\vdots \\\\ x_n' &= x_n - (x_n)_{(\\parallel x_{n-1}')} - \\cdots - (x_n)_{(\\parallel x_1')} \\\\ \\end{split} Answer: (a)What is the result of applying the Gram\u2013Schmidt process to the vectors x_1 = (1,0,0), x_2 = (1,1,0) , and x_3 = (1,1,1) ? Solution : x_1' = (1,0,0) \\\\ x_2' = (0,1,0) \\\\ x_3' = (0,0,1) \\\\ \\square (b) Returning to the general case, show that x_1',...,x_n' are pairwise orthogonal and that each x_j' has the form x_j'= a_{j,1} x_1 +a_{j,2}x_2 +\u00b7\u00b7\u00b7+a_{j,j\u22121}x_{j\u22121} +x_j. Thus every linear combination of the new {x_j'} is also a linear combination of the original {x_j} . The converse is also true and will be shown in Exercise 3.3.13. Proof : First, we want to prove a small lemma: Given x, y, z , if x \\perp y , then we want to show z_{(\\parallel x)} \\perp y Note \u27e8z_{(\\parallel x)},y\u27e9 = \u27e8\\frac{\u27e8z,x\u27e9}{|x|^2}x,y\u27e9 = \\frac{\u27e8z,x\u27e9}{|x|^2} \u27e8x,y\u27e9 = 0 Then we can use induction. And assume x_1',...,x_{k-1}' are orthogonal to each other. Assume 1 \\leq j \\leq k-1 , \u27e8x_j',x_{k}'\u27e9 = \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 - \\sum_{i \\neq j} \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 \u27e8x_j',x_k - {x_k}_{(\\parallel x_j')}\u27e9 = 0 is proved in exercise 2.2.15. Also, since \u27e8x_j',x_i'\u27e9=0 for j \\neq i , then \u27e8x_j', {x_k}_{(\\parallel x_i')}\u27e9 = 0 as just proved in the lemma. So \u27e8x_j',x_{k}'\u27e9 = 0 . For the 2nd part, we can still use induction. And assume for x_1',...,x_{k-1}' , they are all linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Then x_k' = x_k - (x_k)_{(\\parallel x_{k-1}')} - \\cdots - (x_k)_{(\\parallel x_1')} Note (x_k)_{(\\parallel x_j')} is a scaler of x_j', j < k , thus a linear combination of x_1, x_2, x_3, \\cdots, x_{k-1} . Therefore, x_k' is a linear combination of x_1,...,x_{k} . \\square","title":"2.2.16."},{"location":"ch02ex/#23-analysis-continuous-mappings","text":"","title":"2.3 Analysis: Continuous Mappings"},{"location":"ch02ex/#231","text":"For A \u2282 \\mathbb{R}^n , partially verify that \\mathcal{M}(A,\\mathbb{R}^m) is a vector space over \\mathbb{R} by showing that it satisfies vector space axioms (A4) and (D1). Proof : (A4) Addition is commutative \\begin{align*} (f+g)(x) &= f(x) + g(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= g(x) + f(x) \\quad &\\text{by commutativity of \u201c+\u201d in }\\mathbb{R}^m\\\\ &= (g+f)(x) &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m) \\end{align*} Next, (D1) Scalar multiplication distributes over scalar addition \\begin{align*} ((a+b)f)(x) &= (a+b)f(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= af(x) + bf(x) \\quad &\\text{by D1 in }\\mathbb{R}^m\\\\ &= (af)(x) + (bf)(x) \\quad &\\text{by definition of \u201c.\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ &= ((af) + (bf))(x) \\quad &\\text{by definition of \u201c+\u201d in } \\mathcal{M}(A,\\mathbb{R}^m)\\\\ \\end{align*} \\square","title":"2.3.1."},{"location":"ch02ex/#232","text":"Define multiplication * : \\mathcal{M}(A, \\mathbb{R}^m) \\times \\mathcal{M}(A, \\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A, \\mathbb{R}^m) Is \\mathcal{M}(A, \\mathbb{R}^m) a field with \u201c+\u201d from the section and this multiplication? Does it have a subspace that is a field? Solution Define: (f * g)(x) = f(x) * g(x) And f(x) = 1 is it's multiplication identity. However, this function does not have inverse. f(x) = \\begin{cases} 0 &\\text{if } x = 0 \\\\ 1 &\\text{if } x \\neq 0 \\\\ \\end{cases} Consider V = \\{f : f(x) = (r, \\cdots, r) \\in \\mathbb{R}^m \\text{ for all } x \\in A\\} This is a field. \\square","title":"2.3.2."},{"location":"ch02ex/#233","text":"For A \u2282 \\mathbb{R}^n and m \u2208 \\mathbb{Z}^+ define a subspace of the space of mappings from A to \\mathbb{R}^m \\mathcal{C}(A, \\mathbb{R}^m) = \\{ f \\in \\mathcal{M}(A, \\mathbb{R}^m) : f \\text{ is continuous on } A \\}. Briefly explain how this section has shown that \\mathcal{C}(A, \\mathbb{R}^m) is a vector space. Solution : It's stated in Proposition 2.3.7 (Vector space properties of continuity). \\square","title":"2.3.3."},{"location":"ch02ex/#234","text":"Define an inner product and a modulus on \\mathcal{C}([0,1], \\mathbb{R}) by \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt, \\quad |f| = \\sqrt[]{\u27e8f,f\u27e9}. Do the inner product properties (IP1),(IP2),and(IP3) (see Proposition2.2.2) hold for this inner product on \\mathcal{C}([0,1], \\mathbb{R}) How much of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) ? Express the Cauchy\u2013Schwarz inequality as a relation between integrals. Proof : (IP1) \u27e8f, f\u27e9 = \\int_{0}^{1} f(t)f(t)dt \\geq \\int_{0}^{1} 0dt = 0 It achieves 0 only when f(t) = 0 . (IP2) \u27e8f,g\u27e9 = \\int_{0}^{1} f(t)g(t)dt = \\int_{0}^{1} g(t)f(t)dt = \u27e8g,f\u27e9 So IP2 holds. (IP3) First, we want to show \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh Based on the definition of integral, give a partition P on [0,1] . L(fg, P) + L(fh, P) \\leq L(fg+fh, P) \\leq U(fg+fh, P) \\leq U(fg, P) + L(fh, P) So we have \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh. Now \u27e8f,g+h\u27e9 = \\int_{0}^{1} f (g+h) = \\int_{0}^{1} fg + \\int_{0}^{1} fh = \u27e8f,g\u27e9 + \u27e8f,h\u27e9 Also \u27e8af,g\u27e9 = \\int_{0}^{1} (af)g = a(\\int_{0}^{1} fg) = a \u27e8f,g\u27e9 \\square All of the material from Section2.2 on the inner product and modulus in \\mathbb{R}^n carries over to \\mathcal{C}([0,1], \\mathbb{R}) Cauchy\u2013Schwarz inequality \\left| \\int_{0}^{1} f(t)g(t) dt \\right| \\leq \\sqrt[]{\\int_{0}^{1}f^2(t)dt} \\cdot \\sqrt[]{\\int_{0}^{1}g^2(t)dt} \\square","title":"2.3.4."},{"location":"ch02ex/#235","text":"Use the definition of convergence and the componentwise nature of nullness to prove the componentwise nature of convergence. (The argument is short.) Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p = \\text{null} \\\\ \\text{definition of convergence} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j = \\text{null} \\\\ \\text{Lemma 2.3.2 (Componentwise nature of nullness).} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\\\ \\text{definition of convergence} \\\\ \\square","title":"2.3.5"},{"location":"ch02ex/#236","text":"Use the definition of continuity and the componentwise nature of convergence to prove the componentwise nature of continuity. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square","title":"2.3.6."},{"location":"ch02ex/#237","text":"Prove the persistence of continuity under composition. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square","title":"2.3.7"},{"location":"ch02ex/#238","text":"Define f : \\mathbb{Q} \\longrightarrow \\mathbb{R} by the rule f(x) = \\begin{cases} 1 &\\text{if } x^2 < 2,\\\\ 0 &\\text{if } x^2 > 2.\\\\ \\end{cases} Solution : It is continuous. Given any p \\in \\mathbb{Q} , if p^2 < 2 , we can always find a U_{\\delta}(p) such that if q \\in U_{\\delta}(p) , then q^2 < 2 . Same thing when p^2 > 2 . \\square","title":"2.3.8."},{"location":"ch02ex/#239","text":"Which of the following functions on \\mathbb{R}^2 can be defined continuously at 0 ? (a) f(x, y) = \\begin{cases} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution \\begin{split} \\frac{x^4 - y^4}{(x^2 + y^2)^2} &= \\frac{x^2 - y^2}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\begin{split} \\frac{x^2 - y^2}{x^2 + y^2} &= \\frac{1 - m^2}{1 + m^2} \\end{split} So it cannot be defined continuously at 0 . \\square (b) g(x, y) = \\begin{cases} \\frac{x^2 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\frac{x^2 - y^3}{x^2 + y^2} &= 1 - \\frac{y^2 + y^3}{x^2 + y^2} \\end{split} When (x,y) approaches 0 with y = mx , then \\frac{y^2 + y^3}{x^2 + y^2} \\\\ = \\frac{m^2 + m^3y}{1+m^2} \\rightarrow \\frac{m^2}{1+m^2} So it cannot be defined continuously at 0 . \\square (c) h(x, y) = \\begin{cases} \\frac{x^3 - y^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : \\begin{split} \\left| \\frac{x^3 - y^3}{x^2 + y^2} \\right| &= \\left| (x-y) \\frac{x^2 + xy + y^2}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{xy}{x^2 + y^2} \\right| \\\\ & \\leq |x-y| + |x-y| \\left| \\frac{|(x,y)||(x,y)|}{|(x,y)|^2} \\right| \\\\ & = 2 |x - y| \\\\ & \\leq 2 (|x| + |y|) \\rightarrow 0 \\end{split} So it can be defined continuously at 0. \\square (d) k(x, y) = \\begin{cases} \\frac{xy^2}{x^2 + y^6} &\\text{if } (x,y) \\neq 0 \\\\ b &\\text{if } (x,y) = 0 \\\\ \\end{cases} Solution : When (x,y) approaches 0 with y = x , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^3}{x^2 + x^6} = \\frac{x}{1 + x^4} \\rightarrow 0 However, when (x,y) approaches 0 with y = \\sqrt[]{x} , then \\frac{xy^2}{x^2 + y^6} = \\frac{x^2}{x^2 + x^3} = \\frac{1}{1 + x} \\rightarrow 1 So it cannot be defined continuously at 0 . \\square","title":"2.3.9."},{"location":"ch02ex/#2310","text":"Let f(x,y) = g(xy) , where g : \\mathbb{R} \u2192 \\mathbb{R} is continuous. Is f continuous? Proof : Let h(x,y) = xy , then h(x,y) is a continuous function. Then from Proposition 2.3.8 (Persistence of continuity under composition), f = g \\circ h . So f is also continuous. \\square","title":"2.3.10."},{"location":"ch02ex/#2311","text":"Let f,g \\in \\mathcal{M}(\\mathbb{R}^n, \\mathbb{R}) such that f +g and fg are continuous. Are f and g necessarily continuous? Proof : No, it's not necessary. Consider the case when n = 1 . f(x) = \\begin{cases} 0 &\\text{if } x \\in \\mathbb{Q}\\\\ 1 &\\text{if } x \\in \\mathbb{I}\\\\ \\end{cases} Then g(x) = 1 - f(x) . Then f+g = 1, fg = 0 are continuous. But f, g are not continuous. \\square","title":"2.3.11."},{"location":"ch02ex/#24-topology-compact-sets-and-continuity","text":"","title":"2.4 Topology: Compact Sets and Continuity"},{"location":"ch02ex/#241","text":"Are the following subsets of \\mathbb{R}^n closed, bounded, compact? (a) B(0,1) : not closed, bounded, not compact. (b) \\{(x,y) \\in \\mathbb{R}^2 : y - x^2 = 0\\} : closed, not bounded, not compact. (c) \\{(x,y,z) \\in \\mathbb{R}^3: x^2 +y^2 +z^2 \u22121 = 0 \\} closed, bounded, compact. (d) \\{x : f(x) = 0_m\\} , where f \\in \\mathcal{M}(\\mathbb{R}^n, \\mathbb{R}^m) is continuous (this generalizes (b) and (c)), Solution : Consider \\{x_\u03bd\\} \\rightarrow p , since f is continuous, \\{f(x_\u03bd)\\} \\rightarrow f(p) . So f(p) = 0 . So it's closed. It can be either bounded or unbounded, so either compact or not compact. (f) \\{(x_1,...,x_n) : x_1 +\u00b7\u00b7\u00b7+x_n > 0\\} . not closed, not bounded, not compact. \\square","title":"2.4.1."},{"location":"ch02ex/#242","text":"Give a set A \u2282 \\mathbb{R}^n and limit point b of A such that b \\not\\in A . Give a set A \u2282 \\mathbb{R}^n and a point a \u2208 A such that a is not a limit point of A . Solution : Consider n = 1 , let A = (0, 1) , b = 0 . Let A = \\mathbb{N}, a = 0 . \\square","title":"2.4.2."},{"location":"ch02ex/#243","text":"Let A be a closed subset of \\mathbb{R}^n and let f \u2208 \\mathcal{M}(A, \\mathbb{R}^m) . Define the graph of f to be G(f) = \\{(a, f(a)) : a \\in A\\} a subset of \\mathbb{R}^{n+m} . Show that if f is continuous then its graph is closed. Proof : Given \\{x_\u03bd\\} \\rightarrow p , \\{x_\u03bd\\} \\in G(f) . Let x_\u03bd = (a_\u03bd, f(a_\u03bd)) . Then from Proposition 2.3.5 (Componentwise nature of convergence), \\{a_\u03bd\\} \\rightarrow p_0 , and since f is continuous \\{f(a_\u03bd)\\} \\rightarrow f(p_0) . Also since A is closed, so p_0 \\in A . So p = (p_0, f(p_0) \\in G(f) . Then G(f) is closed. \\square","title":"2.4.3."},{"location":"ch02ex/#244","text":"Prove the closed set properties: (1) the empty set \u2205 and the full space \\mathbb{R}^n are closed subsets of \\mathbb{R}^n ; (2) every intersection of closed sets is closed; (3) every finite union of closed sets is closed. Proof : (1) \u2205 does not have limit point, so \u2205 contains all it's limit point, which is nothing. \\mathbb{R}^n 's limit points are in \\mathbb{R}^n . So \\mathbb{R}^n is closed. (2) Let C = \\cap C_i , p is a limit point of C . So we can find a \\{x_\u03bd\\} \\rightarrow p , where x_\u03bd \\in C . Then x_\u03bd \\in C_i . So p \\in C_i . Then p \\in C . (3) Let C = \\cup_{i=1}^{n} C_i , p is a limit point of C . So we can find a \\{x_\u03bd\\} \\rightarrow p , where x_\u03bd \\in C . Then we can find at least one i , such that there are infinitely many x_\u03bd \\in C_i . It is a subsequence of \\{x_\u03bd\\} and also convergences to p . So p \\in C_i \\subseteq C . \\square","title":"2.4.4."},{"location":"ch02ex/#245","text":"Prove that every ball B(p,\u03b5) is bounded in \\mathbb{R}^n . Proof : Assume x \\in B(p,\u03b5) , then |x| \\leq |x-p| + |p| < \\epsilon + p \\square","title":"2.4.5."},{"location":"ch02ex/#246","text":"Show that A is a bounded subset of \\mathbb{R}^n if and only if for each j \u2208 \\{1,...,n\\} , the j th coordinates of its points form a bounded subset of \\mathbb{R} . Proof : We will need Proposition 2.2.7 (Size bounds) A \\text{ is a bounded } \\\\ \\Longleftrightarrow \\\\ |x| < R, \\forall x \\in A \\\\ \\Longleftrightarrow \\\\ |x_j| < R \\\\ \\Longleftrightarrow \\\\ j\\text{th coordinates of its points form a bounded subset of} \\mathbb{R}","title":"2.4.6."},{"location":"ch02ex/#247","text":"Show by example that a closed set need not satisfy the sequential characterization of bounded sets, and that a bounded set need not satisfy the sequential characterization of closed sets. Solutions: Consider A = [0, +\\infty) which is a closed set. Let \\{x_\u03bd\\} = \\{1, 2, 3, \\cdots\\} , then it does not have a subsequence that convergences. Consider A = (0, 1) which is bounded. Let \\{x_\u03bd\\} = \\{1, 1/2, 1/3, \\cdots\\} , we have \\{x_\u03bd\\} \\rightarrow 0 \\not\\in A . \\square","title":"2.4.7."},{"location":"ch02ex/#248","text":"Show by example that the continuous image of a closed set need not be closed, that the continuous image of a closed set need not be bounded, that the continuous image of a bounded set need not be closed, and that the continuous image of a bounded set need not be bounded. Solution : Consider f(x) = \\frac{1}{x}, x \\in [1, +\\infty) . f(C) = (0, 1] which is not closed. Consider f(x) = x, x \\in [1, +\\infty) . f(C) = [1, +\\infty) which is not bounded. Consider f(x) = x, x \\in (0, 1) . f(C) = (0, 1) which is not closed. Consider f(x) = 1/x, x \\in (0, 1) . f(C) = [1, +\\infty) which is not bounded.","title":"2.4.8."},{"location":"ch02ex/#249","text":"A subset A of \\mathbb{R}^n is called discrete if each of its points is isolated. (Recall that the term isolated was defined in this section.) Show or take for granted the (perhaps surprising at first) fact that every mapping whose domain is discrete must be continuous. Is discreteness a topological property? That is, need the continuous image of a discrete set be discrete? Solution : First we show a function f : A \\longrightarrow \\mathbb{R}^n is continuous at any point a \\in A . Give any \\{x_\u03bd\\} \\rightarrow a , since a is isolated, we must have x_\u03bd = a for \u03bd > \\Nu , then \\{f(x_\u03bd)\\} \\rightarrow f(a) , which means f is continuous. Consider f(x) = \\begin{cases} 0 &\\text{if } x = 0\\\\ 1/x &\\text{otherwise}\\\\ \\end{cases} and let A = \\mathbb{N} . So 0 is in f(A) , and 0 is not isolated. \\square","title":"2.4.9."},{"location":"ch02ex/#2410","text":"A subset A of \\mathbb{R}^n is called path-connected if for every two points x,y \u2208 A , there is a continuous mapping \u03b3 : [0,1] \\longrightarrow A such that \u03b3(0) = x and \u03b3(1) = y . (This \u03b3 is the path that connects x and y .) Draw a picture to illustrate the definition of a path-connected set. Prove that path-connectedness is a topological property. Proof : Let f : A \\longrightarrow \\mathbb{R}^m is a continuous map. Given x, y \\in A , we want to show f(x), f(y) is also path-connected. From Proposition 2.3.8 (Persistence of continuity under composition), f \\circ \u03b3 : [0,1] \\longrightarrow \\mathbb{R}^m is a continuous map and (f \\circ \u03b3)(0) = f(x) \\\\ (f \\circ \u03b3)(1) = f(y) \\\\ So f(x), f(y) is also path-connected. \\square","title":"2.4.10"},{"location":"ch02notes/","text":"Chapter 2 Euclidean Space 2.1 Algebra: Vectors Theorem 2.1.1 (Vector space axioms). (A1) Addition is associative (A2) 0 is an additive identity (A3) Existence of additive inverses (A4) Addition is commutative (M1) Scalar multiplication is associative (M2) 1 is a multiplicative identity (D1) Scalar multiplication distributes over scalar addition (D2) Scalar multiplication distributes over vector addition For n > 1 , \\mathbb{R}^n is not endowed with vector-by-vector multiplication. If the vector space axioms are satisfied with V and F replacing \\mathbb{R}^n and \\mathbb{R} then we say that V is a vector space over F . We can use intrinsic vector algebra to prove a result from Euclidean geometry, that the three medians of a triangle intersect. Let p = \\frac{x + y + z}{3} Rewrite it as p = x + \\frac{2}{3}(\\frac{y+z}{2} - x) Note that \\frac{y+z}{2} is the middle point of yz . Then \\frac{y+z}{2} - x is the median from x to yz , so p is on the median. Since x,y,z are symmetric, then p is on all 3 medians. \\square The standard basis of \\mathbb{R}^n is the set of vectors e_1 = (1, 0, \\cdots 0) \\\\ e_2 = (0, 1, \\cdots 0) \\\\ \\cdots \\\\ e_n = (0, 0, \\cdots 1) \\\\ So \\tag{2.1} x = \\sum_{i = 1}^{n}x_i e_i Definition 2.1.2 (Basis). A set of vectors \\{f_i\\} is a basis of \\mathbb{R}^n if every x \u2208 \\mathbb{R}^n is uniquely expressible as a linear combination of the f_i . \\square 2.2 Geometry: Length and Angle Definition 2.2.1 (Inner product). The inner product is a function from pairs of vectors to scalars, \\left\\langle , \\right\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R} defined by the formula \\left\\langle (x_1, \\cdots, x_n) , (y_1, \\cdots, y_n) \\right\\rangle = \\sum_{i = 1}^{n} x_i y_i Proposition 2.2.2 (Inner product properties). (IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = \u27e8x,y\u27e9 + \u27e8x',y\u27e9, \\quad \u27e8ax,y\u27e9 = a \u27e8x,y\u27e9 \\\\ \u27e8x,y+y'\u27e9 = \u27e8x,y\u27e9 + \u27e8x,y'\u27e9, \\quad \u27e8x,by\u27e9 = b \u27e8x,y\u27e9 for all a,b \u2208 \\mathbb{R} , x,x',y,y' \u2208 \\mathbb{R}^n . \\square Like the vector space axioms, the inner product properties are phrased intrinsically, although they need to be proved using coordinates. As mentioned in the previous section,intrinsic methods are neater and more conceptual than using coordinates. More importantly: The rest of the results of this section are proved by reference to the inner product properties, with no further reference to the inner product formula. Definition 2.2.3 (Modulus). The modulus (or absolute value) of a vector x \u2208 \\mathbb{R}^n is defined as |x| = \\sqrt[]{\u27e8x,x\u27e9}. Proposition 2.2.4 (Modulus properties). (Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n . Theorem 2.2.5 (Cauchy\u2013Schwarz inequality). For all x,y \u2208 \\mathbb{R}^n , |\u27e8x,y\u27e9| \\leq |x| |y| with equality if and only if one of x, y is a scalar multiple of the other. Note that the absolute value signs mean di\ufb00erent things on each side of the Cauchy\u2013Schwarz inequality. On the left side, the quantities x and y are vectors, their inner product \u27e8x,y\u27e9 is a scalar, and |\u27e8x,y\u27e9| is its scalar absolute value, while on the right side, |x| and |y| are the scalar absolute values of vectors, and |x||y| is their product. That is, the Cauchy\u2013Schwarz inequality says: The size of the product is at most the product of the sizes. The computation draws on the minutiae of the formulas for the inner product and the modulus, rather than using their properties. It is uninformative, making the Cauchy\u2013Schwarz inequality look like a low-level accident. It suggests that larger-scale mathematics is just a matter of bigger and bigger formulas. To prove the inequality in a way that is enlightening and general, we should work intrinsically, keeping the scalars \u27e8x,y\u27e9 and |x| and |y| notated in their concise forms, and we should use properties, not formulas. Proof . The result is clear when x = 0 , so assume x \\neq 0 . For every a \u2208 \\mathbb{R}^n , \\begin{split} 0 &\\leq \u27e8ax-y,ax-y\u27e9 \\quad \\\\ & \\quad \\text{by positive definiteness} \\\\ &= a\u27e8x,ax-y\u27e9 - \u27e8y,ax-y\u27e9 \\\\ & \\quad \\text{by linearity in the first variable} \\\\ &= a^2 \u27e8x,x\u27e9 - a \u27e8x,y\u27e9 - a \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by linearity in the second variable} \\\\ &= a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 \\\\ & \\quad \\text{by symmetry, definition of modulus.} \\end{split} Define f(a) = a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 Since f(a) is always nonnegative, so f has at most one root. Thus by the quadratic formula its discriminant is nonpositive, 4\u27e8x,y\u27e9^2 \u22124|x|^2|y|^2 \u2264 0 So |\u27e8x,y\u27e9| \\leq |x| |y| Equality holds exactly when the quadratic polynomial f(a) = |ax\u2212 y|^2 has a root a, i.e., exactly when y= ax for some a \u2208 \\mathbb{R}^n . \\square Theorem 2.2.6 (Triangle inequality). For all x,y \u2208 \\mathbb{R}^n , |x+y| \\leq |x| + |y|, with equality if and only if one of x, y is a nonnegative scalar multiple of the other. Proof : \\begin{split} \u27e8x+y,x+y\u27e9 \\\\ &= \u27e8x,x+y\u27e9 + \u27e8y,x+y\u27e9 \\\\ &= \u27e8x,x\u27e9 + \u27e8x,y\u27e9 + \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ &= \u27e8x,x\u27e9 + 2\u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ &\\leq \u27e8x,x\u27e9 + 2|x||y| + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by Cauchy\u2013Schwarz} \\\\ &= (|x| + |y|)^2 \\end{split} Equality holds exactly when \u27e8x,y\u27e9= |x||y| , or equivalently when |\u27e8x,y\u27e9|= |x||y| and \u27e8x,y\u27e9 \u2265 0 . These hold when one of x, y is a scalar multiple of the other and the scalar is nonnegative. \\square While the Cauchy\u2013Schwarz inequality says that the size of the product is at most the product of the sizes, the triangle inequality says: The size of the sum is at most the sum of the sizes. Proposition 2.2.7 (Size bounds). For every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{i=1}^{n} |x_i|. Distance Definition The modulus gives rise to a distance function on Rn that behaves as distance should. Define d: \\mathbb{R}^n \\times \\mathbb{R}^n \\xrightarrow{} \\mathbb{R} by d(x,y) = |y-x| Theorem 2.2.8 (Distance properties). (D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n . Angle Definition If x and y are nonzero vectors in \\mathbb{R}^n , define their angle \u03b8_{x,y} by the condition \\tag{2.2} \\cos \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x||y|}, \\quad 0 \\leq \u03b8_{x,y} \\leq \u03c0. In particular, two nonzero vectors x and y are orthogonal when \u27e8x,y\u27e9 = 0 . Thus 0 orthogonal to all vectors. three altitudes must meet We have q-y \\perp x \\quad \\text{and} \\quad q-x \\perp y And we want to show \\left\\{ \\begin{array}{lr} \u27e8q-y,x\u27e9 = 0 \\\\ \u27e8q-x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 We have \\left\\{ \\begin{array}{lr} \u27e8q,x\u27e9 - \u27e8y,x\u27e9 = 0 \\\\ \u27e8q,y\u27e9 - \u27e8x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 So we have \u27e8q,x\u27e9 = \u27e8y,x\u27e9 = \u27e8x,y\u27e9 = \u27e8q,y\u27e9 \\\\ \\Longrightarrow \\\\ \u27e8q,x-y\u27e9 = 0 \\square 2.3 Analysis: Continuous Mappings Mapping from \\mathbb{R}^n to \\mathbb{R}^m A mapping from \\mathbb{R}^n to \\mathbb{R}^m is some rule that assigns to each point x in \\mathbb{R}^n a point in \\mathbb{R}^m . Generally, mappings will be denoted by letters such as f, g, h . Mappings as a vector space For a given dimension n , a given set A \u2282 \\mathbb{R}^n , and a second dimension m , let \\mathcal{M}(A,\\mathbb{R}^m) denote the set of all mappings f : A \\rightarrow \\mathbb{R}^m . This set forms a vector space over \\mathbb{R} (whose points are functions) under the operations + : \\mathcal{M}(A,\\mathbb{R}^m) \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (f +g)(x) = f(x)+g(x) \\quad\\text{for all } x \u2208 A, and \\cdot : \\mathbb{R} \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (a \\cdot f )(x) = a \\cdot f(x) \\quad\\text{for all } x \u2208 A. Sequence in \\mathbb{R}^n Let A be a subset of \\mathbb{R}^n . A sequence in A is an infinite list of vectors \\{x_1,x_2,x_3,...\\} in A , often written \\{x_\u03bd\\} . Since a vector has n entries, each vector x_\u03bd in the sequence takes the form (x_{1,\u03bd},...,x_{n,\u03bd}) . Definition 2.3.1 (Null Sequence). The sequence (x_{\\nu }) in \\mathbb{R}^n is null if for every \u03b5 > 0 there exists some \u03bd_0 such that \\text{if } \u03bd > \u03bd_0 \\text{ then } |x_v| < \\epsilon. That is, a sequence is null if for every \u03b5 > 0 , all but finitely many terms of the sequence lie within distance \u03b5 of 0_n . If \\{x_\u03bd\\} is a null sequence and |y_\u03bd| \\leq |x_\u03bd| or all \u03bd then also \\{y_\u03bd\\} is null. \\{x_\u03bd\\} and \\{y_\u03bd\\} are null sequence, then |x_\u03bd + y_\u03bd| \\leq |x_\u03bd| + |y_\u03bd| , so \\{x_\u03bd + y_\u03bd\\} is null. \\{c x_\u03bd\\} is null seq, since |cx_\u03bd| = |c||x_\u03bd| So the set of null sequences in \\mathbb{R}^n forms a vector space. A vector sequence \\{x_\u03bd\\} is null if and only if the scalar sequence \\{|x_\u03bd|\\} is null. Lemma 2.3.2 (Componentwise nature of nullness). The vector sequence \\{(x_{1,\u03bd}, \\cdots, x_{n,\u03bd})\\} is null if and only if each of its component scalar sequences \\{x_{j,\u03bd}\\} (j \\in \\{1, \\cdots, n\\}) is null. Proof Use |x_{j,\u03bd}| \\leq |x_\u03bd| \\leq \\sum_{j=1}^{n} |x_{j,\u03bd}| \\square Definition 2.3.3 (Sequence convergence, sequence limit). Let A be a subset of \\mathbb{R}^n . Consider a sequence \\{x_\u03bd\\} in A and a point p \u2208 \\mathbb{R}^n . The sequence \\{x_\u03bd\\} converges to p (or has limit p ), written \\{x_\u03bd\\} \u2192 p , if the sequence \\{x_\u03bd\u2212p\\} is null. When the limit p is a point of A , the sequence \\{x_\u03bd\\} converges in A . A sequence can only converges to one limit. Proposition 2.3.4 (Linearity of convergence). Let \\{x_\u03bd\\} be a sequence in \\mathbb{R}^n converging to p , let \\{y_\u03bd\\} be a sequence in \\mathbb{R}^n converging to q , and let c be a scalar. Then the sequence \\{x_\u03bd +y_\u03bd\\} converges to p+q , and the sequence \\{c x_\u03bd\\} converges to c_p . Proof : \\begin{split} |(x_\u03bd + y_\u03bd) - (p + q)| &= |(x_\u03bd - p) + (y_\u03bd-q)| \\\\ &\\leq |x_\u03bd - p| + |y_\u03bd - q| \\end{split} So (x_\u03bd + y_\u03bd) - (p + q) is null. Then x_\u03bd + y_\u03bd \u2192 p+q . |c x_\u03bd - cp| = |c (x_\u03bd - p)| \\leq |c| |x_\u03bd - p| So c x_\u03bd - cp is null. Then c x_\u03bd \u2192 cp \\square Proposition 2.3.5 (Componentwise nature of convergence). The vector sequence \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} converges to the vector (p_1, \\cdots, p_n) if and only if each component scalar sequence \\{x_{j, \u03bd}\\} (j = 1, \\cdots, n) . converges to the scalar p_j . Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\square Definition 2.3.6 (Continuity). Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a mapping, and let p be a point of A . Then f is continuous at p if for every sequence \\{x_\u03bd\\} in A converging to p , the sequence f(x_\u03bd) converges to f(p) . The mapping f is continuous on A (or just continuous when A is clearly established) if it is continuous at each point p \u2208 A . Modulus function is continuous || : \\mathbb{R}^n \\longrightarrow \\mathbb{R} Proof : Assume \\{x_\u03bd\\} \\rightarrow p , then from exercise 2.2.7 ||x_\u03bd| - |p|| \\leq | x_\u03bd - p | Then |x_\u03bd| - |p| is null, i.e. \\{f(x_\u03bd)\\} \\rightarrow f(p) . So || is continuous. The inner product is continuous Given a \\in \\mathbb{R}^n , define T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad T(x) = \u27e8a,x\u27e9 T is continuous. From Cauchy-Schwarz inequality |\u27e8a,x\u27e9| \\leq |a||x| Given p \\in \\mathbb{R}^n |T(x_\u03bd) - T(p)| = |\u27e8a,x_v\u27e9 - \u27e8a,p\u27e9| = |\u27e8a,x_\u03bd - p\u27e9| \\leq |a| | x_\u03bd - p | Since we know |x_\u03bd - p| \\rightarrow 0 , we have |T(x_\u03bd) - T(p)| \\rightarrow 0 . \\square jth coordinate function map Consider the following mapping \\pi_j : \\mathbb{R}^n \\longrightarrow \\mathbb{R}, \\quad \\pi_j (x_1, \\cdots, x_n) = x_j Since \\pi_j(x) = \u27e8e_j,x\u27e9 , this mapping is continuous. Proposition 2.3.7 (Vector space properties of continuity). Let A be a subset of \\mathbb{R}^n , let f,g : A \\longrightarrow \\mathbb{R}^m be continuous mappings, and let c \\in \\mathbb{R} . Then the sum and the scalar multiple mappings f+g,cf : A \\longrightarrow \\mathbb{R}^m are continuous. Thus the set of continuous mappings from A to \\mathbb{R}^m forms a vector subspace of \\mathcal{M}(A, \\mathbb{R}^m) . Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . Then \\{f(x_\u03bd)\\} \\rightarrow f(p), \\{g(x_\u03bd)\\} \\rightarrow g(p) . Then from Proposition 2.3.4 (Linearity of convergence), \\{f(x_\u03bd) + g(x_\u03bd)\\} \\rightarrow f(p) + g(p) , i.e. \\{(f+g)(x_\u03bd)\\} \\rightarrow (f+g)(p) . So f+g is continuous. For the same reason cf is continuous. \\square Proposition 2.3.8 (Persistence of continuity under composition). Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let B be a superset of f(A) in \\mathbb{R}^m , and let g : B \\longrightarrow \\mathbb{R}^l be a continuous mapping. Then the composition mapping g \\circ f : A \\longrightarrow \\mathbb{R}^l is continuous. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square Theorem 2.3.9 (Componentwise nature of continuity). Let A \u2282 \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m have component functions f_1,...,f_m , and let p be a point in A . Then f \\text{ is continuous at } p \\Longleftrightarrow \\text{each } f_i \\text{ is continuous at } p. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square Some examples Consider f(x, y) = \\begin{cases} \\frac{2xy}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Can the constant b be specified to make f continuous at 0 ? Take a sequence \\{(x_\u03bd, y_\u03bd)\\} \\rightarrow 0 along the line y= mx . f(x_\u03bd, y_\u03bd) = \\\\ \\frac{2x_\u03bd y_\u03bd}{x_\u03bd^2 + y_\u03bd^2} = \\\\ \\frac{2m}{1 + m^2} hence f cannot be made continuous at 0 . \\square Now consider g(x, y) = \\begin{cases} \\frac{x^2y}{x^4 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} If (x,y) approaches to (0,0) with y = x , then g(x,y) = \\frac{x^3}{x^4 + x^2} = \\frac{x}{x^2 + 1} \\rightarrow 0 If (x,y) approaches to (0,0) with y = x^2 , then g(x,y) = \\frac{x^4}{x^4 + x^4} = \\frac{1}{2} hence f cannot be made continuous at 0 . \\square The size bounds to prove continuity Consider h(x, y) = \\begin{cases} \\frac{x^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Note from 2.2.7, we have |x| \\leq |(x,y)| , so \\frac{x^3}{x^2 + y^2} \\leq \\frac{|(x,y)|^3}{|(x,y)|^2} = |(x,y)| \\rightarrow 0 So setting b = 0 makes it continuous at 0 . Summary of the 3 examples The straight line test can prove that a limit does not exist, or it can determine the only candidate for the value of the limit, but it cannot prove that the candidate value is the limit. When the straight line test determines a candidate value of the limit, approaching along a curve can further support the candidate, or it can prove that the limit does not exist by determining a di\ufb00erent candidate as well. The size bounds can prove that a limit does exist, but they can only suggest that a limit does not exist. Proposition 2.3.10 (Persistence of inequality). Let A be a subset of \\mathbb{R}^n and let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let p be a point of A , let b be a point of \\mathbb{R}^m , and suppose that f(p) \\neq b . Then there exists some \\epsilon > 0 , such that \\text{for all } x \\in A \\text{such that } |x-p| < \\epsilon, f(x) \\neq b. Proof : Assume otherwise, then for \\nu \\in \\mathbb{N} , we can find x_\u03bd , such that | x_\u03bd - p | < 1/\u03bd and f(x_\u03bd) = b . Since f is continuous, then f(p) = b . We have a contradiction. \\square 2.4 Topology: Compact Sets and Continuity Definition 2.4.1 (\u03b5-ball). For every point p \u2208 \\mathbb{R}^n and every positive real number \u03b5 > 0 , the \u03b5 -ball centered at p is the set B(p, \u03b5) = \\{x \\in \\mathbb{R}^n : |x-p| \\leq \u03b5\\} Definition 2.4.2 (Limit point). Let A be a subset of \\mathbb{R}^n , and let p be a point of \\mathbb{R}^n . The point p is a limit point of A if every \u03b5 -ball centered at p contains some point x \u2208 A such that x \\neq p . Isolated point Definition 1: A point in A that is not a limit point of A . Definition 2: Equivalently, p is an isolated point of A if p \u2208 A and there exists some \u03b5 > 0 such that B(p,\u03b5)\u2229A= \\{p\\} . Lemma 2.4.3 (Sequential characterization of limit points). Let A be a subset of Rn, and let p be a point of \\mathbb{R}^n . Then p is the limit of a sequence \\{x_\u03bd\\} in A with each x_\u03bd \\neq p if and only if p is a limit point of A . Proof : \\Rightarrow Given B(p, \u03b5) , since \\{x_\u03bd\\} \\rightarrow p , we can find \u03bd_0 , such that \u03bd > \u03bd_0 , |x_\u03bd - p| < \u03b5 , i.e. x_\u03bd \\in B(p, \u03b5) . Since x_\u03bd \\neq p , then p is a limit point of A . \\Leftarrow Assume p is a limit point of A , let x_1 \\in A . Let \u03b5_1 = |x_1 - p| , and \u03b5_2 = \u03b5_1 / 2 . We can find p \\neq x_2 \\in B(p, \u03b5_2) . This process can continue infinitely. Then we find a \\{x_\u03bd\\} \\rightarrow p . \\square Definition 2.4.4 (Closed set). A subset A of \\mathbb{R}^n is closed if it contains all of its limit points. Proposition 2.4.5 (Sequential characterization of closed sets). Let A be a subset of \\mathbb{R}^n . Then A is closed if and only if every sequence in A that converges in \\mathbb{R}^n in fact converges in A . Proof : \\Rightarrow Assume \\{x_\u03bd\\} \\rightarrow p and x_\u03bd \\in A , if p = x_\u03bd for some \u03bd , then p \\in A . If p \\neq x_\u03bd , then p is a limit point of A . Since A is closed, so p \\in A . \\square \\Leftarrow Assume p is a limit point of A , then from Lemma 2.4.3 \\Rightarrow , we can find \\{x_\u03bd\\} \\rightarrow p and x_\u03bd \\in A . Then p \\in A , so p is closed. \\square Definition 2.4.6 (Bounded set). A set A in \\mathbb{R}^n is bounded if A \u2282 B(0,R) for some R > 0 . Proposition 2.4.7 (Convergence implies boundedness). If the sequence \\{x_\u03bd\\} converges in \\mathbb{R}^n then it is bounded. Definition 2.4.8 (Subsequence). A subsequence of the sequence \\{x_\u03bd\\} is a sequence consisting of some (possibly all) of the original terms, in ascending order of indices. Lemma 2.4.9 (Persistence of convergence). Let \\{x_\u03bd\\} converge to p . Then every subsequence \\{x_{\u03bd_k}\\} also converges to p . Theorem 2.4.10 (Bolzano\u2013Weierstrass property in R ). Let A be a bounded subset of \\mathbb{R} . Then every sequence in A has a convergent subsequence. Proof : This is just a summary. Define the max-point if it is at least as big as all later terms, i.e., x_\u03bd \u2265 x_\u03bc for all \u03bc > \u03bd . If there are infinitely many max-points in \\{x_\u03bd\\} then these form a decreasing sequence. If there are only finitely many max-points then \\{x_\u03bd\\} has an increasing sequence starting after the last max-point. \\square Theorem 2.4.11 (Bolzano\u2013Weierstrass property in \\mathbb{R}^n : sequential characterization of bounded sets). Let A be a subset of \\mathbb{R}^n . Then A is bounded if and only if every sequence in A has a subsequence that converges in \\mathbb{R}^n . Proof : \\Longrightarrow Use 2.4.10 on every index. \\Longleftarrow Proof by contradiction. \\square Note how the sequential characterizations in Proposition 2.4.5 > and in the Bolzano\u2013Weierstrass property complement each other. The proposition characterizes every closed set in \\mathbb{R}^n by the fact that if a sequence converges in the ambient space then it converges in the set. The Bolzano\u2013Weierstrass property characterizes every bounded set in \\mathbb{R}^n by the fact that every sequence in the set has a subsequence that converges in the ambient space but not necessarily in the set. Both the sequential characterization of a closed set and the sequential characterization of a bounded set refer to the ambient space \\mathbb{R}^n in which the set lies. Definition 2.4.12 (Compact set). A subset K of \\mathbb{R}^n is compact if it is closed and bounded. Theorem 2.4.13 (Sequential characterization of compact sets). Let K be a subset of \\mathbb{R}^n . Then K is compact if and only if every sequence in K has a subsequence that converges in K . Proof : \\Longrightarrow Since its bounded we can use the Bolzano\u2013Weierstrass property. And since it's closed, it converges in K . \\Longleftarrow Just use the \\Longleftarrow of 2.4.5 and 2.4.11. \\square By contrast to the sequential characterizations of a closed set and of a bounded set, the sequential characterization of a compact set K makes no reference to the ambient space Rn in which K lies. A set\u2019s property of being compact is innate in a way that a set\u2019s property of being closed or of being bounded is not. Theorem 2.4.14 (The continuous image of a compact set is compact). Let K be a compact subset of \\mathbb{R}^n and let f : K \\longrightarrow \\mathbb{R}^m be continuous. Then f(K) , the image set of K under f , is a compact subset of \\mathbb{R}^m . Proof : Let \\{y_\u03bd\\} \\in f(K) , we can find \\{x_\u03bd\\} \\in K , such that f(x_\u03bd) = y_\u03bd . Since K is compact, then we can find subsuquence \\{x_{\u03bd_k}\\} \\longrightarrow p \\in K . Since f is continuous, then \\{f(x_{\u03bd_k})\\} \\longrightarrow f(p) \\in f(K) . And \\{f(x_{\u03bd_k})\\} is subsequence of f(x_\u03bd) . So f(K) is also compact. \\square Theorem 2.4.15 (Extreme value theorem). Let K be a nonempty compact subset of \\mathbb{R}^n and let the function f : K \\longrightarrow R be continuous. Then f takes a minimum and a maximum value on K . Proof : f(K) is compact in \\mathbb{R} , so it has a greatest lower bound a and a least upper bound b by the completeness of the real number system. If a is not a limit point of f(K) and a \\not\\in f(K) , then we can find U_{\\delta}(a) such that U_{\\delta}(a) \\cap f(K) = \\emptyset . That means, a + \\delta /2 is another lower bound. We got a contradiction. So a is either an isolated point or a limit point of f(K) . \\square Topological property A topological property of sets is a property that is preserved under continuity. Theorem 2.4.14 says that compactness is a topological property. Neither the property of being closed nor the property of being bounded is in itself topological. That is, the continuous image of a closed set need not be closed, and the continuous image of a bounded set need not be bounded; for that matter, the continuous image of a closed set need not be bounded, and the continuous image of a bounded set need not be closed (Exercise 2.4.8). The nomenclature continuous image in the slogan-title of Theorem 2.4.14 and in the previous paragraph is, strictly speaking, inaccurate: the image of a mapping is a set, and the notion of a set being continuous doesn\u2019t even make sense according to our grammar. As stated correctly in the body of the theorem, continuous image is short for image under a continuous mapping. \\square","title":"Chapter 02 Notes"},{"location":"ch02notes/#chapter-2-euclidean-space","text":"","title":"Chapter 2 Euclidean Space"},{"location":"ch02notes/#21-algebra-vectors","text":"","title":"2.1 Algebra: Vectors"},{"location":"ch02notes/#theorem-211-vector-space-axioms","text":"(A1) Addition is associative (A2) 0 is an additive identity (A3) Existence of additive inverses (A4) Addition is commutative (M1) Scalar multiplication is associative (M2) 1 is a multiplicative identity (D1) Scalar multiplication distributes over scalar addition (D2) Scalar multiplication distributes over vector addition For n > 1 , \\mathbb{R}^n is not endowed with vector-by-vector multiplication. If the vector space axioms are satisfied with V and F replacing \\mathbb{R}^n and \\mathbb{R} then we say that V is a vector space over F . We can use intrinsic vector algebra to prove a result from Euclidean geometry, that the three medians of a triangle intersect. Let p = \\frac{x + y + z}{3} Rewrite it as p = x + \\frac{2}{3}(\\frac{y+z}{2} - x) Note that \\frac{y+z}{2} is the middle point of yz . Then \\frac{y+z}{2} - x is the median from x to yz , so p is on the median. Since x,y,z are symmetric, then p is on all 3 medians. \\square The standard basis of \\mathbb{R}^n is the set of vectors e_1 = (1, 0, \\cdots 0) \\\\ e_2 = (0, 1, \\cdots 0) \\\\ \\cdots \\\\ e_n = (0, 0, \\cdots 1) \\\\ So \\tag{2.1} x = \\sum_{i = 1}^{n}x_i e_i","title":"Theorem 2.1.1 (Vector space axioms)."},{"location":"ch02notes/#definition-212-basis","text":"A set of vectors \\{f_i\\} is a basis of \\mathbb{R}^n if every x \u2208 \\mathbb{R}^n is uniquely expressible as a linear combination of the f_i . \\square","title":"Definition 2.1.2 (Basis)."},{"location":"ch02notes/#22-geometry-length-and-angle","text":"","title":"2.2 Geometry: Length and Angle"},{"location":"ch02notes/#definition-221-inner-product","text":"The inner product is a function from pairs of vectors to scalars, \\left\\langle , \\right\\rangle : \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R} defined by the formula \\left\\langle (x_1, \\cdots, x_n) , (y_1, \\cdots, y_n) \\right\\rangle = \\sum_{i = 1}^{n} x_i y_i","title":"Definition 2.2.1 (Inner product)."},{"location":"ch02notes/#proposition-222-inner-product-properties","text":"(IP1) The inner product is positive definite: \u27e8x,x\u27e9 \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (IP2) The inner product is symmetric: \u27e8x,y\u27e9= \u27e8y,x\u27e9 for all x,y \u2208 \\mathbb{R}^n (IP3) The inner product is bilinear: \u27e8x+x',y\u27e9 = \u27e8x,y\u27e9 + \u27e8x',y\u27e9, \\quad \u27e8ax,y\u27e9 = a \u27e8x,y\u27e9 \\\\ \u27e8x,y+y'\u27e9 = \u27e8x,y\u27e9 + \u27e8x,y'\u27e9, \\quad \u27e8x,by\u27e9 = b \u27e8x,y\u27e9 for all a,b \u2208 \\mathbb{R} , x,x',y,y' \u2208 \\mathbb{R}^n . \\square Like the vector space axioms, the inner product properties are phrased intrinsically, although they need to be proved using coordinates. As mentioned in the previous section,intrinsic methods are neater and more conceptual than using coordinates. More importantly: The rest of the results of this section are proved by reference to the inner product properties, with no further reference to the inner product formula.","title":"Proposition 2.2.2 (Inner product properties)."},{"location":"ch02notes/#definition-223-modulus","text":"The modulus (or absolute value) of a vector x \u2208 \\mathbb{R}^n is defined as |x| = \\sqrt[]{\u27e8x,x\u27e9}.","title":"Definition 2.2.3 (Modulus)."},{"location":"ch02notes/#proposition-224-modulus-properties","text":"(Mod1) The modulus is positive: |x| \u2265 0 for all x \u2208 \\mathbb{R}^n , with equality if and only if x = 0 . (Mod2) The modulus is absolute-homogeneous: |ax|= |a||x| for all a \u2208 R and x \u2208 \\mathbb{R}^n .","title":"Proposition 2.2.4 (Modulus properties)."},{"location":"ch02notes/#theorem-225-cauchyschwarz-inequality","text":"For all x,y \u2208 \\mathbb{R}^n , |\u27e8x,y\u27e9| \\leq |x| |y| with equality if and only if one of x, y is a scalar multiple of the other. Note that the absolute value signs mean di\ufb00erent things on each side of the Cauchy\u2013Schwarz inequality. On the left side, the quantities x and y are vectors, their inner product \u27e8x,y\u27e9 is a scalar, and |\u27e8x,y\u27e9| is its scalar absolute value, while on the right side, |x| and |y| are the scalar absolute values of vectors, and |x||y| is their product. That is, the Cauchy\u2013Schwarz inequality says: The size of the product is at most the product of the sizes. The computation draws on the minutiae of the formulas for the inner product and the modulus, rather than using their properties. It is uninformative, making the Cauchy\u2013Schwarz inequality look like a low-level accident. It suggests that larger-scale mathematics is just a matter of bigger and bigger formulas. To prove the inequality in a way that is enlightening and general, we should work intrinsically, keeping the scalars \u27e8x,y\u27e9 and |x| and |y| notated in their concise forms, and we should use properties, not formulas. Proof . The result is clear when x = 0 , so assume x \\neq 0 . For every a \u2208 \\mathbb{R}^n , \\begin{split} 0 &\\leq \u27e8ax-y,ax-y\u27e9 \\quad \\\\ & \\quad \\text{by positive definiteness} \\\\ &= a\u27e8x,ax-y\u27e9 - \u27e8y,ax-y\u27e9 \\\\ & \\quad \\text{by linearity in the first variable} \\\\ &= a^2 \u27e8x,x\u27e9 - a \u27e8x,y\u27e9 - a \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by linearity in the second variable} \\\\ &= a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 \\\\ & \\quad \\text{by symmetry, definition of modulus.} \\end{split} Define f(a) = a^2 |x|^2 - 2a \u27e8x,y\u27e9 + |y|^2 Since f(a) is always nonnegative, so f has at most one root. Thus by the quadratic formula its discriminant is nonpositive, 4\u27e8x,y\u27e9^2 \u22124|x|^2|y|^2 \u2264 0 So |\u27e8x,y\u27e9| \\leq |x| |y| Equality holds exactly when the quadratic polynomial f(a) = |ax\u2212 y|^2 has a root a, i.e., exactly when y= ax for some a \u2208 \\mathbb{R}^n . \\square","title":"Theorem 2.2.5 (Cauchy\u2013Schwarz inequality)."},{"location":"ch02notes/#theorem-226-triangle-inequality","text":"For all x,y \u2208 \\mathbb{R}^n , |x+y| \\leq |x| + |y|, with equality if and only if one of x, y is a nonnegative scalar multiple of the other. Proof : \\begin{split} \u27e8x+y,x+y\u27e9 \\\\ &= \u27e8x,x+y\u27e9 + \u27e8y,x+y\u27e9 \\\\ &= \u27e8x,x\u27e9 + \u27e8x,y\u27e9 + \u27e8y,x\u27e9 + \u27e8y,y\u27e9 \\\\ &= \u27e8x,x\u27e9 + 2\u27e8x,y\u27e9 + \u27e8y,y\u27e9 \\\\ &\\leq \u27e8x,x\u27e9 + 2|x||y| + \u27e8y,y\u27e9 \\\\ & \\quad \\text{by Cauchy\u2013Schwarz} \\\\ &= (|x| + |y|)^2 \\end{split} Equality holds exactly when \u27e8x,y\u27e9= |x||y| , or equivalently when |\u27e8x,y\u27e9|= |x||y| and \u27e8x,y\u27e9 \u2265 0 . These hold when one of x, y is a scalar multiple of the other and the scalar is nonnegative. \\square While the Cauchy\u2013Schwarz inequality says that the size of the product is at most the product of the sizes, the triangle inequality says: The size of the sum is at most the sum of the sizes.","title":"Theorem 2.2.6 (Triangle inequality)."},{"location":"ch02notes/#proposition-227-size-bounds","text":"For every j \u2208 \\{1,...,n\\} , |x_j| \\leq |x| \\leq \\sum_{i=1}^{n} |x_i|.","title":"Proposition 2.2.7 (Size bounds)."},{"location":"ch02notes/#distance-definition","text":"The modulus gives rise to a distance function on Rn that behaves as distance should. Define d: \\mathbb{R}^n \\times \\mathbb{R}^n \\xrightarrow{} \\mathbb{R} by d(x,y) = |y-x|","title":"Distance Definition"},{"location":"ch02notes/#theorem-228-distance-properties","text":"(D1) Distance is positive: d(x,y) \u2265 0 for all x,y \u2208 \\mathbb{R}^n , and d(x,y) = 0 if and only if x = y . (D2) Distance is symmetric: d(x,y) = d(y,x) for all x,y \u2208 \\mathbb{R}^n . (D3) Triangle inequality: d(x,z) \u2264 d(x,y)+d(y,z) for all x,y,z \u2208 \\mathbb{R}^n .","title":"Theorem 2.2.8 (Distance properties)."},{"location":"ch02notes/#angle-definition","text":"If x and y are nonzero vectors in \\mathbb{R}^n , define their angle \u03b8_{x,y} by the condition \\tag{2.2} \\cos \u03b8_{x,y} = \\frac{\u27e8x,y\u27e9}{|x||y|}, \\quad 0 \\leq \u03b8_{x,y} \\leq \u03c0. In particular, two nonzero vectors x and y are orthogonal when \u27e8x,y\u27e9 = 0 . Thus 0 orthogonal to all vectors.","title":"Angle Definition"},{"location":"ch02notes/#three-altitudes-must-meet","text":"We have q-y \\perp x \\quad \\text{and} \\quad q-x \\perp y And we want to show \\left\\{ \\begin{array}{lr} \u27e8q-y,x\u27e9 = 0 \\\\ \u27e8q-x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 We have \\left\\{ \\begin{array}{lr} \u27e8q,x\u27e9 - \u27e8y,x\u27e9 = 0 \\\\ \u27e8q,y\u27e9 - \u27e8x,y\u27e9 = 0 \\\\ \\end{array} \\right\\} \\Longrightarrow \u27e8q,x-y\u27e9 = 0 So we have \u27e8q,x\u27e9 = \u27e8y,x\u27e9 = \u27e8x,y\u27e9 = \u27e8q,y\u27e9 \\\\ \\Longrightarrow \\\\ \u27e8q,x-y\u27e9 = 0 \\square","title":"three altitudes must meet"},{"location":"ch02notes/#23-analysis-continuous-mappings","text":"","title":"2.3 Analysis: Continuous Mappings"},{"location":"ch02notes/#mapping-from-mathbbrn-to-mathbbrm","text":"A mapping from \\mathbb{R}^n to \\mathbb{R}^m is some rule that assigns to each point x in \\mathbb{R}^n a point in \\mathbb{R}^m . Generally, mappings will be denoted by letters such as f, g, h .","title":"Mapping from \\mathbb{R}^n to \\mathbb{R}^m"},{"location":"ch02notes/#mappings-as-a-vector-space","text":"For a given dimension n , a given set A \u2282 \\mathbb{R}^n , and a second dimension m , let \\mathcal{M}(A,\\mathbb{R}^m) denote the set of all mappings f : A \\rightarrow \\mathbb{R}^m . This set forms a vector space over \\mathbb{R} (whose points are functions) under the operations + : \\mathcal{M}(A,\\mathbb{R}^m) \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (f +g)(x) = f(x)+g(x) \\quad\\text{for all } x \u2208 A, and \\cdot : \\mathbb{R} \\times \\mathcal{M}(A,\\mathbb{R}^m) \\longrightarrow \\mathcal{M}(A,\\mathbb{R}^m), defined by (a \\cdot f )(x) = a \\cdot f(x) \\quad\\text{for all } x \u2208 A.","title":"Mappings as a vector space"},{"location":"ch02notes/#sequence-in-mathbbrn","text":"Let A be a subset of \\mathbb{R}^n . A sequence in A is an infinite list of vectors \\{x_1,x_2,x_3,...\\} in A , often written \\{x_\u03bd\\} . Since a vector has n entries, each vector x_\u03bd in the sequence takes the form (x_{1,\u03bd},...,x_{n,\u03bd}) .","title":"Sequence in \\mathbb{R}^n"},{"location":"ch02notes/#definition-231-null-sequence","text":"The sequence (x_{\\nu }) in \\mathbb{R}^n is null if for every \u03b5 > 0 there exists some \u03bd_0 such that \\text{if } \u03bd > \u03bd_0 \\text{ then } |x_v| < \\epsilon. That is, a sequence is null if for every \u03b5 > 0 , all but finitely many terms of the sequence lie within distance \u03b5 of 0_n . If \\{x_\u03bd\\} is a null sequence and |y_\u03bd| \\leq |x_\u03bd| or all \u03bd then also \\{y_\u03bd\\} is null. \\{x_\u03bd\\} and \\{y_\u03bd\\} are null sequence, then |x_\u03bd + y_\u03bd| \\leq |x_\u03bd| + |y_\u03bd| , so \\{x_\u03bd + y_\u03bd\\} is null. \\{c x_\u03bd\\} is null seq, since |cx_\u03bd| = |c||x_\u03bd| So the set of null sequences in \\mathbb{R}^n forms a vector space. A vector sequence \\{x_\u03bd\\} is null if and only if the scalar sequence \\{|x_\u03bd|\\} is null.","title":"Definition 2.3.1 (Null Sequence)."},{"location":"ch02notes/#lemma-232-componentwise-nature-of-nullness","text":"The vector sequence \\{(x_{1,\u03bd}, \\cdots, x_{n,\u03bd})\\} is null if and only if each of its component scalar sequences \\{x_{j,\u03bd}\\} (j \\in \\{1, \\cdots, n\\}) is null. Proof Use |x_{j,\u03bd}| \\leq |x_\u03bd| \\leq \\sum_{j=1}^{n} |x_{j,\u03bd}| \\square","title":"Lemma 2.3.2 (Componentwise nature of nullness)."},{"location":"ch02notes/#definition-233-sequence-convergence-sequence-limit","text":"Let A be a subset of \\mathbb{R}^n . Consider a sequence \\{x_\u03bd\\} in A and a point p \u2208 \\mathbb{R}^n . The sequence \\{x_\u03bd\\} converges to p (or has limit p ), written \\{x_\u03bd\\} \u2192 p , if the sequence \\{x_\u03bd\u2212p\\} is null. When the limit p is a point of A , the sequence \\{x_\u03bd\\} converges in A . A sequence can only converges to one limit.","title":"Definition 2.3.3 (Sequence convergence, sequence limit)."},{"location":"ch02notes/#proposition-234-linearity-of-convergence","text":"Let \\{x_\u03bd\\} be a sequence in \\mathbb{R}^n converging to p , let \\{y_\u03bd\\} be a sequence in \\mathbb{R}^n converging to q , and let c be a scalar. Then the sequence \\{x_\u03bd +y_\u03bd\\} converges to p+q , and the sequence \\{c x_\u03bd\\} converges to c_p . Proof : \\begin{split} |(x_\u03bd + y_\u03bd) - (p + q)| &= |(x_\u03bd - p) + (y_\u03bd-q)| \\\\ &\\leq |x_\u03bd - p| + |y_\u03bd - q| \\end{split} So (x_\u03bd + y_\u03bd) - (p + q) is null. Then x_\u03bd + y_\u03bd \u2192 p+q . |c x_\u03bd - cp| = |c (x_\u03bd - p)| \\leq |c| |x_\u03bd - p| So c x_\u03bd - cp is null. Then c x_\u03bd \u2192 cp \\square","title":"Proposition 2.3.4 (Linearity of convergence)."},{"location":"ch02notes/#proposition-235-componentwise-nature-of-convergence","text":"The vector sequence \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} converges to the vector (p_1, \\cdots, p_n) if and only if each component scalar sequence \\{x_{j, \u03bd}\\} (j = 1, \\cdots, n) . converges to the scalar p_j . Proof : \\{(x_{1, \u03bd}, \\cdots, x_{n, \u03bd})\\} \\longrightarrow p \\\\ \\Longleftrightarrow \\\\ x_\u03bd - p \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} - p_j \\quad \\text{is null} \\\\ \\Longleftrightarrow \\\\ x_{j, \u03bd} \\longrightarrow p_j \\square","title":"Proposition 2.3.5 (Componentwise nature of convergence)."},{"location":"ch02notes/#definition-236-continuity","text":"Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a mapping, and let p be a point of A . Then f is continuous at p if for every sequence \\{x_\u03bd\\} in A converging to p , the sequence f(x_\u03bd) converges to f(p) . The mapping f is continuous on A (or just continuous when A is clearly established) if it is continuous at each point p \u2208 A .","title":"Definition 2.3.6 (Continuity)."},{"location":"ch02notes/#modulus-function-is-continuous","text":"|| : \\mathbb{R}^n \\longrightarrow \\mathbb{R} Proof : Assume \\{x_\u03bd\\} \\rightarrow p , then from exercise 2.2.7 ||x_\u03bd| - |p|| \\leq | x_\u03bd - p | Then |x_\u03bd| - |p| is null, i.e. \\{f(x_\u03bd)\\} \\rightarrow f(p) . So || is continuous.","title":"Modulus function is continuous"},{"location":"ch02notes/#the-inner-product-is-continuous","text":"Given a \\in \\mathbb{R}^n , define T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n, \\quad T(x) = \u27e8a,x\u27e9 T is continuous. From Cauchy-Schwarz inequality |\u27e8a,x\u27e9| \\leq |a||x| Given p \\in \\mathbb{R}^n |T(x_\u03bd) - T(p)| = |\u27e8a,x_v\u27e9 - \u27e8a,p\u27e9| = |\u27e8a,x_\u03bd - p\u27e9| \\leq |a| | x_\u03bd - p | Since we know |x_\u03bd - p| \\rightarrow 0 , we have |T(x_\u03bd) - T(p)| \\rightarrow 0 . \\square","title":"The inner product is continuous"},{"location":"ch02notes/#jth-coordinate-function-map","text":"Consider the following mapping \\pi_j : \\mathbb{R}^n \\longrightarrow \\mathbb{R}, \\quad \\pi_j (x_1, \\cdots, x_n) = x_j Since \\pi_j(x) = \u27e8e_j,x\u27e9 , this mapping is continuous.","title":"jth coordinate function map"},{"location":"ch02notes/#proposition-237-vector-space-properties-of-continuity","text":"Let A be a subset of \\mathbb{R}^n , let f,g : A \\longrightarrow \\mathbb{R}^m be continuous mappings, and let c \\in \\mathbb{R} . Then the sum and the scalar multiple mappings f+g,cf : A \\longrightarrow \\mathbb{R}^m are continuous. Thus the set of continuous mappings from A to \\mathbb{R}^m forms a vector subspace of \\mathcal{M}(A, \\mathbb{R}^m) . Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . Then \\{f(x_\u03bd)\\} \\rightarrow f(p), \\{g(x_\u03bd)\\} \\rightarrow g(p) . Then from Proposition 2.3.4 (Linearity of convergence), \\{f(x_\u03bd) + g(x_\u03bd)\\} \\rightarrow f(p) + g(p) , i.e. \\{(f+g)(x_\u03bd)\\} \\rightarrow (f+g)(p) . So f+g is continuous. For the same reason cf is continuous. \\square","title":"Proposition 2.3.7 (Vector space properties of continuity)."},{"location":"ch02notes/#proposition-238-persistence-of-continuity-under-composition","text":"Let A be a subset of \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let B be a superset of f(A) in \\mathbb{R}^m , and let g : B \\longrightarrow \\mathbb{R}^l be a continuous mapping. Then the composition mapping g \\circ f : A \\longrightarrow \\mathbb{R}^l is continuous. Proof : Given p \\in A , and \\{x_\u03bd\\} \\rightarrow p . We have \\{f(x_\u03bd)\\} \\rightarrow f(p) . Since f(x_\u03bd), f(p) \\in B , and g is a continuous mapping, we have \\{g(f(x_\u03bd))\\} \\rightarrow g(f(p)) . Therefore, g \\circ f is continuous. \\square","title":"Proposition 2.3.8 (Persistence of continuity under composition)."},{"location":"ch02notes/#theorem-239-componentwise-nature-of-continuity","text":"Let A \u2282 \\mathbb{R}^n , let f : A \\longrightarrow \\mathbb{R}^m have component functions f_1,...,f_m , and let p be a point in A . Then f \\text{ is continuous at } p \\Longleftrightarrow \\text{each } f_i \\text{ is continuous at } p. Proof : \\Rightarrow Given any sequence (x_{\\nu}) \\rightarrow p , since f is continuous at p , then f(x_{\\nu}) \\rightarrow f(p) . From Proposition 2.3.5 Componentwise nature of convergence f_i(x_{\\nu}) \\rightarrow f_i(p) . So f_i is continuous at p . \\Leftarrow Given any sequence (x_{\\nu}) \\rightarrow p , and since f_i is continuous at p , then f_i(x_{\\nu}) \\rightarrow f_i(p) . From Proposition 2.3.5 Componentwise nature of convergence f(x_{\\nu}) \\rightarrow f(p) . So f is continuous at p . \\square","title":"Theorem 2.3.9 (Componentwise nature of continuity)."},{"location":"ch02notes/#some-examples","text":"Consider f(x, y) = \\begin{cases} \\frac{2xy}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Can the constant b be specified to make f continuous at 0 ? Take a sequence \\{(x_\u03bd, y_\u03bd)\\} \\rightarrow 0 along the line y= mx . f(x_\u03bd, y_\u03bd) = \\\\ \\frac{2x_\u03bd y_\u03bd}{x_\u03bd^2 + y_\u03bd^2} = \\\\ \\frac{2m}{1 + m^2} hence f cannot be made continuous at 0 . \\square Now consider g(x, y) = \\begin{cases} \\frac{x^2y}{x^4 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} If (x,y) approaches to (0,0) with y = x , then g(x,y) = \\frac{x^3}{x^4 + x^2} = \\frac{x}{x^2 + 1} \\rightarrow 0 If (x,y) approaches to (0,0) with y = x^2 , then g(x,y) = \\frac{x^4}{x^4 + x^4} = \\frac{1}{2} hence f cannot be made continuous at 0 . \\square","title":"Some examples"},{"location":"ch02notes/#the-size-bounds-to-prove-continuity","text":"Consider h(x, y) = \\begin{cases} \\frac{x^3}{x^2 + y^2} &\\text{if } (x,y) \\neq 0\\\\ b &\\text{if } (x,y) = 0\\\\ \\end{cases} Note from 2.2.7, we have |x| \\leq |(x,y)| , so \\frac{x^3}{x^2 + y^2} \\leq \\frac{|(x,y)|^3}{|(x,y)|^2} = |(x,y)| \\rightarrow 0 So setting b = 0 makes it continuous at 0 .","title":"The size bounds to prove continuity"},{"location":"ch02notes/#summary-of-the-3-examples","text":"The straight line test can prove that a limit does not exist, or it can determine the only candidate for the value of the limit, but it cannot prove that the candidate value is the limit. When the straight line test determines a candidate value of the limit, approaching along a curve can further support the candidate, or it can prove that the limit does not exist by determining a di\ufb00erent candidate as well. The size bounds can prove that a limit does exist, but they can only suggest that a limit does not exist.","title":"Summary of the 3 examples"},{"location":"ch02notes/#proposition-2310-persistence-of-inequality","text":"Let A be a subset of \\mathbb{R}^n and let f : A \\longrightarrow \\mathbb{R}^m be a continuous mapping. Let p be a point of A , let b be a point of \\mathbb{R}^m , and suppose that f(p) \\neq b . Then there exists some \\epsilon > 0 , such that \\text{for all } x \\in A \\text{such that } |x-p| < \\epsilon, f(x) \\neq b. Proof : Assume otherwise, then for \\nu \\in \\mathbb{N} , we can find x_\u03bd , such that | x_\u03bd - p | < 1/\u03bd and f(x_\u03bd) = b . Since f is continuous, then f(p) = b . We have a contradiction. \\square","title":"Proposition 2.3.10 (Persistence of inequality)."},{"location":"ch02notes/#24-topology-compact-sets-and-continuity","text":"","title":"2.4 Topology: Compact Sets and Continuity"},{"location":"ch02notes/#definition-241-ball","text":"For every point p \u2208 \\mathbb{R}^n and every positive real number \u03b5 > 0 , the \u03b5 -ball centered at p is the set B(p, \u03b5) = \\{x \\in \\mathbb{R}^n : |x-p| \\leq \u03b5\\}","title":"Definition 2.4.1 (\u03b5-ball)."},{"location":"ch02notes/#definition-242-limit-point","text":"Let A be a subset of \\mathbb{R}^n , and let p be a point of \\mathbb{R}^n . The point p is a limit point of A if every \u03b5 -ball centered at p contains some point x \u2208 A such that x \\neq p .","title":"Definition 2.4.2 (Limit point)."},{"location":"ch02notes/#isolated-point","text":"Definition 1: A point in A that is not a limit point of A . Definition 2: Equivalently, p is an isolated point of A if p \u2208 A and there exists some \u03b5 > 0 such that B(p,\u03b5)\u2229A= \\{p\\} .","title":"Isolated point"},{"location":"ch02notes/#lemma-243-sequential-characterization-of-limit-points","text":"Let A be a subset of Rn, and let p be a point of \\mathbb{R}^n . Then p is the limit of a sequence \\{x_\u03bd\\} in A with each x_\u03bd \\neq p if and only if p is a limit point of A . Proof : \\Rightarrow Given B(p, \u03b5) , since \\{x_\u03bd\\} \\rightarrow p , we can find \u03bd_0 , such that \u03bd > \u03bd_0 , |x_\u03bd - p| < \u03b5 , i.e. x_\u03bd \\in B(p, \u03b5) . Since x_\u03bd \\neq p , then p is a limit point of A . \\Leftarrow Assume p is a limit point of A , let x_1 \\in A . Let \u03b5_1 = |x_1 - p| , and \u03b5_2 = \u03b5_1 / 2 . We can find p \\neq x_2 \\in B(p, \u03b5_2) . This process can continue infinitely. Then we find a \\{x_\u03bd\\} \\rightarrow p . \\square","title":"Lemma 2.4.3 (Sequential characterization of limit points)."},{"location":"ch02notes/#definition-244-closed-set","text":"A subset A of \\mathbb{R}^n is closed if it contains all of its limit points.","title":"Definition 2.4.4 (Closed set)."},{"location":"ch02notes/#proposition-245-sequential-characterization-of-closed-sets","text":"Let A be a subset of \\mathbb{R}^n . Then A is closed if and only if every sequence in A that converges in \\mathbb{R}^n in fact converges in A . Proof : \\Rightarrow Assume \\{x_\u03bd\\} \\rightarrow p and x_\u03bd \\in A , if p = x_\u03bd for some \u03bd , then p \\in A . If p \\neq x_\u03bd , then p is a limit point of A . Since A is closed, so p \\in A . \\square \\Leftarrow Assume p is a limit point of A , then from Lemma 2.4.3 \\Rightarrow , we can find \\{x_\u03bd\\} \\rightarrow p and x_\u03bd \\in A . Then p \\in A , so p is closed. \\square","title":"Proposition 2.4.5 (Sequential characterization of closed sets)."},{"location":"ch02notes/#definition-246-bounded-set","text":"A set A in \\mathbb{R}^n is bounded if A \u2282 B(0,R) for some R > 0 .","title":"Definition 2.4.6 (Bounded set)."},{"location":"ch02notes/#proposition-247-convergence-implies-boundedness","text":"If the sequence \\{x_\u03bd\\} converges in \\mathbb{R}^n then it is bounded.","title":"Proposition 2.4.7 (Convergence implies boundedness)."},{"location":"ch02notes/#definition-248-subsequence","text":"A subsequence of the sequence \\{x_\u03bd\\} is a sequence consisting of some (possibly all) of the original terms, in ascending order of indices.","title":"Definition 2.4.8 (Subsequence)."},{"location":"ch02notes/#lemma-249-persistence-of-convergence","text":"Let \\{x_\u03bd\\} converge to p . Then every subsequence \\{x_{\u03bd_k}\\} also converges to p .","title":"Lemma 2.4.9 (Persistence of convergence)."},{"location":"ch02notes/#theorem-2410-bolzanoweierstrass-property-in-r","text":"Let A be a bounded subset of \\mathbb{R} . Then every sequence in A has a convergent subsequence. Proof : This is just a summary. Define the max-point if it is at least as big as all later terms, i.e., x_\u03bd \u2265 x_\u03bc for all \u03bc > \u03bd . If there are infinitely many max-points in \\{x_\u03bd\\} then these form a decreasing sequence. If there are only finitely many max-points then \\{x_\u03bd\\} has an increasing sequence starting after the last max-point. \\square","title":"Theorem 2.4.10 (Bolzano\u2013Weierstrass property in R)."},{"location":"ch02notes/#theorem-2411-bolzanoweierstrass-property-in-mathbbrn-sequential-characterization-of-bounded-sets","text":"Let A be a subset of \\mathbb{R}^n . Then A is bounded if and only if every sequence in A has a subsequence that converges in \\mathbb{R}^n . Proof : \\Longrightarrow Use 2.4.10 on every index. \\Longleftarrow Proof by contradiction. \\square Note how the sequential characterizations in Proposition 2.4.5 > and in the Bolzano\u2013Weierstrass property complement each other. The proposition characterizes every closed set in \\mathbb{R}^n by the fact that if a sequence converges in the ambient space then it converges in the set. The Bolzano\u2013Weierstrass property characterizes every bounded set in \\mathbb{R}^n by the fact that every sequence in the set has a subsequence that converges in the ambient space but not necessarily in the set. Both the sequential characterization of a closed set and the sequential characterization of a bounded set refer to the ambient space \\mathbb{R}^n in which the set lies.","title":"Theorem 2.4.11 (Bolzano\u2013Weierstrass property in \\mathbb{R}^n: sequential characterization of bounded sets)."},{"location":"ch02notes/#definition-2412-compact-set","text":"A subset K of \\mathbb{R}^n is compact if it is closed and bounded.","title":"Definition 2.4.12 (Compact set)."},{"location":"ch02notes/#theorem-2413-sequential-characterization-of-compact-sets","text":"Let K be a subset of \\mathbb{R}^n . Then K is compact if and only if every sequence in K has a subsequence that converges in K . Proof : \\Longrightarrow Since its bounded we can use the Bolzano\u2013Weierstrass property. And since it's closed, it converges in K . \\Longleftarrow Just use the \\Longleftarrow of 2.4.5 and 2.4.11. \\square By contrast to the sequential characterizations of a closed set and of a bounded set, the sequential characterization of a compact set K makes no reference to the ambient space Rn in which K lies. A set\u2019s property of being compact is innate in a way that a set\u2019s property of being closed or of being bounded is not.","title":"Theorem 2.4.13 (Sequential characterization of compact sets)."},{"location":"ch02notes/#theorem-2414-the-continuous-image-of-a-compact-set-is-compact","text":"Let K be a compact subset of \\mathbb{R}^n and let f : K \\longrightarrow \\mathbb{R}^m be continuous. Then f(K) , the image set of K under f , is a compact subset of \\mathbb{R}^m . Proof : Let \\{y_\u03bd\\} \\in f(K) , we can find \\{x_\u03bd\\} \\in K , such that f(x_\u03bd) = y_\u03bd . Since K is compact, then we can find subsuquence \\{x_{\u03bd_k}\\} \\longrightarrow p \\in K . Since f is continuous, then \\{f(x_{\u03bd_k})\\} \\longrightarrow f(p) \\in f(K) . And \\{f(x_{\u03bd_k})\\} is subsequence of f(x_\u03bd) . So f(K) is also compact. \\square","title":"Theorem 2.4.14 (The continuous image of a compact set is compact)."},{"location":"ch02notes/#theorem-2415-extreme-value-theorem","text":"Let K be a nonempty compact subset of \\mathbb{R}^n and let the function f : K \\longrightarrow R be continuous. Then f takes a minimum and a maximum value on K . Proof : f(K) is compact in \\mathbb{R} , so it has a greatest lower bound a and a least upper bound b by the completeness of the real number system. If a is not a limit point of f(K) and a \\not\\in f(K) , then we can find U_{\\delta}(a) such that U_{\\delta}(a) \\cap f(K) = \\emptyset . That means, a + \\delta /2 is another lower bound. We got a contradiction. So a is either an isolated point or a limit point of f(K) . \\square","title":"Theorem 2.4.15 (Extreme value theorem)."},{"location":"ch02notes/#topological-property","text":"A topological property of sets is a property that is preserved under continuity. Theorem 2.4.14 says that compactness is a topological property. Neither the property of being closed nor the property of being bounded is in itself topological. That is, the continuous image of a closed set need not be closed, and the continuous image of a bounded set need not be bounded; for that matter, the continuous image of a closed set need not be bounded, and the continuous image of a bounded set need not be closed (Exercise 2.4.8). The nomenclature continuous image in the slogan-title of Theorem 2.4.14 and in the previous paragraph is, strictly speaking, inaccurate: the image of a mapping is a set, and the notion of a set being continuous doesn\u2019t even make sense according to our grammar. As stated correctly in the body of the theorem, continuous image is short for image under a continuous mapping. \\square","title":"Topological property"},{"location":"ch03ex/","text":"Chapter 3 Linear Mappings and Their Matrices 3.1 Linear Mappings 3.1.1. Prove that T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is linear if and only if it satisfies (3.1) and (3.2). (It may help to rewrite (3.1) with the symbols x_1 and x_2 in place of x and y . Then prove one direction by showing that (3.1) and (3.2) are implied by the defining condition for linearity, and prove the other direction by using induction to show that (3.1) and (3.2) imply the defining condition. Note that as pointed out in the text, one direction of this argument has a bit more substance than the other.) Proof : \\Rightarrow Use induction, when k = 1 T(\u03b1_1 x_1) = \u03b1_1 T(x_1) This is (3.2). Then assume k = n holds, we are checking k = n + 1 . \\begin{align*} T(\\sum_{i = 1}^{n+1} \u03b1_i x_i) &= T((\\sum_{i = 1}^{n} \u03b1_i x_i) + \u03b1_{n+1} x_{n+1}) \\\\ &= T((\\sum_{i = 1}^{n} \u03b1_i x_i)) + T(\u03b1_{n+1} x_{n+1}) \\qquad \\text{Use (3.1)}\\\\ &= T((\\sum_{i = 1}^{n} \u03b1_i x_i)) + \u03b1_{n+1} T(x_{n+1}) \\qquad \\text{Use (3.2)}\\\\ &= \\sum_{i = 1}^{n}\u03b1_kT(x_k) + \u03b1_{n+1} T(x_{n+1}) \\qquad \\text{Use induction}\\\\ &= \\sum_{i = 1}^{n+1}\u03b1_kT(x_k)\\\\ \\end{align*} \\Leftarrow To prove (3.1), set k = 2, \u03b1_1 = \u03b1_2 = 1 . To prove (3.2), set k = 1 . \\square 3.1.2. Suppose that T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is linear. Show that T(0_n) = 0_m . (An intrinsic argument is nicer.) Proof : Note in both \\mathbb{R}^n, \\mathbb{R}^m , 0 v = 0 . So T(0) = T(0 v_n) = 0 T(v_n) = 0 v_m = 0 \\square 3.1.3 Fix a vector a \u2208 \\mathbb{R} . Show that the mapping f : \\mathbb{R}^n \\longrightarrow \\mathbb{R} given by T(x) = \u27e8a,x\u27e9 is linear, and that T(e_j) = a_j for j = 1,...,n . Proof : (3.1) T(x + y) = \u27e8a,x+y\u27e9 = \u27e8a,x\u27e9 + \u27e8a,y\u27e9 = T(x) + T(y) (3.2) T(\u03b1x) = \u27e8a,\u03b1x\u27e9 = \u03b1 \u27e8a,x\u27e9 = \u03b1T(x) T(e_j) = a_j follows the definition of the inner product. \\square 3.1.4 Find the linear mapping T : \\mathbb{R}^3 \\longrightarrow \\mathbb{R} such that T(0,1,1) = 1 , T(1,0,1) = 2 , and T(1,1,0) = 3 . Solution We need to solve this equations \\begin{cases} 0x + 1y + 1z = 1 &\\text{}\\\\ 1x + 0y + 1z = 2 &\\text{}\\\\ 1x + 1y + 0z = 3 &\\text{}\\\\ \\end{cases} So T(x) = \u27e8a,x\u27e9, a = (2, 1, 0) . \\square 3.1.5. Complete the proof of the componentwise nature of linearity. Proof : For \u03b1 \\in \\mathbb{R}, x \\in \\mathbb{R}^n If T_1, \\cdots, T_m are linear, then T(\u03b1x) = (T_1(\\alpha x), \\cdots, T_m(\\alpha x)) \\\\ = (\u03b1T_1(x), \\cdots, \u03b1T_m(x)) \\\\ = \u03b1 (T_1(x), \\cdots, T_m(x)) \\\\ = \u03b1 T(x) On the other hand, if T is linear, Then \\begin{split} (T_1(\\alpha x), \\cdots, T_m(\\alpha x)) &= T(\u03b1x) \\\\ &= \u03b1 T(x) \\\\ &= \u03b1 (T_1(x), \\cdots, T_m(x)) \\\\ &= (\u03b1T_1(x), \\cdots, \u03b1T_m(x)) \\\\ \\end{split} So T_i(\u03b1x) = \u03b1 T_i(x) \\square 3.1.6. Carry out the matrix-by-vector multiplications Solution : (a) [1,3,6] (b) (ax+by, cx+dy, ex+fy) (c) (x_1 y_1, \\cdots, x_n y_n) (d) (0,0,0) \\square 3.1.7. Prove that the identity mapping \\text{id} : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n is linear. What is its matrix? Explain. Proof (3.1) \\text{id}(x+y) = x+y = \\text{id}(x) + \\text{id}(y) (3.2) \\text{id}(\u03b1x) = \u03b1x = \u03b1 \\text{id}(x) Then matrix is \\begin{bmatrix} 1 &0 &\\cdots &0 \\\\ 0 &1 &\\cdots &0 \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0 &0 &\\cdots &1 \\\\ \\end{bmatrix} \\square 3.1.8. Let \u03b8 denote a fixed but generic angle. Argue geometrically that the mapping R : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n given by counterclockwise rotation by \u03b8 is linear, and then find its matrix. Proof : Geometrically is quite obvious. Consider R(e_1) = (\\cos \u03b8, \\sin \u03b8) \\\\ R(e_2) = (-\\sin \u03b8, \\cos \u03b8) \\\\ Then matrix is \\begin{bmatrix} \\cos \u03b8 & -\\sin \u03b8 \\\\ \\sin \u03b8 & \\cos \u03b8 \\\\ \\end{bmatrix} \\square 3.1.9. Show that the mapping Q : \\mathbb{R}^2 \\longrightarrow \\mathbb{R}^2 given by reflection through the x-axis is linear. Find its matrix. R(e_1) = (1, 0) \\\\ R(e_2) = (0, -1) \\\\ Then matrix is \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\\\ \\end{bmatrix} \\square 3.1.10. Show that the mapping P : \\mathbb{R}^2 \\longrightarrow \\mathbb{R}^2 given by orthogonal projection onto the diagonal line x = y is linear. Find its matrix. (See Exercise 2.2.15.) Proof : We can use vector d = (1,1) to represent the diagonal line x = y . Then for x, y \\in \\mathbb{R}^2 P(x) = \\frac{\u27e8d,x\u27e9}{|d|^2} d \\\\ P(y) = \\frac{\u27e8d,y\u27e9}{|d|^2} d \\\\ P(x+y) = \\frac{\u27e8d,x+y\u27e9}{|d|^2} d \\\\ = \\frac{\u27e8d,x\u27e9}{|d|^2} d + \\frac{\u27e8d,y\u27e9}{|d|^2} d = P(x) + P(y) Also P(\u03b1x) = \\frac{\u27e8d,\u03b1x\u27e9}{|d|^2} d = \u03b1\\frac{\u27e8d,x\u27e9}{|d|^2} d = \u03b1 P(x) So P is linear. R(e_1) = (\\sqrt[]{2}/2, \\sqrt[]{2}/2) \\\\ R(e_2) = (\\sqrt[]{2}/2, \\sqrt[]{2}/2) \\\\ Then matrix is \\begin{bmatrix} \\sqrt[]{2}/2 & \\sqrt[]{2}/2 \\\\ \\sqrt[]{2}/2 & \\sqrt[]{2}/2 \\\\ \\end{bmatrix} \\square 3.1.11. Draw the graph of a generic linear mapping from \\mathbb{R}^2 to \\mathbb{R}^3 . Solution : skip. 3.1.12. Continue the proof of Proposition 3.1.8 by proving the other three statements about S + T and aS satisfying (3.1) and (3.2). Proof : (3.2) for S+T . (S+T)(\u03b1x) = S(\u03b1x) + T(\u03b1x) = \u03b1S(x) + \u03b1T(x) = \u03b1 (S(x) + T(x)) = \u03b1 (S+T)(x) Next, (3.1) for aS . \\begin{align*} (aS)(x+y) &= a(S(x + y)) & \\quad \\text{definition of the } aS \\\\ &= a(S(x) + S(y)) & \\quad \\text{S is linear} \\\\ &= aS(x) + aS(y) & \\quad \\text{Vector space axioms D2} \\\\ &= (aS)(x) + (aS)(y) & \\quad \\text{definition of the } aS \\\\ \\end{align*} Next, (3.2) for aS . \\begin{aligned} (aS)(\u03b1x) &= a(S(\u03b1x)) & \\quad \\text{definition of the } aS \\\\ &= a(\u03b1S(x)) & \\quad \\text{S is linear} \\\\ &= \u03b1(aS(x)) & \\quad \\mathbb{R} \\text{ is a field} \\\\ &= \u03b1((aS)(x)) & \\quad \\text{definition of the } aS \\\\ \\end{aligned} \\square 3.1.13. If S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) , show that S \\circ T : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m lies in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^m) . Proof : (3.1) \\begin{align*} (S \\circ T)(x+y) &= S(T(x+y)) \\\\ &= S(T(x) + T(y)) \\\\ &= (S \\circ T)(x) + (S \\circ T)(y) \\\\ \\end{align*} Next (3.2) \\begin{align*} (S \\circ T)(\u03b1x) &= S(T(\u03b1x)) \\\\ &= S(\u03b1T(x)) \\\\ &= \u03b1(S(T(x))) \\\\ &= \u03b1((S \\circ T)(x)) \\\\ \\end{align*} So S \\circ T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^m) . 3.1.14 (a) Let S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) . Its transpose is the mapping S^T : \\mathbb{R}^m \\longrightarrow \\mathbb{R}^n defined by the characterizing condition \u27e8x,S^T(y)\u27e9 = \u27e8S(x),y\u27e9 \\qquad \\text{for all } x \\in \\mathbb{R}^n \\text{ and } y \\in \\mathbb{R}^m. Granting that indeed a unique such S^T exists, use the characterizing condition to show that S^T(y+y') = S^T(y) + S^T(y') \\qquad \\text{for all } y, y' \\in \\mathbb{R}^m by showing that \u27e8x,S^T(y+y')\u27e9 = \u27e8x,S^T(y)+S^T(y')\u27e9 \\qquad \\text{for all } x \\in \\mathbb{R}^n \\text{ and } y, y' \\in \\mathbb{R}^m. Proof : \\begin{align*} \u27e8x,S^T(y+y')\u27e9 &= \u27e8S(x),y+y'\u27e9 \\\\ &= \u27e8S(x),y\u27e9 + \u27e8S(x),y'\u27e9 \\\\ &= \u27e8x,S^T(y)\u27e9 + \u27e8x,S^T(y')\u27e9 \\\\ &= \u27e8x,S^T(y) + S^T(y')\u27e9 \\\\ \\end{align*} Because x can be arbitrary, so we have S^T(y+y') = S^T(y) + S^T(y') \\qquad \\text{for all } y, y' \\in \\mathbb{R}^m \\square (b) Keeping S from part (a), now further introduce T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) , so that also S \\circ T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^m) . Show that the transpose of the composition is the composition of the transposes in reverse order, (S \u25e6T)^T = T^T \u25e6S^T, by showing that \u27e8x,(S \u25e6T)^T(z)\u27e9 = \u27e8x, (T^T \u25e6S^T)(z)\u27e9 Proof : \\begin{align*} \u27e8x,(S \u25e6T)^T(z)\u27e9 &= \u27e8(S\u25e6T)(x), z\u27e9 \\\\ &= \u27e8S(T(x)), z\u27e9 \\\\ &= \u27e8T(x), S^T(z)\u27e9 \\\\ &= \u27e8x, T^T (S^T(z))\u27e9 \\\\ &= \u27e8x, (T^T \u25e6 S^T)(z)\u27e9 \\\\ \\end{align*} \\square 3.1.15 A mapping f : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is called a\ufb03ne if it has the form f(x) = T(x) + b , where T \u2208 \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and b \u2208 \\mathbb{R}^m . State precisely and prove: the composition of a\ufb03ne mappings is a\ufb03ne. Proof : If f : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^n and g : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m are affine, then g \\circ f : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m \\quad \\text{ is affine}. \\begin{align*} g(f(x)) &= g(T(x) + b) \\\\ &= g(T(x)) + g(b) \\\\ &= S(T(x)) + c + g(b) \\\\ \\end{align*} \\square 3.1.16 Let T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m be a linear mapping. Note that since T is continuous and since the absolute value function on \\mathbb{R}^m is continuous, the composite function |T| : \\mathbb{R}^n \\longrightarrow \\mathbb{R} is continuous. (a) Let S= \\{x \u2208 \\mathbb{R}^n : |x| = 1\\} . Explain why S is a compact subset of \\mathbb{R}^n . Explain why it follows that |T| takes a maximum value c on S . Proof : S is bounded. Then we just need to show S is closed. Use \"Sequential characterization of closed sets\", assume \\{x_\u03bd\\} \\rightarrow p \\in \\mathbb{R}^n . Since || is a continuous function and \\lim_{\\nu \\to \\infty} |x_\u03bd| = 1 Then |p| = 1 . So p \\in S , then S is closed. So S is compact. Then based on Theorem 2.4.15 (Extreme value theorem), |T| takes a maximum value c on S . \\square (b) Show that |T(x)| \u2264 c|x| for all x \u2208 \\mathbb{R}^n . This result is the linear magnification boundedness lemma. We will use it in Chapter 4. Proof : x = |x|\\frac{x}{|x|}\\\\ \\begin{align*} T(x) &= T(|x|\\frac{x}{|x|}) \\\\ &= |x|T(\\frac{x}{|x|}) &\\qquad T\\text{ is linear} \\\\ &\\leq c|x| &\\qquad \\left| \\frac{x}{|x|} \\right| = 1 \\\\ \\end{align*} \\square 3.1.17. Let T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m be a linear mapping. (a) Explain why the set D = \\{x \u2208 \\mathbb{R}^n : |x| = 1\\} is compact. Proof : See 3.1.16 (a) (b) Use part (a) of this exercise and part (b) of the preceding exercise to explain why therefore the set \\{|T(x)| : x \u2208 D\\} has a maximum. This maximum is called the norm of T and is denoted \\left\\| T \\right\\| . Proof : See 3.1.16 (b). (c) Explain why \\left\\| T \\right\\| is the smallest value K that satisfies the condition from part (b) of the preceding exercise, |T(x)| \u2264 K|x| for all x \u2208 \\mathbb{R}^n . Proof : First, we know from 3.1.16 (b), |T(x)| \u2264 \\left\\| T \\right\\| |x| . Next, we can find a point c \\in D such that |T(c)| = \\left\\| T \\right\\| |c| = \\left\\| T \\right\\| \\square (d) Show that for every S,T \u2208 \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and every a \u2208 \\mathbb{R} , \\left\\| S + T \\right\\| \\leq \\left\\| S \\right\\| + \\left\\| T \\right\\| \\qquad \\text{and} \\qquad \\left\\| aT \\right\\| = |a|\\left\\| T \\right\\| Define a distance function d: \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) \\times \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) \\longrightarrow \\mathbb{R}, \\quad d(S, T) = \\left\\| S - T \\right\\| Show that this function satisfies the distance properties of Theorem 2.2.8. Proof : \\left\\| S+T \\right\\| = |(S+T)(c)| \\\\ = |S(c) + T(c)| \\\\ \\leq |S(c)| + |T(c)| \\\\ \\leq \\left\\| S \\right\\| + \\left\\| T \\right\\| \\left\\| aT \\right\\| = |(aT)(c)| \\\\ |a(T(c))| = |a| |T(c)| = |a| \\left\\| T \\right\\| \\square (D1) d(S, T) \\geq 0 . Since |(S - T)(x)| \\geq 0 , then \\left\\| S-T \\right\\| \\geq 0 . d(S,T) = 0 if and only if S = T . \\square (D2) d(S,T) = d(T,S) Because |(S-T)(x)| = |(T-S)(x)| \\square (D3) d(R,T) \u2264 d(R,S)+d(S,T) d(R,T) = \\left\\| R - T \\right\\| = |(R-T)(c)| \\\\ = |R(c) - T(c)| \\\\ = |(R(c)-S(c)) + (S(c)-T(c))| \\\\ \\leq |R(c)-S(c)| + |S(c)-T(c)| \\\\ = |(R-S)(c)| + |(S-T)(c)| \\\\ \\leq \\left\\| R-S \\right\\| + \\left\\| S-T \\right\\| \\\\ = d(R,S)+d(S,T) \\square (e) Show that for every S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and every T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) \\left\\| ST \\right\\| \\leq \\left\\| S \\right\\| \\left\\| T \\right\\| Proof : Assume \\left\\| ST \\right\\| = |S(T(c))| Let x = T(c) \\in \\mathbb{R}^n . |S(x)| \\leq \\left\\| S \\right\\| |x| \\\\ |x| = |T(c)| \\leq \\left\\| T \\right\\| |c| = \\left\\| T \\right\\| Therefore \\begin{align*} \\left\\| ST \\right\\| &= |S(T(c))| \\\\ &= |S(x)| \\\\ &\\leq \\left\\| S \\right\\| |x| \\\\ &\\leq \\left\\| S \\right\\| \\left\\| T \\right\\| \\\\ \\end{align*} \\square 3.2 Operations on Matrices 3.2.1. Justify Definition 3.2.2 of scalar multiplication of matrices. Solution : The j th column of \u03b1A is $$ (\u03b1S)(e_j) = \u03b1 (S(e_j)) = \u03b1 \\times \\text{( j th column of A)} $$ So it's reasonable to define \u03b1A = [\u03b1a_{ij}]_{m \\times n} . \\square 3.2.2. Carry out the matrix multiplications. Solution : skipped. 3.2.3 Prove more of Proposition 3.2.5, that A(B+C) = AB+AC , (\u03b1A)B= A(\u03b1B) , and I_mA= A for suitable matrices A,B,C and any scalar \u03b1 . Proof : $$ \\begin{align*} (R \u25e6 (S + T))(x) &= R((S+T)(x)) &\\quad \\text{by definition of <script type=\"math/tex\">R \u25e6 (S + T) } \\ &= R(S(x)+T(x)) &\\quad \\text{by definition of (S + T) } \\ &= R(S(x)) + R(S(x)) &\\quad \\text{ R is linear mapping} \\ &= (R \u25e6 S)(x) + (R \u25e6 T)(x) &\\quad \\text{by definition of R \u25e6 S and R \u25e6 T } \\end{align*} $$ $$ \\begin{align*} ((\u03b1S) \u25e6 T)(x) &= (\u03b1S)(T(x)) &\\quad \\text{by definition of <script type=\"math/tex\">(\u03b1S) \u25e6 T } \\ &= \u03b1 (S(T(x))) &\\quad \\text{by definition of (\u03b1S) } \\ &= S(\u03b1T(x)) &\\quad \\text{ S is linear} \\ &= S((\u03b1T)(x)) &\\quad \\text{by definition of (\u03b1T) } \\ &= (S\u25e6(\u03b1T))(x) &\\quad \\text{by definition of S\u25e6(\u03b1T) } \\\\ \\end{align*} $$ $$ \\begin{align*} (\\text{id} \u25e6 A) (x) &= \\text{id}(A(x)) &\\quad \\text{by definition of id <script type=\"math/tex\"> \u25e6 A } \\\\ &= A(x) &\\quad \\text{by definition of id} \\\\ \\end{align*} $$ 3.2.4. (If you have not yet worked Exercise 3.1.14 then do so before working this exercise.) Let A = [a_{ij}]_{m \\times n} \\in M_{m, n}(\\mathbb{R}) be the matrix of S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) Its transpose A^T \\in M_{n, m}(\\mathbb{R}) is the matrix of the transpose mapping S^T . Since S and S^T act respectively as multiplication by A and A^T , the characterizing property of S from Exercise 3.1.14 gives \u27e8x,A^T y\u27e9 = \u27e8Ax,y\u27e9 \\quad \\text{for all } x \\in \\mathbb{R}^n \\text{ and } y \\in \\mathbb{R}^m. Make specific choices of x and y to show that the transpose A^T \\in M_{m, n}(\\mathbb{R}) is obtained by flipping A about its northwest\u2013southeast diagonal; that is, show that the (i,j) th entry of A^T is a_{ji} . It follows that the rows of A^T are the columns of A , and the columns of A^T are the rows of A . Proof : Given any 1 \\leq p \\leq m, 1 \\leq q \\leq n , let x = e_p \\in \\mathbb{R}^n, y = e_q \\in \\mathbb{R}^m . Let A = [a_{ij}]_{m \\times n}, A^T = [b_{ij}]_{n \\times m} . Then A^T y = A^T e_q = j \\text{th column of }A^T , then \u27e8x,A^T y\u27e9 = b_{ij} . On the other hand, Ax = i \\text{th column of }A , then \u27e8Ax,y\u27e9 = a_{ji} . So we proved. (Similarly, let B \\in M_{n, p}(\\mathbb{R}) be the matrix of T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) so that B^T is the matrix of T^T . Because matrix multiplication is compatible with linear mapping composition, we know immediately from Exercise 3.1.14(b), with no reference to the concrete description of the matrix transposes A^T and B^T in terms of the original matrices A and B , that the transpose of the product is the product of the transposes in reverse order, (AB)^T = B^T A^T \\quad \\text{for all } A \\in M_{m, n}(\\mathbb{R}) \\text{ and } B \\in M_{n, p}(\\mathbb{R}). That is, by characterizing the transpose mapping in Exercise 3.1.14, we easily derived the construction of the transpose matrix here and obtained the formula for the product of transpose matrices with no reference to their construction.) \\square 3.2.5. The trace of a square matrix A \\in M_{n}(\\mathbb{R}) is the sum of its diagonal elements, \\text{tr}(A) = \\sum_{i = 1}^{n} a_{ii} Show that \\text{tr}(AB) = \\text{tr}(BA), \\quad A,B \\in M_{n}(\\mathbb{R}) Proof : \\text{tr}(AB) = \\sum_{i = 1}^{n} c_{ii} \\\\ = \\sum_{i = 1}^{n} \\left( \\sum_{j = 1}^{n} a_{ij} b_{ji} \\right) \\\\ = \\sum_{i = 1}^{n} \\sum_{j = 1}^{n} a_{ij} b_{ji} \\\\ = \\sum_{j = 1}^{n} \\sum_{i = 1}^{n} b_{ji} a_{ij} \\\\ = \\sum_{j = 1}^{n} \\left( \\sum_{i = 1}^{n} b_{ji} a_{ij} \\right) \\\\ = \\text{tr}(BA) \\square 3.2.6 For every matrix A \\in M_{m, n}(\\mathbb{R}) and column vector a \\in \\mathbb{R}^m , define the a\ufb03ne mapping (cf. Exercise 3.1.15) \\text{Aff}_{A, a} : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m by the rule \\text{Aff}_{A, a} = Ax + a for all x \\in \\mathbb{R}^n , viewing x as a column vector. (a) Explain why every a\ufb03ne mapping from \\mathbb{R}^n to \\mathbb{R}^m takes this form. Proof : An affine mapping is this form: f(x) = T(x) + b , T \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and b \\in \\mathbb{R}^m . Since T \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) , then we can find matrix A \\in M_{m, n}(\\mathbb{R}) such that T(x) = A(x) . So \\text{Aff}_{A, a} = Ax + a . \\square (b) Given such A and a , define the matrix A' \\in M_{{m+1}, {n+1}}(\\mathbb{R}) to be A' = \\begin{bmatrix} A & a \\\\ \\bold{0}_n & 1 \\\\ \\end{bmatrix}. Show that for all x \u2208 \\mathbb{R}^n , A' \\begin{bmatrix} x \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} \\text{Aff}_{A, a}(x) \\\\ 1 \\end{bmatrix}. Thus, a\ufb03ne mappings, like linear mappings, behave as matrix-by-vector multiplications but where the vectors are the usual input and output vectors augmented with an extra \u201c1\u201d at the bottom. Proof : For e_j, j < n+1 , A' e_j = \\begin{bmatrix} A_j \\\\ 0 \\\\ \\end{bmatrix} \\\\ A' e_{n+1} = \\begin{bmatrix} a \\\\ 1 \\\\ \\end{bmatrix} Let x' = \\begin{bmatrix} x \\\\ 1 \\\\ \\end{bmatrix} Then x' = \\sum_{j = 1}^{n} x_j e_j + e_{n+1} Then A'x' = \\sum_{j = 1}^{n} x_j A' e_j + A' e_{n+1} \\\\ = \\sum_{j = 1}^{n} x_j A_j + \\begin{bmatrix} a \\\\ 1 \\\\ \\end{bmatrix} \\\\ = \\sum_{j = 1}^{n} \\begin{bmatrix} A_j x_j \\\\ 0 \\\\ \\end{bmatrix} + \\begin{bmatrix} a \\\\ 1 \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \\text{Aff}_{A, a}(x) \\\\ 1 \\end{bmatrix} \\square (c) The a\ufb03ne mapping \\text{Aff}_{B, b} : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^n determined by B \\in M_{n, p}(\\mathbb{R}) and b \\in \\mathbb{R}^n has matrix B' = \\begin{bmatrix} B & b \\\\ \\bold{0}_p & 1 \\\\ \\end{bmatrix} Show that \\text{Aff}_{A, a} \\circ \\text{Aff}_{B, b} : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m has matrix A'B' . That is, matrix multiplication is compatible with composition of a\ufb03ne mappings. Proof : First we know B'x = \\begin{bmatrix} \\text{Aff}_{B, b}(x) \\\\ 1 \\end{bmatrix} Then $$ \\begin{align*} (A'B')x &= A' (B'x) &\\quad \\text{Properties of matrix multiplication} \\\\ &= A' \\begin{bmatrix} \\text{Aff}_{B, b}(x) \\\\ 1 \\end{bmatrix} &\\quad \\text{Compute <script type=\"math/tex\">B'x from (b)} \\\\ &= \\begin{bmatrix} \\text{Aff}_{A, a} ( \\text{Aff}_{B, b}(x) ) \\\\ 1 \\\\ \\end{bmatrix} &\\quad \\text{Again from (b)} \\\\ &= \\begin{bmatrix} (\\text{Aff}_{A, a} \\circ \\text{Aff}_{B, b})(x) \\\\ 1 \\end{bmatrix} &\\quad \\text{Definition of composition of maps.} \\end{align*} $$ \\square 3.2.7. The exponential of a square matrix A is the infinite matrix sum e^A = I + A + \\frac{1}{2!} A^2 + \\frac{1}{3!} A^3 + \\cdots Compute the exponentials of the following matrices. Solution : (a) A = [\u03bb] e^A = [e^\u03bb] \\square (b) A = \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} $$ A^2 = \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} \\ = \\begin{bmatrix} \u03bb^2 & 2\u03bb \\\\ 0 & \u03bb^2 \\\\ \\end{bmatrix} $$ A^3 = \\begin{bmatrix} \u03bb^2 & 2\u03bb \\\\ 0 & \u03bb^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 \\\\ 0 & \u03bb^3 \\\\ \\end{bmatrix} Then e^A = \\begin{bmatrix} e^\u03bb & e^\u03bb \\\\ 0 & e^\u03bb \\\\ \\end{bmatrix} \\square (c) A = \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} A^2 = \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 \\\\ 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} A^3 = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 \\\\ 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} A^4 = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^4 & 4\u03bb^3 & 6\u03bb^2 \\\\ 0 & \u03bb^4 & 4\u03bb^3 \\\\ 0 & 0 & \u03bb^4 \\\\ \\end{bmatrix} So e^A = \\begin{bmatrix} e^\u03bb & e^\u03bb & e^\u03bb/2 \\\\ 0 & e^\u03bb & e^\u03bb \\\\ 0 & 0 & e^\u03bb \\\\ \\end{bmatrix} \\square (d) A = \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} A^2 = \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 & 0 \\\\ 0 & \u03bb^2 & 2\u03bb & 1 \\\\ 0 & 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} So \\frac{1}{2!} A^2 = \\begin{bmatrix} \\frac{1}{2!} \u03bb^2 & \u03bb & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2!} \u03bb^2 & \u03bb & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2!} \u03bb^2 & \u03bb \\\\ 0 & 0 & 0 & \\frac{1}{2!} \u03bb^2 \\\\ \\end{bmatrix} Then A^3 = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 & 0 \\\\ 0 & \u03bb^2 & 2\u03bb & 1 \\\\ 0 & 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb & 1 \\\\ 0 & \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} So \\frac{1}{3!} A^3 = \\begin{bmatrix} \\frac{1}{3!} \u03bb^3 & \\frac{1}{2!} \u03bb^2 & \\frac{1}{2} \u03bb & \\frac{1}{3!} \\\\ 0 & \\frac{1}{3!} \u03bb^3 & \\frac{1}{2!} \u03bb^2 & \\frac{1}{2} \u03bb \\\\ 0 & 0 & \\frac{1}{3!} \u03bb^3 & \\frac{1}{2!} \u03bb^2 \\\\ 0 & 0 & 0 & \\frac{1}{3!} \u03bb^3 \\\\ \\end{bmatrix} Then A^4 = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb & 1 \\\\ 0 & \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^4 & 4 \u03bb^3 & 6 \u03bb^2 & 4 \u03bb \\\\ 0 & \u03bb^4 & 4 \u03bb^3 & 6 \u03bb^2 \\\\ 0 & 0 & \u03bb^4 & 4 \u03bb^3 \\\\ 0 & 0 & 0 & \u03bb^4 \\\\ \\end{bmatrix} \\\\ So \\frac{1}{4!} A^4 = \\begin{bmatrix} (1/4!)\u03bb^4 & (1/3!) \u03bb^3 & (1/2)(1/2!) \u03bb^2 & (1/3!) \u03bb \\\\ 0 & (1/4!)\u03bb^4 & (1/3!) \u03bb^3 & (1/2)(1/2!) \u03bb^2 \\\\ 0 & 0 & (1/4!)\u03bb^4 & (1/3!) \u03bb^3 \\\\ 0 & 0 & 0 & (1/4!)\u03bb^4 \\\\ \\end{bmatrix} \\\\ Then finally e^A = \\begin{bmatrix} e^\u03bb & e^\u03bb & \\frac{1}{2!} e^\u03bb & \\frac{1}{3!} e^\u03bb \\\\ 0 & e^\u03bb & e^\u03bb & \\frac{1}{2!} e^\u03bb \\\\ 0 & 0 & e^\u03bb & e^\u03bb \\\\ 0 & 0 & 0 & e^\u03bb \\\\ \\end{bmatrix} \\\\ \\square 3.2.8. Let a,b,d be real numbers with ad = 1 . show that \\begin{bmatrix} a & b \\\\ 0 & d \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 & ab \\\\ 0 & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a & 0 \\\\ 0 & d \\\\ \\end{bmatrix} . Proof : Just regular matrix multiplication. (b) Let a,b,c,d be real numbers with c \\neq 0 and ad\u2212bc = 1 . Show that $$ $$ \\square 3.3 The Inverse of a Linear Mapping 3.3.2. Finish the proof of Proposition 3.3.2. Proof : To prove (b), each row r_j of S_{i,a} M is the row r_i of M . And r_i of S_{i,a} M is a r_i . \\square 3.3.4. Finish the proof of Lemma 3.3.3, part (1). Proof : We need to prove S_{i,a}^{-1} = S_{i,a^{-1}} Note S_{i,a} is the identity matrix I_m with a times it's i th row, and multiplying this from the left by S_{i,a^{-1}} makes the i th row back to be (0,\\cdots , 1, \\cdots 0) . We also need to prove T_{i;j}^{-1} = T_{i;j} Note T_{i;j} is the identity matrix I_m with swapping its i th row with its j th row, and multiplying this from the left by T_{i;j} makes it back to I_m . 3.3.5. What is the e\ufb00ect of right multiplying the m\u00d7n matrix M by an n\u00d7n matrix R_{i;j,a}, S_{i,a}, T_{i;j} ? Solution : We can directly carry out the matrix multiplication and the effect is described in 3.3.6. 3.3.6. Recall the transpose of a matrix M (cf. Exercise 3.2.4), denoted M^T . Prove R_{i;j,a}^T = R_{j;i,a}M ; S_{i,a}^T = S_{i,a} ; T_{i;j}^T = T_{i;j} . Use these results and the formula (AB)^T = B^TA^T to redo the previous problem. Proof : Let A = [a_{ij}]_{m \\times n}, A^T = [b_{ij}]_{n \\times m} . Then we know, a_{ij} = b_{ji} . So then it's easy to prove. For the second part (A R_{i;j,a})^T = R_{i;j,a}^T A^T = R_{j;i,a} A^T So the effect is to use a times i th column and add to the j th column. (A S_{i,a})^T = S_{i,a} A^T So the effect is to use a times i th column of A . Similar, the effect is to swap the i th and j th column of A . 3.3.7. Are the following matrices echelon? For each matrix M , solve the equation Mx= 0 . Solution : (a) no, the last column is not a neither a new column nor an old column. (b) (c) yes. (d) No. The 2nd/3rd column should be swapped with the 1st row. (e) The 3rd row should be subtract from the 2nd row. (f) The 1st and 2nd row should be swapped. 3.3.8. For each matrix A solve the equation Ax= 0 . (a) \\begin{bmatrix} -1 & 1 & 4 \\\\ 1 & 3 & 8 \\\\ 1 & 2 & 5 \\\\ \\end{bmatrix} Solution : Multiply by R_{2;1,1} we got \\begin{bmatrix} -1 & 1 & 4 \\\\ 0 & 4 & 12 \\\\ 1 & 2 & 5 \\\\ \\end{bmatrix} Multiply by R_{3;1,1} we got \\begin{bmatrix} -1 & 1 & 4 \\\\ 0 & 4 & 12 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by S_{1,-1} we got \\begin{bmatrix} 1 & -1 & -4 \\\\ 0 & 4 & 12 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by R_{1;3,1/3} we got \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 4 & 12 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by R_{2;3,-4/3} we got \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by T_{2;3} we got \\begin{bmatrix} 1 & 0 & -3 \\\\ 0 & 3 & 9 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} Multiply by S_{2,1/3} we got \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} Then x = (1, -3, 1) can be one solution. \\square 3.3.9. Balance the chemical equation \\text{Ca} + \\text{H}_3 \\text{PO}_4 \\rightarrow \\text{Ca}_3 \\text{P}_2 \\text{O}_8 + \\text{H}_2 Solution \\begin{bmatrix} 1 & 0 & -3 & 0 \\\\ 0 & 3 & 0 & -2 \\\\ 0 & 1 & -2 & 0 \\\\ 0 & 4 & -8 & 0 \\\\ \\end{bmatrix} So (3, 2, 1, 3) is a solution. 3 \\text{Ca} + 2\\text{H}_3 \\text{PO}_4 \\rightarrow 1 \\text{Ca}_3 \\text{P}_2 \\text{O}_8 + 3 \\text{H}_2 \\square 3.3.10. Prove by induction that the only square echelon matrix with all new columns is the identity matrix. Proof : Assume this is true for n \\times n , i.e. E_n = I_n . Consider (n+1) \\times (n+1) echelon matrix. Since the last column is a new column, then E_{n+1} = \\begin{bmatrix} A & 0 \\\\ 0 & 1 \\\\ \\end{bmatrix} If any column of A is not a new column, then it does not have a leading 1 's. Then that column does not have a leading 1 's in E_{n+1} either. So it's not a new column in E_{n+1} . We have a contradiction. So every column in A is a new column, and since its size is n \\times n , then A = I_n . So E_{n+1} = I_{n+1} . \\square 3.3.11. Are the following matrices invertible? Find the inverse when possible, and then check your answer. Solution : We check (b) B = [P | I] = \\\\ \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 4 & -1 & 2 & 0 & 1 & 0 \\\\ 6 & 4 & 1 & 0 & 0 & 1 \\\\ \\end{bmatrix} Apply R_{2;1,-2} \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 0 & -11 & 4 & -2 & 1 & 0 \\\\ 6 & 4 & 1 & 0 & 0 & 1 \\\\ \\end{bmatrix} Apply R_{3;1,-3} \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 0 & -11 & 4 & -2 & 1 & 0 \\\\ 0 & -11 & 4 & -3 & 0 & 1 \\\\ \\end{bmatrix} Apply R_{3;2,-1} \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 0 & -11 & 4 & -2 & 1 & 0 \\\\ 0 & 0 & 0 & -1 & -1 & 1 \\\\ \\end{bmatrix} So it's not invertible. \\square 3.3.12. The matrix A is called lower triangular if a_{ij} = 0 whenever i < j . If A is a lower triangular square matrix with all diagonal entries equal to 1 , show that A is invertible and A^{\u22121} takes the same form. Proof : We can mutiply A with R_{2;1,-a_{2,1}}, \\cdots R_{n;1,-a_{n,1}} to get the first column to become a new column, let the new matrix be A_1 . And the right half be B_1 . Then B_1 is still a lower triangular. Similarly, we can mutiply A_1 with R_{3;2,-a_{3,2}}, \\cdots R_{n;2,-a_{n,2}} to get A_2 and B_2 which are also lower triangular. Eventually, we can get A_n = I_n and B_n , which are also lower triangular. \\square 3.3.13. This exercise refers back to the Gram\u2013Schmidt exercise in Chapter 2. That exercise expresses the relation between the vectors \\{x'_j\\} and the vectors \\{x_j\\} formally as x' = Ax , where x' is a column vector whose entries are the vectors x'_1, \\cdots, x'_n , x is the corresponding column vector of x_j \u2019s, and A is an n\u00d7n lower triangular matrix. Show that each x_j has the form x_j = a'_{j1} x'_1 + \\cdots + a'_{j,j-1} x'_{j-1} + x'_j and thus every linear combination of the original \\{x_j\\} is also a linear combination of the new \\{x'_j\\} . Proof : Since A is lower triangular matrix, from 3.3.12, A is invertible, and A^{-1} is also a lower triangular matrix, such that x = A^{-1} x' If we expand it, we have x_j = a'_{j1} x'_1 + \\cdots + a'_{j,j-1} x'_{j-1} + x'_j \\square","title":"Chapter 03 Exercises"},{"location":"ch03ex/#chapter-3-linear-mappings-and-their-matrices","text":"","title":"Chapter 3 Linear Mappings and Their Matrices"},{"location":"ch03ex/#31-linear-mappings","text":"","title":"3.1 Linear Mappings"},{"location":"ch03ex/#311","text":"Prove that T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is linear if and only if it satisfies (3.1) and (3.2). (It may help to rewrite (3.1) with the symbols x_1 and x_2 in place of x and y . Then prove one direction by showing that (3.1) and (3.2) are implied by the defining condition for linearity, and prove the other direction by using induction to show that (3.1) and (3.2) imply the defining condition. Note that as pointed out in the text, one direction of this argument has a bit more substance than the other.) Proof : \\Rightarrow Use induction, when k = 1 T(\u03b1_1 x_1) = \u03b1_1 T(x_1) This is (3.2). Then assume k = n holds, we are checking k = n + 1 . \\begin{align*} T(\\sum_{i = 1}^{n+1} \u03b1_i x_i) &= T((\\sum_{i = 1}^{n} \u03b1_i x_i) + \u03b1_{n+1} x_{n+1}) \\\\ &= T((\\sum_{i = 1}^{n} \u03b1_i x_i)) + T(\u03b1_{n+1} x_{n+1}) \\qquad \\text{Use (3.1)}\\\\ &= T((\\sum_{i = 1}^{n} \u03b1_i x_i)) + \u03b1_{n+1} T(x_{n+1}) \\qquad \\text{Use (3.2)}\\\\ &= \\sum_{i = 1}^{n}\u03b1_kT(x_k) + \u03b1_{n+1} T(x_{n+1}) \\qquad \\text{Use induction}\\\\ &= \\sum_{i = 1}^{n+1}\u03b1_kT(x_k)\\\\ \\end{align*} \\Leftarrow To prove (3.1), set k = 2, \u03b1_1 = \u03b1_2 = 1 . To prove (3.2), set k = 1 . \\square","title":"3.1.1."},{"location":"ch03ex/#312","text":"Suppose that T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is linear. Show that T(0_n) = 0_m . (An intrinsic argument is nicer.) Proof : Note in both \\mathbb{R}^n, \\mathbb{R}^m , 0 v = 0 . So T(0) = T(0 v_n) = 0 T(v_n) = 0 v_m = 0 \\square","title":"3.1.2."},{"location":"ch03ex/#313","text":"Fix a vector a \u2208 \\mathbb{R} . Show that the mapping f : \\mathbb{R}^n \\longrightarrow \\mathbb{R} given by T(x) = \u27e8a,x\u27e9 is linear, and that T(e_j) = a_j for j = 1,...,n . Proof : (3.1) T(x + y) = \u27e8a,x+y\u27e9 = \u27e8a,x\u27e9 + \u27e8a,y\u27e9 = T(x) + T(y) (3.2) T(\u03b1x) = \u27e8a,\u03b1x\u27e9 = \u03b1 \u27e8a,x\u27e9 = \u03b1T(x) T(e_j) = a_j follows the definition of the inner product. \\square","title":"3.1.3"},{"location":"ch03ex/#314","text":"Find the linear mapping T : \\mathbb{R}^3 \\longrightarrow \\mathbb{R} such that T(0,1,1) = 1 , T(1,0,1) = 2 , and T(1,1,0) = 3 . Solution We need to solve this equations \\begin{cases} 0x + 1y + 1z = 1 &\\text{}\\\\ 1x + 0y + 1z = 2 &\\text{}\\\\ 1x + 1y + 0z = 3 &\\text{}\\\\ \\end{cases} So T(x) = \u27e8a,x\u27e9, a = (2, 1, 0) . \\square","title":"3.1.4"},{"location":"ch03ex/#315","text":"Complete the proof of the componentwise nature of linearity. Proof : For \u03b1 \\in \\mathbb{R}, x \\in \\mathbb{R}^n If T_1, \\cdots, T_m are linear, then T(\u03b1x) = (T_1(\\alpha x), \\cdots, T_m(\\alpha x)) \\\\ = (\u03b1T_1(x), \\cdots, \u03b1T_m(x)) \\\\ = \u03b1 (T_1(x), \\cdots, T_m(x)) \\\\ = \u03b1 T(x) On the other hand, if T is linear, Then \\begin{split} (T_1(\\alpha x), \\cdots, T_m(\\alpha x)) &= T(\u03b1x) \\\\ &= \u03b1 T(x) \\\\ &= \u03b1 (T_1(x), \\cdots, T_m(x)) \\\\ &= (\u03b1T_1(x), \\cdots, \u03b1T_m(x)) \\\\ \\end{split} So T_i(\u03b1x) = \u03b1 T_i(x) \\square","title":"3.1.5."},{"location":"ch03ex/#316","text":"Carry out the matrix-by-vector multiplications Solution : (a) [1,3,6] (b) (ax+by, cx+dy, ex+fy) (c) (x_1 y_1, \\cdots, x_n y_n) (d) (0,0,0) \\square","title":"3.1.6."},{"location":"ch03ex/#317","text":"Prove that the identity mapping \\text{id} : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n is linear. What is its matrix? Explain. Proof (3.1) \\text{id}(x+y) = x+y = \\text{id}(x) + \\text{id}(y) (3.2) \\text{id}(\u03b1x) = \u03b1x = \u03b1 \\text{id}(x) Then matrix is \\begin{bmatrix} 1 &0 &\\cdots &0 \\\\ 0 &1 &\\cdots &0 \\\\ \\vdots &\\vdots &\\ddots &\\vdots \\\\ 0 &0 &\\cdots &1 \\\\ \\end{bmatrix} \\square","title":"3.1.7."},{"location":"ch03ex/#318","text":"Let \u03b8 denote a fixed but generic angle. Argue geometrically that the mapping R : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^n given by counterclockwise rotation by \u03b8 is linear, and then find its matrix. Proof : Geometrically is quite obvious. Consider R(e_1) = (\\cos \u03b8, \\sin \u03b8) \\\\ R(e_2) = (-\\sin \u03b8, \\cos \u03b8) \\\\ Then matrix is \\begin{bmatrix} \\cos \u03b8 & -\\sin \u03b8 \\\\ \\sin \u03b8 & \\cos \u03b8 \\\\ \\end{bmatrix} \\square","title":"3.1.8."},{"location":"ch03ex/#319","text":"Show that the mapping Q : \\mathbb{R}^2 \\longrightarrow \\mathbb{R}^2 given by reflection through the x-axis is linear. Find its matrix. R(e_1) = (1, 0) \\\\ R(e_2) = (0, -1) \\\\ Then matrix is \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\\\ \\end{bmatrix} \\square","title":"3.1.9."},{"location":"ch03ex/#3110","text":"Show that the mapping P : \\mathbb{R}^2 \\longrightarrow \\mathbb{R}^2 given by orthogonal projection onto the diagonal line x = y is linear. Find its matrix. (See Exercise 2.2.15.) Proof : We can use vector d = (1,1) to represent the diagonal line x = y . Then for x, y \\in \\mathbb{R}^2 P(x) = \\frac{\u27e8d,x\u27e9}{|d|^2} d \\\\ P(y) = \\frac{\u27e8d,y\u27e9}{|d|^2} d \\\\ P(x+y) = \\frac{\u27e8d,x+y\u27e9}{|d|^2} d \\\\ = \\frac{\u27e8d,x\u27e9}{|d|^2} d + \\frac{\u27e8d,y\u27e9}{|d|^2} d = P(x) + P(y) Also P(\u03b1x) = \\frac{\u27e8d,\u03b1x\u27e9}{|d|^2} d = \u03b1\\frac{\u27e8d,x\u27e9}{|d|^2} d = \u03b1 P(x) So P is linear. R(e_1) = (\\sqrt[]{2}/2, \\sqrt[]{2}/2) \\\\ R(e_2) = (\\sqrt[]{2}/2, \\sqrt[]{2}/2) \\\\ Then matrix is \\begin{bmatrix} \\sqrt[]{2}/2 & \\sqrt[]{2}/2 \\\\ \\sqrt[]{2}/2 & \\sqrt[]{2}/2 \\\\ \\end{bmatrix} \\square","title":"3.1.10."},{"location":"ch03ex/#3111","text":"Draw the graph of a generic linear mapping from \\mathbb{R}^2 to \\mathbb{R}^3 . Solution : skip.","title":"3.1.11."},{"location":"ch03ex/#3112","text":"Continue the proof of Proposition 3.1.8 by proving the other three statements about S + T and aS satisfying (3.1) and (3.2). Proof : (3.2) for S+T . (S+T)(\u03b1x) = S(\u03b1x) + T(\u03b1x) = \u03b1S(x) + \u03b1T(x) = \u03b1 (S(x) + T(x)) = \u03b1 (S+T)(x) Next, (3.1) for aS . \\begin{align*} (aS)(x+y) &= a(S(x + y)) & \\quad \\text{definition of the } aS \\\\ &= a(S(x) + S(y)) & \\quad \\text{S is linear} \\\\ &= aS(x) + aS(y) & \\quad \\text{Vector space axioms D2} \\\\ &= (aS)(x) + (aS)(y) & \\quad \\text{definition of the } aS \\\\ \\end{align*} Next, (3.2) for aS . \\begin{aligned} (aS)(\u03b1x) &= a(S(\u03b1x)) & \\quad \\text{definition of the } aS \\\\ &= a(\u03b1S(x)) & \\quad \\text{S is linear} \\\\ &= \u03b1(aS(x)) & \\quad \\mathbb{R} \\text{ is a field} \\\\ &= \u03b1((aS)(x)) & \\quad \\text{definition of the } aS \\\\ \\end{aligned} \\square","title":"3.1.12."},{"location":"ch03ex/#3113","text":"If S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) , show that S \\circ T : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m lies in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^m) . Proof : (3.1) \\begin{align*} (S \\circ T)(x+y) &= S(T(x+y)) \\\\ &= S(T(x) + T(y)) \\\\ &= (S \\circ T)(x) + (S \\circ T)(y) \\\\ \\end{align*} Next (3.2) \\begin{align*} (S \\circ T)(\u03b1x) &= S(T(\u03b1x)) \\\\ &= S(\u03b1T(x)) \\\\ &= \u03b1(S(T(x))) \\\\ &= \u03b1((S \\circ T)(x)) \\\\ \\end{align*} So S \\circ T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^m) .","title":"3.1.13."},{"location":"ch03ex/#3114","text":"(a) Let S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) . Its transpose is the mapping S^T : \\mathbb{R}^m \\longrightarrow \\mathbb{R}^n defined by the characterizing condition \u27e8x,S^T(y)\u27e9 = \u27e8S(x),y\u27e9 \\qquad \\text{for all } x \\in \\mathbb{R}^n \\text{ and } y \\in \\mathbb{R}^m. Granting that indeed a unique such S^T exists, use the characterizing condition to show that S^T(y+y') = S^T(y) + S^T(y') \\qquad \\text{for all } y, y' \\in \\mathbb{R}^m by showing that \u27e8x,S^T(y+y')\u27e9 = \u27e8x,S^T(y)+S^T(y')\u27e9 \\qquad \\text{for all } x \\in \\mathbb{R}^n \\text{ and } y, y' \\in \\mathbb{R}^m. Proof : \\begin{align*} \u27e8x,S^T(y+y')\u27e9 &= \u27e8S(x),y+y'\u27e9 \\\\ &= \u27e8S(x),y\u27e9 + \u27e8S(x),y'\u27e9 \\\\ &= \u27e8x,S^T(y)\u27e9 + \u27e8x,S^T(y')\u27e9 \\\\ &= \u27e8x,S^T(y) + S^T(y')\u27e9 \\\\ \\end{align*} Because x can be arbitrary, so we have S^T(y+y') = S^T(y) + S^T(y') \\qquad \\text{for all } y, y' \\in \\mathbb{R}^m \\square (b) Keeping S from part (a), now further introduce T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) , so that also S \\circ T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^m) . Show that the transpose of the composition is the composition of the transposes in reverse order, (S \u25e6T)^T = T^T \u25e6S^T, by showing that \u27e8x,(S \u25e6T)^T(z)\u27e9 = \u27e8x, (T^T \u25e6S^T)(z)\u27e9 Proof : \\begin{align*} \u27e8x,(S \u25e6T)^T(z)\u27e9 &= \u27e8(S\u25e6T)(x), z\u27e9 \\\\ &= \u27e8S(T(x)), z\u27e9 \\\\ &= \u27e8T(x), S^T(z)\u27e9 \\\\ &= \u27e8x, T^T (S^T(z))\u27e9 \\\\ &= \u27e8x, (T^T \u25e6 S^T)(z)\u27e9 \\\\ \\end{align*} \\square","title":"3.1.14"},{"location":"ch03ex/#3115","text":"A mapping f : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is called a\ufb03ne if it has the form f(x) = T(x) + b , where T \u2208 \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and b \u2208 \\mathbb{R}^m . State precisely and prove: the composition of a\ufb03ne mappings is a\ufb03ne. Proof : If f : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^n and g : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m are affine, then g \\circ f : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m \\quad \\text{ is affine}. \\begin{align*} g(f(x)) &= g(T(x) + b) \\\\ &= g(T(x)) + g(b) \\\\ &= S(T(x)) + c + g(b) \\\\ \\end{align*} \\square","title":"3.1.15"},{"location":"ch03ex/#3116","text":"Let T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m be a linear mapping. Note that since T is continuous and since the absolute value function on \\mathbb{R}^m is continuous, the composite function |T| : \\mathbb{R}^n \\longrightarrow \\mathbb{R} is continuous. (a) Let S= \\{x \u2208 \\mathbb{R}^n : |x| = 1\\} . Explain why S is a compact subset of \\mathbb{R}^n . Explain why it follows that |T| takes a maximum value c on S . Proof : S is bounded. Then we just need to show S is closed. Use \"Sequential characterization of closed sets\", assume \\{x_\u03bd\\} \\rightarrow p \\in \\mathbb{R}^n . Since || is a continuous function and \\lim_{\\nu \\to \\infty} |x_\u03bd| = 1 Then |p| = 1 . So p \\in S , then S is closed. So S is compact. Then based on Theorem 2.4.15 (Extreme value theorem), |T| takes a maximum value c on S . \\square (b) Show that |T(x)| \u2264 c|x| for all x \u2208 \\mathbb{R}^n . This result is the linear magnification boundedness lemma. We will use it in Chapter 4. Proof : x = |x|\\frac{x}{|x|}\\\\ \\begin{align*} T(x) &= T(|x|\\frac{x}{|x|}) \\\\ &= |x|T(\\frac{x}{|x|}) &\\qquad T\\text{ is linear} \\\\ &\\leq c|x| &\\qquad \\left| \\frac{x}{|x|} \\right| = 1 \\\\ \\end{align*} \\square","title":"3.1.16"},{"location":"ch03ex/#3117","text":"Let T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m be a linear mapping. (a) Explain why the set D = \\{x \u2208 \\mathbb{R}^n : |x| = 1\\} is compact. Proof : See 3.1.16 (a) (b) Use part (a) of this exercise and part (b) of the preceding exercise to explain why therefore the set \\{|T(x)| : x \u2208 D\\} has a maximum. This maximum is called the norm of T and is denoted \\left\\| T \\right\\| . Proof : See 3.1.16 (b). (c) Explain why \\left\\| T \\right\\| is the smallest value K that satisfies the condition from part (b) of the preceding exercise, |T(x)| \u2264 K|x| for all x \u2208 \\mathbb{R}^n . Proof : First, we know from 3.1.16 (b), |T(x)| \u2264 \\left\\| T \\right\\| |x| . Next, we can find a point c \\in D such that |T(c)| = \\left\\| T \\right\\| |c| = \\left\\| T \\right\\| \\square (d) Show that for every S,T \u2208 \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and every a \u2208 \\mathbb{R} , \\left\\| S + T \\right\\| \\leq \\left\\| S \\right\\| + \\left\\| T \\right\\| \\qquad \\text{and} \\qquad \\left\\| aT \\right\\| = |a|\\left\\| T \\right\\| Define a distance function d: \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) \\times \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) \\longrightarrow \\mathbb{R}, \\quad d(S, T) = \\left\\| S - T \\right\\| Show that this function satisfies the distance properties of Theorem 2.2.8. Proof : \\left\\| S+T \\right\\| = |(S+T)(c)| \\\\ = |S(c) + T(c)| \\\\ \\leq |S(c)| + |T(c)| \\\\ \\leq \\left\\| S \\right\\| + \\left\\| T \\right\\| \\left\\| aT \\right\\| = |(aT)(c)| \\\\ |a(T(c))| = |a| |T(c)| = |a| \\left\\| T \\right\\| \\square (D1) d(S, T) \\geq 0 . Since |(S - T)(x)| \\geq 0 , then \\left\\| S-T \\right\\| \\geq 0 . d(S,T) = 0 if and only if S = T . \\square (D2) d(S,T) = d(T,S) Because |(S-T)(x)| = |(T-S)(x)| \\square (D3) d(R,T) \u2264 d(R,S)+d(S,T) d(R,T) = \\left\\| R - T \\right\\| = |(R-T)(c)| \\\\ = |R(c) - T(c)| \\\\ = |(R(c)-S(c)) + (S(c)-T(c))| \\\\ \\leq |R(c)-S(c)| + |S(c)-T(c)| \\\\ = |(R-S)(c)| + |(S-T)(c)| \\\\ \\leq \\left\\| R-S \\right\\| + \\left\\| S-T \\right\\| \\\\ = d(R,S)+d(S,T) \\square (e) Show that for every S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and every T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) \\left\\| ST \\right\\| \\leq \\left\\| S \\right\\| \\left\\| T \\right\\| Proof : Assume \\left\\| ST \\right\\| = |S(T(c))| Let x = T(c) \\in \\mathbb{R}^n . |S(x)| \\leq \\left\\| S \\right\\| |x| \\\\ |x| = |T(c)| \\leq \\left\\| T \\right\\| |c| = \\left\\| T \\right\\| Therefore \\begin{align*} \\left\\| ST \\right\\| &= |S(T(c))| \\\\ &= |S(x)| \\\\ &\\leq \\left\\| S \\right\\| |x| \\\\ &\\leq \\left\\| S \\right\\| \\left\\| T \\right\\| \\\\ \\end{align*} \\square","title":"3.1.17."},{"location":"ch03ex/#32-operations-on-matrices","text":"","title":"3.2 Operations on Matrices"},{"location":"ch03ex/#321","text":"Justify Definition 3.2.2 of scalar multiplication of matrices. Solution : The j th column of \u03b1A is $$ (\u03b1S)(e_j) = \u03b1 (S(e_j)) = \u03b1 \\times \\text{( j th column of A)} $$ So it's reasonable to define \u03b1A = [\u03b1a_{ij}]_{m \\times n} . \\square","title":"3.2.1."},{"location":"ch03ex/#322","text":"Carry out the matrix multiplications. Solution : skipped.","title":"3.2.2."},{"location":"ch03ex/#323","text":"Prove more of Proposition 3.2.5, that A(B+C) = AB+AC , (\u03b1A)B= A(\u03b1B) , and I_mA= A for suitable matrices A,B,C and any scalar \u03b1 . Proof : $$ \\begin{align*} (R \u25e6 (S + T))(x) &= R((S+T)(x)) &\\quad \\text{by definition of <script type=\"math/tex\">R \u25e6 (S + T) } \\ &= R(S(x)+T(x)) &\\quad \\text{by definition of (S + T) } \\ &= R(S(x)) + R(S(x)) &\\quad \\text{ R is linear mapping} \\ &= (R \u25e6 S)(x) + (R \u25e6 T)(x) &\\quad \\text{by definition of R \u25e6 S and R \u25e6 T } \\end{align*} $$ $$ \\begin{align*} ((\u03b1S) \u25e6 T)(x) &= (\u03b1S)(T(x)) &\\quad \\text{by definition of <script type=\"math/tex\">(\u03b1S) \u25e6 T } \\ &= \u03b1 (S(T(x))) &\\quad \\text{by definition of (\u03b1S) } \\ &= S(\u03b1T(x)) &\\quad \\text{ S is linear} \\ &= S((\u03b1T)(x)) &\\quad \\text{by definition of (\u03b1T) } \\ &= (S\u25e6(\u03b1T))(x) &\\quad \\text{by definition of S\u25e6(\u03b1T) } \\\\ \\end{align*} $$ $$ \\begin{align*} (\\text{id} \u25e6 A) (x) &= \\text{id}(A(x)) &\\quad \\text{by definition of id <script type=\"math/tex\"> \u25e6 A } \\\\ &= A(x) &\\quad \\text{by definition of id} \\\\ \\end{align*} $$","title":"3.2.3"},{"location":"ch03ex/#324","text":"(If you have not yet worked Exercise 3.1.14 then do so before working this exercise.) Let A = [a_{ij}]_{m \\times n} \\in M_{m, n}(\\mathbb{R}) be the matrix of S \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) Its transpose A^T \\in M_{n, m}(\\mathbb{R}) is the matrix of the transpose mapping S^T . Since S and S^T act respectively as multiplication by A and A^T , the characterizing property of S from Exercise 3.1.14 gives \u27e8x,A^T y\u27e9 = \u27e8Ax,y\u27e9 \\quad \\text{for all } x \\in \\mathbb{R}^n \\text{ and } y \\in \\mathbb{R}^m. Make specific choices of x and y to show that the transpose A^T \\in M_{m, n}(\\mathbb{R}) is obtained by flipping A about its northwest\u2013southeast diagonal; that is, show that the (i,j) th entry of A^T is a_{ji} . It follows that the rows of A^T are the columns of A , and the columns of A^T are the rows of A . Proof : Given any 1 \\leq p \\leq m, 1 \\leq q \\leq n , let x = e_p \\in \\mathbb{R}^n, y = e_q \\in \\mathbb{R}^m . Let A = [a_{ij}]_{m \\times n}, A^T = [b_{ij}]_{n \\times m} . Then A^T y = A^T e_q = j \\text{th column of }A^T , then \u27e8x,A^T y\u27e9 = b_{ij} . On the other hand, Ax = i \\text{th column of }A , then \u27e8Ax,y\u27e9 = a_{ji} . So we proved. (Similarly, let B \\in M_{n, p}(\\mathbb{R}) be the matrix of T \\in \\mathcal{L}(\\mathbb{R}^p, \\mathbb{R}^n) so that B^T is the matrix of T^T . Because matrix multiplication is compatible with linear mapping composition, we know immediately from Exercise 3.1.14(b), with no reference to the concrete description of the matrix transposes A^T and B^T in terms of the original matrices A and B , that the transpose of the product is the product of the transposes in reverse order, (AB)^T = B^T A^T \\quad \\text{for all } A \\in M_{m, n}(\\mathbb{R}) \\text{ and } B \\in M_{n, p}(\\mathbb{R}). That is, by characterizing the transpose mapping in Exercise 3.1.14, we easily derived the construction of the transpose matrix here and obtained the formula for the product of transpose matrices with no reference to their construction.) \\square","title":"3.2.4."},{"location":"ch03ex/#325","text":"The trace of a square matrix A \\in M_{n}(\\mathbb{R}) is the sum of its diagonal elements, \\text{tr}(A) = \\sum_{i = 1}^{n} a_{ii} Show that \\text{tr}(AB) = \\text{tr}(BA), \\quad A,B \\in M_{n}(\\mathbb{R}) Proof : \\text{tr}(AB) = \\sum_{i = 1}^{n} c_{ii} \\\\ = \\sum_{i = 1}^{n} \\left( \\sum_{j = 1}^{n} a_{ij} b_{ji} \\right) \\\\ = \\sum_{i = 1}^{n} \\sum_{j = 1}^{n} a_{ij} b_{ji} \\\\ = \\sum_{j = 1}^{n} \\sum_{i = 1}^{n} b_{ji} a_{ij} \\\\ = \\sum_{j = 1}^{n} \\left( \\sum_{i = 1}^{n} b_{ji} a_{ij} \\right) \\\\ = \\text{tr}(BA) \\square","title":"3.2.5."},{"location":"ch03ex/#326","text":"For every matrix A \\in M_{m, n}(\\mathbb{R}) and column vector a \\in \\mathbb{R}^m , define the a\ufb03ne mapping (cf. Exercise 3.1.15) \\text{Aff}_{A, a} : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m by the rule \\text{Aff}_{A, a} = Ax + a for all x \\in \\mathbb{R}^n , viewing x as a column vector. (a) Explain why every a\ufb03ne mapping from \\mathbb{R}^n to \\mathbb{R}^m takes this form. Proof : An affine mapping is this form: f(x) = T(x) + b , T \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) and b \\in \\mathbb{R}^m . Since T \\in \\mathcal{L}(\\mathbb{R}^n, \\mathbb{R}^m) , then we can find matrix A \\in M_{m, n}(\\mathbb{R}) such that T(x) = A(x) . So \\text{Aff}_{A, a} = Ax + a . \\square (b) Given such A and a , define the matrix A' \\in M_{{m+1}, {n+1}}(\\mathbb{R}) to be A' = \\begin{bmatrix} A & a \\\\ \\bold{0}_n & 1 \\\\ \\end{bmatrix}. Show that for all x \u2208 \\mathbb{R}^n , A' \\begin{bmatrix} x \\\\ 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} \\text{Aff}_{A, a}(x) \\\\ 1 \\end{bmatrix}. Thus, a\ufb03ne mappings, like linear mappings, behave as matrix-by-vector multiplications but where the vectors are the usual input and output vectors augmented with an extra \u201c1\u201d at the bottom. Proof : For e_j, j < n+1 , A' e_j = \\begin{bmatrix} A_j \\\\ 0 \\\\ \\end{bmatrix} \\\\ A' e_{n+1} = \\begin{bmatrix} a \\\\ 1 \\\\ \\end{bmatrix} Let x' = \\begin{bmatrix} x \\\\ 1 \\\\ \\end{bmatrix} Then x' = \\sum_{j = 1}^{n} x_j e_j + e_{n+1} Then A'x' = \\sum_{j = 1}^{n} x_j A' e_j + A' e_{n+1} \\\\ = \\sum_{j = 1}^{n} x_j A_j + \\begin{bmatrix} a \\\\ 1 \\\\ \\end{bmatrix} \\\\ = \\sum_{j = 1}^{n} \\begin{bmatrix} A_j x_j \\\\ 0 \\\\ \\end{bmatrix} + \\begin{bmatrix} a \\\\ 1 \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \\text{Aff}_{A, a}(x) \\\\ 1 \\end{bmatrix} \\square (c) The a\ufb03ne mapping \\text{Aff}_{B, b} : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^n determined by B \\in M_{n, p}(\\mathbb{R}) and b \\in \\mathbb{R}^n has matrix B' = \\begin{bmatrix} B & b \\\\ \\bold{0}_p & 1 \\\\ \\end{bmatrix} Show that \\text{Aff}_{A, a} \\circ \\text{Aff}_{B, b} : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m has matrix A'B' . That is, matrix multiplication is compatible with composition of a\ufb03ne mappings. Proof : First we know B'x = \\begin{bmatrix} \\text{Aff}_{B, b}(x) \\\\ 1 \\end{bmatrix} Then $$ \\begin{align*} (A'B')x &= A' (B'x) &\\quad \\text{Properties of matrix multiplication} \\\\ &= A' \\begin{bmatrix} \\text{Aff}_{B, b}(x) \\\\ 1 \\end{bmatrix} &\\quad \\text{Compute <script type=\"math/tex\">B'x from (b)} \\\\ &= \\begin{bmatrix} \\text{Aff}_{A, a} ( \\text{Aff}_{B, b}(x) ) \\\\ 1 \\\\ \\end{bmatrix} &\\quad \\text{Again from (b)} \\\\ &= \\begin{bmatrix} (\\text{Aff}_{A, a} \\circ \\text{Aff}_{B, b})(x) \\\\ 1 \\end{bmatrix} &\\quad \\text{Definition of composition of maps.} \\end{align*} $$ \\square","title":"3.2.6"},{"location":"ch03ex/#327","text":"The exponential of a square matrix A is the infinite matrix sum e^A = I + A + \\frac{1}{2!} A^2 + \\frac{1}{3!} A^3 + \\cdots Compute the exponentials of the following matrices. Solution : (a) A = [\u03bb] e^A = [e^\u03bb] \\square (b) A = \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} $$ A^2 = \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} \\ = \\begin{bmatrix} \u03bb^2 & 2\u03bb \\\\ 0 & \u03bb^2 \\\\ \\end{bmatrix} $$ A^3 = \\begin{bmatrix} \u03bb^2 & 2\u03bb \\\\ 0 & \u03bb^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 \\\\ 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 \\\\ 0 & \u03bb^3 \\\\ \\end{bmatrix} Then e^A = \\begin{bmatrix} e^\u03bb & e^\u03bb \\\\ 0 & e^\u03bb \\\\ \\end{bmatrix} \\square (c) A = \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} A^2 = \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 \\\\ 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} A^3 = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 \\\\ 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} A^4 = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 \\\\ 0 & \u03bb & 1 \\\\ 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^4 & 4\u03bb^3 & 6\u03bb^2 \\\\ 0 & \u03bb^4 & 4\u03bb^3 \\\\ 0 & 0 & \u03bb^4 \\\\ \\end{bmatrix} So e^A = \\begin{bmatrix} e^\u03bb & e^\u03bb & e^\u03bb/2 \\\\ 0 & e^\u03bb & e^\u03bb \\\\ 0 & 0 & e^\u03bb \\\\ \\end{bmatrix} \\square (d) A = \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} A^2 = \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 & 0 \\\\ 0 & \u03bb^2 & 2\u03bb & 1 \\\\ 0 & 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} So \\frac{1}{2!} A^2 = \\begin{bmatrix} \\frac{1}{2!} \u03bb^2 & \u03bb & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2!} \u03bb^2 & \u03bb & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2!} \u03bb^2 & \u03bb \\\\ 0 & 0 & 0 & \\frac{1}{2!} \u03bb^2 \\\\ \\end{bmatrix} Then A^3 = \\begin{bmatrix} \u03bb^2 & 2\u03bb & 1 & 0 \\\\ 0 & \u03bb^2 & 2\u03bb & 1 \\\\ 0 & 0 & \u03bb^2 & 2\u03bb \\\\ 0 & 0 & 0 & \u03bb^2 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb & 1 \\\\ 0 & \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} So \\frac{1}{3!} A^3 = \\begin{bmatrix} \\frac{1}{3!} \u03bb^3 & \\frac{1}{2!} \u03bb^2 & \\frac{1}{2} \u03bb & \\frac{1}{3!} \\\\ 0 & \\frac{1}{3!} \u03bb^3 & \\frac{1}{2!} \u03bb^2 & \\frac{1}{2} \u03bb \\\\ 0 & 0 & \\frac{1}{3!} \u03bb^3 & \\frac{1}{2!} \u03bb^2 \\\\ 0 & 0 & 0 & \\frac{1}{3!} \u03bb^3 \\\\ \\end{bmatrix} Then A^4 = \\begin{bmatrix} \u03bb^3 & 3\u03bb^2 & 3\u03bb & 1 \\\\ 0 & \u03bb^3 & 3\u03bb^2 & 3\u03bb \\\\ 0 & 0 & \u03bb^3 & 3\u03bb^2 \\\\ 0 & 0 & 0 & \u03bb^3 \\\\ \\end{bmatrix} \\begin{bmatrix} \u03bb & 1 & 0 & 0 \\\\ 0 & \u03bb & 1 & 0 \\\\ 0 & 0 & \u03bb & 1 \\\\ 0 & 0 & 0 & \u03bb \\\\ \\end{bmatrix} \\\\ = \\begin{bmatrix} \u03bb^4 & 4 \u03bb^3 & 6 \u03bb^2 & 4 \u03bb \\\\ 0 & \u03bb^4 & 4 \u03bb^3 & 6 \u03bb^2 \\\\ 0 & 0 & \u03bb^4 & 4 \u03bb^3 \\\\ 0 & 0 & 0 & \u03bb^4 \\\\ \\end{bmatrix} \\\\ So \\frac{1}{4!} A^4 = \\begin{bmatrix} (1/4!)\u03bb^4 & (1/3!) \u03bb^3 & (1/2)(1/2!) \u03bb^2 & (1/3!) \u03bb \\\\ 0 & (1/4!)\u03bb^4 & (1/3!) \u03bb^3 & (1/2)(1/2!) \u03bb^2 \\\\ 0 & 0 & (1/4!)\u03bb^4 & (1/3!) \u03bb^3 \\\\ 0 & 0 & 0 & (1/4!)\u03bb^4 \\\\ \\end{bmatrix} \\\\ Then finally e^A = \\begin{bmatrix} e^\u03bb & e^\u03bb & \\frac{1}{2!} e^\u03bb & \\frac{1}{3!} e^\u03bb \\\\ 0 & e^\u03bb & e^\u03bb & \\frac{1}{2!} e^\u03bb \\\\ 0 & 0 & e^\u03bb & e^\u03bb \\\\ 0 & 0 & 0 & e^\u03bb \\\\ \\end{bmatrix} \\\\ \\square","title":"3.2.7."},{"location":"ch03ex/#328","text":"Let a,b,d be real numbers with ad = 1 . show that \\begin{bmatrix} a & b \\\\ 0 & d \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 & ab \\\\ 0 & 1 \\\\ \\end{bmatrix} \\begin{bmatrix} a & 0 \\\\ 0 & d \\\\ \\end{bmatrix} . Proof : Just regular matrix multiplication. (b) Let a,b,c,d be real numbers with c \\neq 0 and ad\u2212bc = 1 . Show that $$ $$ \\square","title":"3.2.8."},{"location":"ch03ex/#33-the-inverse-of-a-linear-mapping","text":"","title":"3.3 The Inverse of a Linear Mapping"},{"location":"ch03ex/#332","text":"Finish the proof of Proposition 3.3.2. Proof : To prove (b), each row r_j of S_{i,a} M is the row r_i of M . And r_i of S_{i,a} M is a r_i . \\square","title":"3.3.2."},{"location":"ch03ex/#334","text":"Finish the proof of Lemma 3.3.3, part (1). Proof : We need to prove S_{i,a}^{-1} = S_{i,a^{-1}} Note S_{i,a} is the identity matrix I_m with a times it's i th row, and multiplying this from the left by S_{i,a^{-1}} makes the i th row back to be (0,\\cdots , 1, \\cdots 0) . We also need to prove T_{i;j}^{-1} = T_{i;j} Note T_{i;j} is the identity matrix I_m with swapping its i th row with its j th row, and multiplying this from the left by T_{i;j} makes it back to I_m .","title":"3.3.4."},{"location":"ch03ex/#335","text":"What is the e\ufb00ect of right multiplying the m\u00d7n matrix M by an n\u00d7n matrix R_{i;j,a}, S_{i,a}, T_{i;j} ? Solution : We can directly carry out the matrix multiplication and the effect is described in 3.3.6.","title":"3.3.5."},{"location":"ch03ex/#336","text":"Recall the transpose of a matrix M (cf. Exercise 3.2.4), denoted M^T . Prove R_{i;j,a}^T = R_{j;i,a}M ; S_{i,a}^T = S_{i,a} ; T_{i;j}^T = T_{i;j} . Use these results and the formula (AB)^T = B^TA^T to redo the previous problem. Proof : Let A = [a_{ij}]_{m \\times n}, A^T = [b_{ij}]_{n \\times m} . Then we know, a_{ij} = b_{ji} . So then it's easy to prove. For the second part (A R_{i;j,a})^T = R_{i;j,a}^T A^T = R_{j;i,a} A^T So the effect is to use a times i th column and add to the j th column. (A S_{i,a})^T = S_{i,a} A^T So the effect is to use a times i th column of A . Similar, the effect is to swap the i th and j th column of A .","title":"3.3.6."},{"location":"ch03ex/#337","text":"Are the following matrices echelon? For each matrix M , solve the equation Mx= 0 . Solution : (a) no, the last column is not a neither a new column nor an old column. (b) (c) yes. (d) No. The 2nd/3rd column should be swapped with the 1st row. (e) The 3rd row should be subtract from the 2nd row. (f) The 1st and 2nd row should be swapped.","title":"3.3.7."},{"location":"ch03ex/#338","text":"For each matrix A solve the equation Ax= 0 . (a) \\begin{bmatrix} -1 & 1 & 4 \\\\ 1 & 3 & 8 \\\\ 1 & 2 & 5 \\\\ \\end{bmatrix} Solution : Multiply by R_{2;1,1} we got \\begin{bmatrix} -1 & 1 & 4 \\\\ 0 & 4 & 12 \\\\ 1 & 2 & 5 \\\\ \\end{bmatrix} Multiply by R_{3;1,1} we got \\begin{bmatrix} -1 & 1 & 4 \\\\ 0 & 4 & 12 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by S_{1,-1} we got \\begin{bmatrix} 1 & -1 & -4 \\\\ 0 & 4 & 12 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by R_{1;3,1/3} we got \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 4 & 12 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by R_{2;3,-4/3} we got \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ 0 & 3 & 9 \\\\ \\end{bmatrix} Multiply by T_{2;3} we got \\begin{bmatrix} 1 & 0 & -3 \\\\ 0 & 3 & 9 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} Multiply by S_{2,1/3} we got \\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix} Then x = (1, -3, 1) can be one solution. \\square","title":"3.3.8."},{"location":"ch03ex/#339","text":"Balance the chemical equation \\text{Ca} + \\text{H}_3 \\text{PO}_4 \\rightarrow \\text{Ca}_3 \\text{P}_2 \\text{O}_8 + \\text{H}_2 Solution \\begin{bmatrix} 1 & 0 & -3 & 0 \\\\ 0 & 3 & 0 & -2 \\\\ 0 & 1 & -2 & 0 \\\\ 0 & 4 & -8 & 0 \\\\ \\end{bmatrix} So (3, 2, 1, 3) is a solution. 3 \\text{Ca} + 2\\text{H}_3 \\text{PO}_4 \\rightarrow 1 \\text{Ca}_3 \\text{P}_2 \\text{O}_8 + 3 \\text{H}_2 \\square","title":"3.3.9."},{"location":"ch03ex/#3310","text":"Prove by induction that the only square echelon matrix with all new columns is the identity matrix. Proof : Assume this is true for n \\times n , i.e. E_n = I_n . Consider (n+1) \\times (n+1) echelon matrix. Since the last column is a new column, then E_{n+1} = \\begin{bmatrix} A & 0 \\\\ 0 & 1 \\\\ \\end{bmatrix} If any column of A is not a new column, then it does not have a leading 1 's. Then that column does not have a leading 1 's in E_{n+1} either. So it's not a new column in E_{n+1} . We have a contradiction. So every column in A is a new column, and since its size is n \\times n , then A = I_n . So E_{n+1} = I_{n+1} . \\square","title":"3.3.10."},{"location":"ch03ex/#3311","text":"Are the following matrices invertible? Find the inverse when possible, and then check your answer. Solution : We check (b) B = [P | I] = \\\\ \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 4 & -1 & 2 & 0 & 1 & 0 \\\\ 6 & 4 & 1 & 0 & 0 & 1 \\\\ \\end{bmatrix} Apply R_{2;1,-2} \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 0 & -11 & 4 & -2 & 1 & 0 \\\\ 6 & 4 & 1 & 0 & 0 & 1 \\\\ \\end{bmatrix} Apply R_{3;1,-3} \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 0 & -11 & 4 & -2 & 1 & 0 \\\\ 0 & -11 & 4 & -3 & 0 & 1 \\\\ \\end{bmatrix} Apply R_{3;2,-1} \\begin{bmatrix} 2 & 5 & -1 & 1 & 0 & 0 \\\\ 0 & -11 & 4 & -2 & 1 & 0 \\\\ 0 & 0 & 0 & -1 & -1 & 1 \\\\ \\end{bmatrix} So it's not invertible. \\square","title":"3.3.11."},{"location":"ch03ex/#3312","text":"The matrix A is called lower triangular if a_{ij} = 0 whenever i < j . If A is a lower triangular square matrix with all diagonal entries equal to 1 , show that A is invertible and A^{\u22121} takes the same form. Proof : We can mutiply A with R_{2;1,-a_{2,1}}, \\cdots R_{n;1,-a_{n,1}} to get the first column to become a new column, let the new matrix be A_1 . And the right half be B_1 . Then B_1 is still a lower triangular. Similarly, we can mutiply A_1 with R_{3;2,-a_{3,2}}, \\cdots R_{n;2,-a_{n,2}} to get A_2 and B_2 which are also lower triangular. Eventually, we can get A_n = I_n and B_n , which are also lower triangular. \\square","title":"3.3.12."},{"location":"ch03ex/#3313","text":"This exercise refers back to the Gram\u2013Schmidt exercise in Chapter 2. That exercise expresses the relation between the vectors \\{x'_j\\} and the vectors \\{x_j\\} formally as x' = Ax , where x' is a column vector whose entries are the vectors x'_1, \\cdots, x'_n , x is the corresponding column vector of x_j \u2019s, and A is an n\u00d7n lower triangular matrix. Show that each x_j has the form x_j = a'_{j1} x'_1 + \\cdots + a'_{j,j-1} x'_{j-1} + x'_j and thus every linear combination of the original \\{x_j\\} is also a linear combination of the new \\{x'_j\\} . Proof : Since A is lower triangular matrix, from 3.3.12, A is invertible, and A^{-1} is also a lower triangular matrix, such that x = A^{-1} x' If we expand it, we have x_j = a'_{j1} x'_1 + \\cdots + a'_{j,j-1} x'_{j-1} + x'_j \\square","title":"3.3.13."},{"location":"ch03notes/","text":"Chapter 3 Linear Mappings and Their Matrices 3.1 Linear Mappings 3.2 Operations on Matrices The matrices for the linear mapping S+T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m \\qquad \\text{and} \\qquad aS : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m Should be denoted A+B \\in M_{m, n}(\\mathbb{R}) \\qquad \\text{and} \\qquad aA \\in M_{m, n}(\\mathbb{R}) the jth column of A + B = (S + T)(e_j) \\\\ = S(e_j) + T(e_j) \\\\ = \\text{the sum of the jth columns of A and B}. Definition 3.2.1 (Matrix addition). If A = [a_{ij}]_{m \\times n} and B = [b_{ij}]_{m \\times n} then A+B = [a_{ij} + b_{ij}]_{m \\times n} . Definition 3.2.2 (Scalar-by-matrix multiplication). If \u03b1 \u2208 \\mathbb{R} and A = [a_{ij}]_{m \\times n} then \u03b1A = [\u03b1a_{ij}]_{m \\times n} . Proposition 3.2.3 ( M_{m, n}(\\mathbb{R}) forms a vector space). The set M_{m, n}(\\mathbb{R}) of m\u00d7n matrices forms a vector space over \\mathbb{R} . Composition If S : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m \\qquad \\text{and} \\qquad T : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^n are linear then their composition. S \\circ T : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m is linear as well. Suppose that S and T respectively have matrices A \\in M_{m, n}(\\mathbb{R}) \\qquad \\text{and} \\qquad B \\in M_{n, p}(\\mathbb{R}) Then the composition S\u25e6T has a matrix in M_{m, p}(\\mathbb{R}) that is naturally defined as the matrix-by-matrix product AB \\in M_{m, p}(\\mathbb{R}), the order of multiplication being chosen for consistency with the composition. Under this specification, \\begin{split} \\text{(A times B)\u2019s jth column} &= (S\u25e6T)(e_j) \\\\ &= S(T(e_j)) \\\\ &= \\text{A times (B\u2019s jth column).} \\end{split} Definition 3.2.4 (Matrix multiplication). Given two matrices A \\in M_{m, n}(\\mathbb{R}) \\qquad \\text{and} \\qquad B \\in M_{n, p}(\\mathbb{R}) such that A has as many columns as B has rows, their product, AB \\in M_{m, p}(\\mathbb{R}), has for its (i,j) th entry (for every (i,j) \u2208 \\{1,...,m\\}\u00d7\\{1,...,p\\} ) the inner product of the i th row of A and the j th column of B . In symbols, $$ (AB)_{ij} = \u27e8\\text{ i th row of A }, \\text{ j th column of B }\u27e9, $$ or, at the level of individual entries, if A = [a_{ij}]_{m \\times n} and B = [b_{ij}]_{n \\times p} then AB = \\left[ \\sum_{k = 1}^{n} a_{ik} b_{kj} \\right]_{m \\times p} $$ \\text{(A times B)\u2019s jth column \\quad equals \\quad A times (B\u2019s jth column),} \\ \\text{ith row of (A times B) \\quad equals \\quad (ith row of A) times B. } $$ Proposition 3.2.5 (Properties of matrix multiplication). Matrix multiplication is associative: (AB)C = A(BC) Matrix multiplication distributes over matrix addition: A(B +C) = AB +AC \\\\ (A+B)C= AC +BC \\\\ Scalar multiplication passes through matrix multiplication: \u03b1(AB) = (\u03b1A)B= A(\u03b1B) The identity matrix is a multiplicative identity, I_mA= A= AI_n Proof : The right way to prove these is intrinsic, by recalling that addition, scalar multiplication, and multiplication of matrices precisely mirror addition, scalar multiplication, and composition of mappings. $$ \\begin{align*} ((S \u25e6T)\u25e6U)(x) &= (S \u25e6T)(U(x)) &\\quad \\text{by definition of <script type=\"math/tex\">R\u25e6U where R= S \u25e6T } & \\ &= S(T(U(x))) &\\quad \\text{by definition of S \u25e6T } & \\ &= S((T \u25e6U)(x)) &\\quad \\text{by definition of T \u25e6U } & \\ &= (S \u25e6(T \u25e6U))(x) &\\quad \\text{by definition of S \u25e6V where V= T \u25e6U .} & \\\\ \\end{align*} $$ Alternatively, one can verify the equalities elementwise by manipulating sums. The author finds this method as grim as it is gratuitous: the coordinates work because they must, but their presence only clutters the argument. 3.3 The Inverse of a Linear Mapping Given a linear mapping S : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m , does it have an inverse? That is, is there a mapping T : \\mathbb{R}^m \\longrightarrow \\mathbb{R}^n such that S \\circ T = id_m \\quad \\text{and} \\quad T \\circ S = id_n Note the inverse T , if it exists, must be unique, T' = T' \\circ id_m = T' \\circ (S \\circ T) = (T' \\circ S) \\circ T = id_n \\circ T. If the inverse T exists then it too is linear. Assume y_1, y_2 \\in \\mathbb{R}^m , then we can find x_1, x_2 \\in \\mathbb{R}^m , such that S(x_1) = y_1, S(x_2) = y_2 . Then $$ \\begin{align*} T(y_1 + y_2) &= T(S(x_1) + S(x_2)) &\\quad \\text{above discussion} \\\\ &= T(S(x_1 + x_2)) &\\quad \\text{<script type=\"math/tex\">S is linear} \\ &= (T \\circ S)(x_1 + x_2) &\\quad \\text{Definition of composition} \\ &= x_1 + x_2 &\\quad \\text{ T inverts S } \\\\ &= T(y_1) + T(y_2) &\\quad \\text{above discussion} \\\\ \\end{align*} $$ If the equation Ax = 0_m has a nonzero solution x \\in \\mathbb{R}^n then A has no inverse. x = (A^{-1} A) x = A^{-1} 0 = 0 We have a contradiction. Definition 3.3.1 (Elementary matrices). Recombine matrix: (Here the a sits in the (i,j)th position, the diagonal entries are 1 and all other entries are 0. The a is above the diagonal as shown only when i < j ; otherwise it is below.) Scale matrix: Here the a sits in the ith diagonal position, all other diagonal entries are 1 , and all other entries are 0 . Transposition matrix: Here the diagonal entries are 1 except the ith and jth, the (i,j)th and (j,i)th entries are 1, and all other entries are 0. Proposition 3.3.2 (E\ufb00ects of the elementary matrices). Let M be an m\u00d7n matrix; call its rows r_k . Then: (1) The m\u00d7n matrix R_{i;j,a}M has the same rows as M except that its ith row is r_i +ar_j . (2) The m\u00d7n matrix S_{i,a}M has the same rows as M except that its ith row is ar_i . (3) The m\u00d7n matrix T_{i;j}M has the same rows as M except that its ith row is r_j and its jth row is r_i . Lemma 3.3.3 (Invertibility of products of the elementary matrices). Products of elementary matrices are invertible. More specifically: (1) The elementary matrices are invertible by other elementary matrices. Specifically, (R_{i;j,a})^{\u22121} = R_{i;j,-a}, \\quad (S_{i,a})^{\u22121} = S_{i,a^{-1}} , \\quad (T_{i;j})^{\u22121} = T_{i;j}. \\square (2) If the m\u00d7m matrices M and N are invertible by M^{\u22121} and N^{\u22121} , then the product matrix MN is invertible by N^{\u22121}M^{\u22121} . (Note the order reversal.) (3) Every product of elementary matrices is invertible by another such product, specifically the product of the inverses of the original matrices, but taken in reverse order. \\square Proposition 3.3.4 (Persistence of solution). Let A be an m \u00d7 n matrix and let P be a product of m\u00d7m elementary matrices. Then the equations Ax= 0_m \\quad \\text{and} \\quad (PA)x = 0_m are satisfied by the same vectors x in \\mathbb{R}^n . \\square Definition 3.3.5 (Echelon matrix). A matrix E is called echelon if it has the form E = \\begin{bmatrix} 0 & \\cdots & 0 & 1 & * & \\cdots & * & 0 & * & \\cdots & * & 0 & * & \\cdots \\\\ & & & & & & & 1 & * & \\cdots & * & 0 & * & \\cdots \\\\ & & & & & & & & & & & 1 & * & \\cdots \\\\ & & & & & & & & & & & & & \\\\ \\end{bmatrix} Here the \u2217 's are arbitrary entries, and all entries below the stairway are 0 . Thus each row\u2019s first nonzero entry is a 1 , each row\u2019s leading 1 is farther right than that of the row above it, each leading 1 has a column of 0 \u2019s above it, and any rows of 0 \u2019s are at the bottom. \\square Theorem 3.3.6 (Matrices reduce to echelon form). Every matrix A row reduces to a unique echelon matrix E . In an echelon matrix E , the columns with leading 1 's are called new columns , and all others are old columns . Theorem 3.3.7 (Invertibility and echelon form for matrices). A nonsquare matrix A is never invertible. A square matrix A is invertible if and only if its echelon form is the identity matrix. Proposition 3.3.8 (Matrix inversion algorithm). Given A \u2208 M_n(\\mathbb{R}^n) , set up the matrix B = [A | I_n] in M_{n,2n}(\\mathbb{R}^n) . Carry out row operations on this matrix to reduce the left side to echelon form. If the left side reduces to In then A is invertible and the right side is A^{\u22121} . If the left side doesn\u2019t reduce to In then A is not invertible. Theorem 3.3.9 (Echelon criterion for invertibility). The linear mapping S : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is invertible only when m = n and its matrix A has echelon matrix I_n , in which case its inverse S^{\u22121} is the linear mapping with matrix A^{\u22121} . \\square","title":"Chapter 03 Notes"},{"location":"ch03notes/#chapter-3-linear-mappings-and-their-matrices","text":"","title":"Chapter 3 Linear Mappings and Their Matrices"},{"location":"ch03notes/#31-linear-mappings","text":"","title":"3.1 Linear Mappings"},{"location":"ch03notes/#32-operations-on-matrices","text":"The matrices for the linear mapping S+T : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m \\qquad \\text{and} \\qquad aS : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m Should be denoted A+B \\in M_{m, n}(\\mathbb{R}) \\qquad \\text{and} \\qquad aA \\in M_{m, n}(\\mathbb{R}) the jth column of A + B = (S + T)(e_j) \\\\ = S(e_j) + T(e_j) \\\\ = \\text{the sum of the jth columns of A and B}.","title":"3.2 Operations on Matrices"},{"location":"ch03notes/#definition-321-matrix-addition","text":"If A = [a_{ij}]_{m \\times n} and B = [b_{ij}]_{m \\times n} then A+B = [a_{ij} + b_{ij}]_{m \\times n} .","title":"Definition 3.2.1 (Matrix addition)."},{"location":"ch03notes/#definition-322-scalar-by-matrix-multiplication","text":"If \u03b1 \u2208 \\mathbb{R} and A = [a_{ij}]_{m \\times n} then \u03b1A = [\u03b1a_{ij}]_{m \\times n} .","title":"Definition 3.2.2 (Scalar-by-matrix multiplication)."},{"location":"ch03notes/#proposition-323-m_m-nmathbbr-forms-a-vector-space","text":"The set M_{m, n}(\\mathbb{R}) of m\u00d7n matrices forms a vector space over \\mathbb{R} .","title":"Proposition 3.2.3 (M_{m, n}(\\mathbb{R}) forms a vector space)."},{"location":"ch03notes/#composition","text":"If S : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m \\qquad \\text{and} \\qquad T : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^n are linear then their composition. S \\circ T : \\mathbb{R}^p \\longrightarrow \\mathbb{R}^m is linear as well. Suppose that S and T respectively have matrices A \\in M_{m, n}(\\mathbb{R}) \\qquad \\text{and} \\qquad B \\in M_{n, p}(\\mathbb{R}) Then the composition S\u25e6T has a matrix in M_{m, p}(\\mathbb{R}) that is naturally defined as the matrix-by-matrix product AB \\in M_{m, p}(\\mathbb{R}), the order of multiplication being chosen for consistency with the composition. Under this specification, \\begin{split} \\text{(A times B)\u2019s jth column} &= (S\u25e6T)(e_j) \\\\ &= S(T(e_j)) \\\\ &= \\text{A times (B\u2019s jth column).} \\end{split}","title":"Composition"},{"location":"ch03notes/#definition-324-matrix-multiplication","text":"Given two matrices A \\in M_{m, n}(\\mathbb{R}) \\qquad \\text{and} \\qquad B \\in M_{n, p}(\\mathbb{R}) such that A has as many columns as B has rows, their product, AB \\in M_{m, p}(\\mathbb{R}), has for its (i,j) th entry (for every (i,j) \u2208 \\{1,...,m\\}\u00d7\\{1,...,p\\} ) the inner product of the i th row of A and the j th column of B . In symbols, $$ (AB)_{ij} = \u27e8\\text{ i th row of A }, \\text{ j th column of B }\u27e9, $$ or, at the level of individual entries, if A = [a_{ij}]_{m \\times n} and B = [b_{ij}]_{n \\times p} then AB = \\left[ \\sum_{k = 1}^{n} a_{ik} b_{kj} \\right]_{m \\times p} $$ \\text{(A times B)\u2019s jth column \\quad equals \\quad A times (B\u2019s jth column),} \\ \\text{ith row of (A times B) \\quad equals \\quad (ith row of A) times B. } $$","title":"Definition 3.2.4 (Matrix multiplication)."},{"location":"ch03notes/#proposition-325-properties-of-matrix-multiplication","text":"Matrix multiplication is associative: (AB)C = A(BC) Matrix multiplication distributes over matrix addition: A(B +C) = AB +AC \\\\ (A+B)C= AC +BC \\\\ Scalar multiplication passes through matrix multiplication: \u03b1(AB) = (\u03b1A)B= A(\u03b1B) The identity matrix is a multiplicative identity, I_mA= A= AI_n Proof : The right way to prove these is intrinsic, by recalling that addition, scalar multiplication, and multiplication of matrices precisely mirror addition, scalar multiplication, and composition of mappings. $$ \\begin{align*} ((S \u25e6T)\u25e6U)(x) &= (S \u25e6T)(U(x)) &\\quad \\text{by definition of <script type=\"math/tex\">R\u25e6U where R= S \u25e6T } & \\ &= S(T(U(x))) &\\quad \\text{by definition of S \u25e6T } & \\ &= S((T \u25e6U)(x)) &\\quad \\text{by definition of T \u25e6U } & \\ &= (S \u25e6(T \u25e6U))(x) &\\quad \\text{by definition of S \u25e6V where V= T \u25e6U .} & \\\\ \\end{align*} $$ Alternatively, one can verify the equalities elementwise by manipulating sums. The author finds this method as grim as it is gratuitous: the coordinates work because they must, but their presence only clutters the argument.","title":"Proposition 3.2.5 (Properties of matrix multiplication)."},{"location":"ch03notes/#33-the-inverse-of-a-linear-mapping","text":"Given a linear mapping S : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m , does it have an inverse? That is, is there a mapping T : \\mathbb{R}^m \\longrightarrow \\mathbb{R}^n such that S \\circ T = id_m \\quad \\text{and} \\quad T \\circ S = id_n Note the inverse T , if it exists, must be unique, T' = T' \\circ id_m = T' \\circ (S \\circ T) = (T' \\circ S) \\circ T = id_n \\circ T. If the inverse T exists then it too is linear. Assume y_1, y_2 \\in \\mathbb{R}^m , then we can find x_1, x_2 \\in \\mathbb{R}^m , such that S(x_1) = y_1, S(x_2) = y_2 . Then $$ \\begin{align*} T(y_1 + y_2) &= T(S(x_1) + S(x_2)) &\\quad \\text{above discussion} \\\\ &= T(S(x_1 + x_2)) &\\quad \\text{<script type=\"math/tex\">S is linear} \\ &= (T \\circ S)(x_1 + x_2) &\\quad \\text{Definition of composition} \\ &= x_1 + x_2 &\\quad \\text{ T inverts S } \\\\ &= T(y_1) + T(y_2) &\\quad \\text{above discussion} \\\\ \\end{align*} $$ If the equation Ax = 0_m has a nonzero solution x \\in \\mathbb{R}^n then A has no inverse. x = (A^{-1} A) x = A^{-1} 0 = 0 We have a contradiction.","title":"3.3 The Inverse of a Linear Mapping"},{"location":"ch03notes/#definition-331-elementary-matrices","text":"Recombine matrix: (Here the a sits in the (i,j)th position, the diagonal entries are 1 and all other entries are 0. The a is above the diagonal as shown only when i < j ; otherwise it is below.) Scale matrix: Here the a sits in the ith diagonal position, all other diagonal entries are 1 , and all other entries are 0 . Transposition matrix: Here the diagonal entries are 1 except the ith and jth, the (i,j)th and (j,i)th entries are 1, and all other entries are 0.","title":"Definition 3.3.1 (Elementary matrices)."},{"location":"ch03notes/#proposition-332-effects-of-the-elementary-matrices","text":"Let M be an m\u00d7n matrix; call its rows r_k . Then: (1) The m\u00d7n matrix R_{i;j,a}M has the same rows as M except that its ith row is r_i +ar_j . (2) The m\u00d7n matrix S_{i,a}M has the same rows as M except that its ith row is ar_i . (3) The m\u00d7n matrix T_{i;j}M has the same rows as M except that its ith row is r_j and its jth row is r_i .","title":"Proposition 3.3.2 (E\ufb00ects of the elementary matrices)."},{"location":"ch03notes/#lemma-333-invertibility-of-products-of-the-elementary-matrices","text":"Products of elementary matrices are invertible. More specifically: (1) The elementary matrices are invertible by other elementary matrices. Specifically, (R_{i;j,a})^{\u22121} = R_{i;j,-a}, \\quad (S_{i,a})^{\u22121} = S_{i,a^{-1}} , \\quad (T_{i;j})^{\u22121} = T_{i;j}. \\square (2) If the m\u00d7m matrices M and N are invertible by M^{\u22121} and N^{\u22121} , then the product matrix MN is invertible by N^{\u22121}M^{\u22121} . (Note the order reversal.) (3) Every product of elementary matrices is invertible by another such product, specifically the product of the inverses of the original matrices, but taken in reverse order. \\square","title":"Lemma 3.3.3 (Invertibility of products of the elementary matrices)."},{"location":"ch03notes/#proposition-334-persistence-of-solution","text":"Let A be an m \u00d7 n matrix and let P be a product of m\u00d7m elementary matrices. Then the equations Ax= 0_m \\quad \\text{and} \\quad (PA)x = 0_m are satisfied by the same vectors x in \\mathbb{R}^n . \\square","title":"Proposition 3.3.4 (Persistence of solution)."},{"location":"ch03notes/#definition-335-echelon-matrix","text":"A matrix E is called echelon if it has the form E = \\begin{bmatrix} 0 & \\cdots & 0 & 1 & * & \\cdots & * & 0 & * & \\cdots & * & 0 & * & \\cdots \\\\ & & & & & & & 1 & * & \\cdots & * & 0 & * & \\cdots \\\\ & & & & & & & & & & & 1 & * & \\cdots \\\\ & & & & & & & & & & & & & \\\\ \\end{bmatrix} Here the \u2217 's are arbitrary entries, and all entries below the stairway are 0 . Thus each row\u2019s first nonzero entry is a 1 , each row\u2019s leading 1 is farther right than that of the row above it, each leading 1 has a column of 0 \u2019s above it, and any rows of 0 \u2019s are at the bottom. \\square","title":"Definition 3.3.5 (Echelon matrix)."},{"location":"ch03notes/#theorem-336-matrices-reduce-to-echelon-form","text":"Every matrix A row reduces to a unique echelon matrix E . In an echelon matrix E , the columns with leading 1 's are called new columns , and all others are old columns .","title":"Theorem 3.3.6 (Matrices reduce to echelon form)."},{"location":"ch03notes/#theorem-337-invertibility-and-echelon-form-for-matrices","text":"A nonsquare matrix A is never invertible. A square matrix A is invertible if and only if its echelon form is the identity matrix.","title":"Theorem 3.3.7 (Invertibility and echelon form for matrices)."},{"location":"ch03notes/#proposition-338-matrix-inversion-algorithm","text":"Given A \u2208 M_n(\\mathbb{R}^n) , set up the matrix B = [A | I_n] in M_{n,2n}(\\mathbb{R}^n) . Carry out row operations on this matrix to reduce the left side to echelon form. If the left side reduces to In then A is invertible and the right side is A^{\u22121} . If the left side doesn\u2019t reduce to In then A is not invertible.","title":"Proposition 3.3.8 (Matrix inversion algorithm)."},{"location":"ch03notes/#theorem-339-echelon-criterion-for-invertibility","text":"The linear mapping S : \\mathbb{R}^n \\longrightarrow \\mathbb{R}^m is invertible only when m = n and its matrix A has echelon matrix I_n , in which case its inverse S^{\u22121} is the linear mapping with matrix A^{\u22121} . \\square","title":"Theorem 3.3.9 (Echelon criterion for invertibility)."},{"location":"unsolved/","text":"2.1.7. Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.)","title":"Unsolved Problems"},{"location":"unsolved/#217","text":"Show the uniqueness of the additive identity and the additive inverse using only (A1), (A2), (A3). (This is tricky; the opening pages of some books on group theory will help.)","title":"2.1.7."}]}